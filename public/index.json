[{"authors":["andrew"],"categories":null,"content":"Andrew Heiss is an assistant professor at the Andrew Young School of Policy Studies at Georgia State University, researching international NGOs and teaching data science, program evaluation, and economics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1606601144,"objectID":"92c4fbf08c28d675fd0ae26ad532273f","permalink":"https://evalf20.classes.andrewheiss.com/authors/andrew/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/andrew/","section":"authors","summary":"Andrew Heiss is an assistant professor at the Andrew Young School of Policy Studies at Georgia State University, researching international NGOs and teaching data science, program evaluation, and economics.","tags":null,"title":"Andrew Heiss","type":"authors"},{"authors":null,"categories":null,"content":"  I have included a bunch of extra resources and guides related to causal inference, program evaluation, R, data, and other relevant topics. Enjoy!\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1606601144,"objectID":"8939c748f3090c6f91bdac5d32db55ec","permalink":"https://evalf20.classes.andrewheiss.com/resource/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/","section":"resource","summary":"I have included a bunch of extra resources and guides related to causal inference, program evaluation, R, data, and other relevant topics. Enjoy!","tags":null,"title":"Helpful resources","type":"docs"},{"authors":null,"categories":null,"content":"   Weekly check-in Problem sets Evaluation assignments Exams Final project   The main goals of this class are to help you design, critique, code, and run rigorous, valid, and feasible evaluations of public sector programs. Each type of assignment in this class is designed to help you achieve one or more of these goals.\nWeekly check-in Every week, after you finish working through the content, I want to hear about what you learned and what questions you still have. To facilitate this, and to encourage engagement with the course content, you’ll need to fill out a short response on iCollege. This should be ≈150 words. That’s fairly short: there are ≈250 words on a typical double-spaced page in Microsoft Word (500 when single-spaced).\nYou should answer these two questions each week:\nWhat was the most exciting thing you learned from the session? Why? What was the muddiest thing from the session this week? What are you still wondering about?  I will grade these check-ins using a check system:\n ✔+: (11.5 points (115%) in gradebook) Response shows phenomenal thought and engagement with the course content. I will not assign these often. ✔: (10 points (100%) in gradebook) Response is thoughtful, well-written, and shows engagement with the course content. This is the expected level of performance. ✔−: (5 points (50%) in gradebook) Response is hastily composed, too short, and/or only cursorily engages with the course content. This grade signals that you need to improve next time. I will hopefully not assign these often.  Notice that is essentially a pass/fail or completion-based system. I’m not grading your writing ability, I’m not counting the exact number of words you’re writing, and I’m not looking for encyclopedic citations of every single reading to prove that you did indeed read everything. I’m looking for thoughtful engagement, that’s all. Do good work and you’ll get a ✓.\nYou will submit these responses via iCollege.\n Problem sets To practice writing R code, running inferential models, and thinking about causation, you will complete a series of problem sets.\nThere are 9 problem sets on the schedule. I will keep the highest grades for 8 of them. That is, I will drop the lowest score (even if it’s a zero). This means you can skip one of the problem sets. You need to show that you made a good faith effort to work each question. I will not grade these in detail. The problem sets will be graded using a check system:\n ✔+: (33 points (110%) in gradebook) Assignment is 100% completed. Every question was attempted and answered, and most answers are correct. Document is clean and easy to follow. Work is exceptional. I will not assign these often. ✔: (30 points (100%) in gradebook) Assignment is 70–99% complete and most answers are correct. This is the expected level of performance. ✔−: (15 points (50%) in gradebook) Assignment is less than 70% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not asisgn these often.  You may (and should!) work together on the problem sets, but you must turn in your own answers. You cannot work in groups of more than four people, and you must note who participated in the group in your assignment.\n Evaluation assignments For your final project, you will conduct a pre-registered evaluation of a social program using synthetic data. To (1) give you practice with the principles of program evaluation, research design, measurement, and causal diagrams, and (2) help you with the foundation of your final project, you will complete a set of four evaluation-related assignments.\nIdeally these will become major sections of your final project. However, there is no requirement that the programs you use in these assignments must be the same as the final project. If, through these assignments, you discover that your initially chosen program is too simple, too complex, too boring, etc., you can change at any time.\nThese assignments will be graded using a check system:\n ✔+: (33 points (110%) in gradebook) Assignment is 100% completed. Every question was attempted and answered, and most answers are correct. Document is clean and easy to follow. Work is exceptional. I will not assign these often. ✔: (30 points (100%) in gradebook) Assignment is 70–99% complete and most answers are correct. This is the expected level of performance. ✔−: (15 points (50%) in gradebook) Assignment is less than 70% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not asisgn these often.   Exams There will be two exams covering (1) program evaluation, design, and causation, and (2) the core statistical tools of program evaluation and causal inference.\nYou will take these exams online through iCollege. The exams will have a time limit, but you can use notes and readings and the Google. You must take the exams on your own though, and not talk to anyone about them.\n Final project At the end of the course, you will demonstrate your knowledge of program evaluation and causal inference by completing a final project.\nComplete details for the final project are here.\nThere is no final exam. This project is your final exam.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1606601144,"objectID":"3aa23ffb1eb3dedbe4d8a9c2165e2c58","permalink":"https://evalf20.classes.andrewheiss.com/assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/","section":"assignment","summary":"Weekly check-in Problem sets Evaluation assignments Exams Final project   The main goals of this class are to help you design, critique, code, and run rigorous, valid, and feasible evaluations of public sector programs. Each type of assignment in this class is designed to help you achieve one or more of these goals.\nWeekly check-in Every week, after you finish working through the content, I want to hear about what you learned and what questions you still have.","tags":null,"title":"Assignment details","type":"docs"},{"authors":null,"categories":null,"content":"  Visit this section after you have finished the readings and lecture videos. It contains fully annotated R code and other supplementary information and it will be indispensable as you work on your problem sets and project.\nMany sections also contain videos of me live coding the examples so you can see what it looks like to work with R in real time. You’ll notice me make all sorts of little errors, which is totally normal—everyone does!\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1606601144,"objectID":"00e8826988eea7dfc8b8047b4c0184ce","permalink":"https://evalf20.classes.andrewheiss.com/example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/","section":"example","summary":"Visit this section after you have finished the readings and lecture videos. It contains fully annotated R code and other supplementary information and it will be indispensable as you work on your problem sets and project.\nMany sections also contain videos of me live coding the examples so you can see what it looks like to work with R in real time. You’ll notice me make all sorts of little errors, which is totally normal—everyone does!","tags":null,"title":"Code examples","type":"docs"},{"authors":null,"categories":null,"content":"  Each class session has a set of required readings that you should complete before watching the lecture.\nEvery class session also has a YouTube playlist of short recorded videos for each of the lecture sections. The lecture slides are special HTML files made with the R package xaringan (R can do so much!). On each class session page you’ll see buttons for opening the presentation in a new tab or for downloading a PDF of the slides in case you want to print them or store them on your computer:\n View all slides in new window  Download PDF of all slides The slides are also embedded on each page. You can click in the slides and navigate through them with ← and →. If you type ? (or shift + /) while viewing the slides you can see a list of slide-specific commands (like f for fullscreen or p for presenter mode if you want to see my notes).\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1606601144,"objectID":"1413f960ec974b9863bc45d887efa8bd","permalink":"https://evalf20.classes.andrewheiss.com/content/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/content/","section":"content","summary":"Each class session has a set of required readings that you should complete before watching the lecture.\nEvery class session also has a YouTube playlist of short recorded videos for each of the lecture sections. The lecture slides are special HTML files made with the R package xaringan (R can do so much!). On each class session page you’ll see buttons for opening the presentation in a new tab or for downloading a PDF of the slides in case you want to print them or store them on your computer:","tags":null,"title":"Readings, lectures, and videos","type":"docs"},{"authors":null,"categories":null,"content":"   Evidence, causation, and evaluation Regression and inference Theories of change and measurement Counterfactuals and DAGs Threats to validity   Evidence, causation, and evaluation You should understand…\n …the difference between experimental research and observational research …the sometimes conflicting roles of science and intuition in public administration and policy …the difference between the various types of evaluations and how they target specific parts of a logic model …the difference between identifying correlation (math) and identifying causation (philosophy and theory) …what it means for a relationship to be causal   Regression and inference You should understand…\n …the difference between correlation coefficients and regression coefficients …the difference between outcome/response/dependent and explanatory/predictor/independent variables …the two purposes of regression …what each of the components in a regression equation stand for, in both “flavors” of notation:  \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\) for the statistical flavor \\(y = \\alpha + \\beta x_1 + \\gamma x_2 + \\epsilon\\) for the econometrics flavor  …how sliders and switches work as metaphors for regression coefficients …what it means to hold variables constant (or to control for variables) …the different elements of the grammar of graphics and be able to identify how variables are encoded in a graph (i.e. how columns in a dataset can be represented through x/y coordinates, through color, through size, through fill, etc.)  You should be able to…\n …write and interpret R code that calculates summary statistics for groups (i.e. group_by() %\u0026gt;% summarize()) …write and interpret R code that builds linear models (i.e. lm()) …interpret regression coefficients …interpret other regression diagnostics like \\(R^2\\) …use the %\u0026gt;% pipe in R to chain functions together …use ggplot() to visualize data  Helpful resources:\n  Garrett Grolemund and Hadley Wickham, R for Data Science  Kieran Healy, “How ggplot works,” chapter 3 in Data Visualization: A Practical Introduction   Theories of change and measurement You should understand…\n …how to describe a program’s theory of change …the difference between inputs, activities, outputs, and outcomes …the elements of a program’s impact theory: causes (activities) linked to effects (outcomes) …the elements of a program’s logic model: the explicit links between all its inputs, activities, outputs, and outcomes …the difference between implicit and articulated program theories …the purpose of smaller-scale mechanism testing …how indicators can be measured at different levels of abstraction …what makes an indicator a good indicator  You should be able to…\n …identify a program’s underlying theory based on its mission statement …draw a program impact theory chart that links activities to outcomes …draw a program logic model that links inputs to activities to outputs to outcomes …identify the most central elements of a potential outcome measurement   Counterfactuals and DAGs You should understand…\n …how a causal model encodes our understanding of a causal process …how to identify front door and back door paths between treatment/exposure and outcome …why we avoid closing front door paths …why we close back door paths …why adjusting for colliders can distort causal effects …the difference between logic models and DAGs …the difference between individual level causal effects, average treatment effects (ATE), conditional average treatment effect (CATE), average treatment on the treated effects (ATT), and average treatment on the untreated (ATU) …what the fundamental problem of causal inference is and how we can attempt to address it  You should be able to…\n …draw a possible DAG for a given causal relationship …identify all pathways between treatment/exposure and outcome …identify which nodes in the DAG need to be adjusted for (or closed) …identify colliders (which should not be adjusted for)  Helpful resources:\n  Malcom Barrett, “An Introduction to Directed Acyclic Graphs”  Malcom Barrett, “An Introduction to ggdag”  Judea Pearl, “A Crash Course in Good and Bad Control”: A quick summary of back doors, front doors, confounders, colliders, and when to control/not control for DAG nodes  Causal Inference Bootcamp, “Average Treatment Effects,” Duke University  Causal Inference Bootcamp, “Unit Level Effects,” Duke University  Causal Inference Bootcamp, “Conditional Average Treatment Effects,” Duke University  Causal Inference Bootcamp, “Counterfactuals,” Duke University  Neel Ocean, “Understanding Selection Bias”: explanation of how to identify selection bias from the ATT and the ATE, with an explanation of how ATE = ATT + selection bias under the potential outcomes framework  Paul Hünermund, “Sample Selection vs. Selection Into Treatment”   Threats to validity You should understand…\n …what it means when a study has internal validity and know how to identify the major threats to internal validity, including: omitted variable bias (selection and attrition), trend issues (maturation, secular trends, seasonality, testing, regression to the mean), study calibration issues (measurement error, time frame of study), and contamination issues (Hawthorne effects, John Henry effects, spillovers, and intervening events) …why selection bias is the most pernicious and difficult threat to internal validity and how we can account for it …what it means when a study has external validity …what it means when the measures used in a study have construct validity …what it means when the analysis used in a study has statistical conclusion validity  You should be able to…\n …identify existing and potential threats to validity in a study …suggest ways of addressing these threats  Helpful resources:\n Really, just google “threats to internal validity” or “threats to external validity” and you’ll find a billion different slide decks, articles, and lessons about these. They’re a pretty standard part of any research design class.   ","date":1601424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"321645e7d4ea6c0724c6a04f77432675","permalink":"https://evalf20.classes.andrewheiss.com/resource/exam1/","publishdate":"2020-09-30T00:00:00Z","relpermalink":"/resource/exam1/","section":"resource","summary":"Evidence, causation, and evaluation Regression and inference Theories of change and measurement Counterfactuals and DAGs Threats to validity   Evidence, causation, and evaluation You should understand…\n …the difference between experimental research and observational research …the sometimes conflicting roles of science and intuition in public administration and policy …the difference between the various types of evaluations and how they target specific parts of a logic model …the difference between identifying correlation (math) and identifying causation (philosophy and theory) …what it means for a relationship to be causal   Regression and inference You should understand…","tags":null,"title":"Things you should know for Exam 1","type":"docs"},{"authors":null,"categories":null,"content":"   RStudio.cloud RStudio on your computer  Install R Install RStudio Install tidyverse Install tinytex    You will do all of your work in this class with the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code.\nRStudio.cloud R is free, but it can sometimes be a pain to install and configure. To make life easier, you can (and should!) use the free RStudio.cloud service initially, which lets you run a full instance of RStudio in your web browser. This means you won’t have to install anything on your computer to get started with R! We will have a shared class workspace in RStudio.cloud that will let you quickly copy templates for labs and problem sets.\nGo to https://rstudio.cloud/ and create an account. You’ll receive a link to join the shared class workspace separately. If you don’t get this link, let me know and I will invite you.\n RStudio on your computer RStudio.cloud is convenient, but it can be slow and it is not designed to be able to handle larger datasets, more complicated analysis, or fancier graphics. Over the course of the semester, you should wean yourself off of RStudio.cloud and install all these things locally. This is also important if you want to customize fonts, since RStudio.cloud has extremely limited support for fonts other than Helvetica.\nHere’s how you install all these things\nInstall R First you need to install R itself (the engine).\nGo to the CRAN (Collective R Archive Network)1 website: https://cran.r-project.org/\n Click on “Download R for XXX”, where XXX is either Mac or Windows:\n If you use macOS, scroll down to the first .pkg file in the list of files (in this picture, it’s R-4.0.0.pkg; as of right now, the current version is also 4.0.0) and download it.\n If you use Windows, click “base” (or click on the bolded “install R for the first time” link) and download it.\n  Double click on the downloaded file (check your Downloads folder). Click yes through all the prompts to install like any other program.\n If you use macOS, download and install XQuartz. You do not need to do this on Windows.\n   Install RStudio Next, you need to install RStudio, the nicer graphical user interface (GUI) for R (the dashboard). Once R and RStudio are both installed, you can ignore R and only use RStudio. RStudio will use R automatically and you won’t ever have to interact with it directly.\nGo to the free download location on RStudio’s website: https://www.rstudio.com/products/rstudio/download/#download\n The website should automatically detect your operating system (macOS or Windows) and show a big download button for it:\nIf not, scroll down a little to the large table and choose the version of RStudio that matches your operating system.\n Double click on the downloaded file (again, check your Downloads folder). Click yes through all the prompts to install like any other program.\n  Double click on RStudio to run it (check your applications folder or start menu).\n Install tidyverse R packages are easy to install with RStudio. Select the packages panel, click on “Install,” type the name of the package you want to install, and press enter.\nThis can sometimes be tedious when you’re installing lots of packages, though. The tidyverse, for instance, consists of dozens of packages (including ggplot2) that all work together. Rather than install each individually, you can install a single magical package and get them all at the same time.\nGo to the packages panel in RStudio, click on “Install,” type “tidyverse”, and press enter. You’ll see a bunch of output in the RStudio console as all the tidyverse packages are installed.\nNotice also that RStudio will generate a line of code for you and run it: install.packages(\"tidyverse\"). You can also just paste and run this instead of using the packages panel.\n Install tinytex When you knit to PDF, R uses a special scientific typesetting program named LaTeX (pronounced “lay-tek” or “lah-tex”; for goofy nerdy reasons, the x is technically the “ch” sound in “Bach”, but most people just say it as “k”—saying “layteks” is frowned on for whatever reason).\nLaTeX is neat and makes pretty documents, but it’s a huge program—the macOS version, for instance, is nearly 4 GB! To make life easier, there’s an R package named tinytex that installs a minimal LaTeX program and that automatically deals with differences between macOS and Windows.\nHere’s how to install tinytex so you can knit to pretty PDFs:\nUse the Packages in panel in RStudio to install tinytex like you did above with tidyverse. Alternatively, run install.packages(\"tinytex\") in the console. Run tinytex::install_tinytex() in the console. Wait for a bit while R downloads and installs everything you need. The end! You should now be able to knit to PDF.     It’s a goofy name, but CRAN is where most R packages—and R itself—lives.↩︎\n   ","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"efb59c0882a965443ffcbafa3cd27ca6","permalink":"https://evalf20.classes.andrewheiss.com/resource/install/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/resource/install/","section":"resource","summary":"RStudio.cloud RStudio on your computer  Install R Install RStudio Install tidyverse Install tinytex    You will do all of your work in this class with the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code.","tags":null,"title":"Installing R, RStudio, tidyverse, and tinytex","type":"docs"},{"authors":null,"categories":null,"content":"   Randomization Matching and inverse probability weighting Difference-in-difference Regression discontinuity Instrumental variables   Randomization  Understand why randomization is crucial for causal inference and counterfactuals Understand the process for analyzing a randomized controlled trial  Crucial resources:\n Readings, slides, and videos for randomization and matching Guide for RCTs Problem set 3 Task 1 in problem set 8   Matching and inverse probability weighting  Understand the intuition behind matching and inverse probability weighting Understand the process for adjusting for confounders and closing backdoors with both matching and inverse probability weighting  Crucial resources:\n Readings, slides, and videos for randomization and matching Guide for matching and inverse probability weighting Problem set 3 Task 2 in problem set 8   Difference-in-difference  Understand the intuition behind making causal inferences with difference-in-differences Understand the process for analyzing diff-in-diffs  Crucial resources:\n Readings, slides, and videos for diff-in-diff Guide for diff-in-diff Problem set 4 Problem set 5 Task 3 in problem set 8   Regression discontinuity  Understand the intuition behind making causal inferences with regression discontinuity Understand the process for analyzing regression discontinuities, both fuzzy and sharp Understand the difference between ATE and LATE  Crucial resources:\n Readings, slides, and videos for regression discontinuity I (sharp) Readings, slides, and videos for regression discontinuity II (fuzzy) Guide for sharp diff-in-diff Guide for fuzzy diff-in-diff Problem set 6 Task 4 in problem set 8   Instrumental variables  Understand the intuition behind using instruments for causal inference Understand the three characteristics of a good instrument Understand the process for analyzing data with instrumental variables and 2SLS Understand the difference between ATE and LATE  Crucial resources:\n Readings, slides, and videos for instrumental variables I Readings, slides, and videos for instrumental variables II Guide for instrumental variables Problem set 7 Task 5 in problem set 8   ","date":1605139200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"fd91e0322b0d6848d873e526d5d08dc1","permalink":"https://evalf20.classes.andrewheiss.com/resource/exam2/","publishdate":"2020-11-12T00:00:00Z","relpermalink":"/resource/exam2/","section":"resource","summary":"Randomization Matching and inverse probability weighting Difference-in-difference Regression discontinuity Instrumental variables   Randomization  Understand why randomization is crucial for causal inference and counterfactuals Understand the process for analyzing a randomized controlled trial  Crucial resources:\n Readings, slides, and videos for randomization and matching Guide for RCTs Problem set 3 Task 1 in problem set 8   Matching and inverse probability weighting  Understand the intuition behind matching and inverse probability weighting Understand the process for adjusting for confounders and closing backdoors with both matching and inverse probability weighting  Crucial resources:","tags":null,"title":"Things you should know for Exam 2","type":"docs"},{"authors":null,"categories":null,"content":"   Learning R R in the wild   Learning R I highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.\nSearching for help with R on Google can sometimes be tricky because the program name is a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”).\nIf you use Twitter, post R-related questions and content with #rstats. The community there is exceptionally generous and helpful. Also check out StackOverflow (a Q\u0026amp;A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)).\nThese resources are also really really helpful:\n R for Data Science: A free online book for learning the basics of R and the tidyverse. R and RStudio cheat sheets: A large collection of simple cheat sheets for RStudio, ggplot2, and other R-related things. Stat 545: Dr. Jenny Bryan at RStudio has an entire introductory course in R, visualization, and data analysis online. STA 112FS: Data Science: Dr. Mine Çetinkaya-Rundel at the University of Edinburgh / Duke University has an entire introductory course in R, visualization, and data science online. CSE 631: Principles \u0026amp; Practice of Data Visualization: Yet another introductory course for R and ggplot2 by Dr. Alison Presmanes Hill at RStudio.   R in the wild A popular (and increasingly standard) way for sharing your analyses and visualizations is to post an annotated explanation of your process somewhere online. RStudio allows you to publish knitted HTML files directly to RPubs, but you can also post your output to a blog or other type of website.1 Reading these kinds of posts is one of the best ways to learn R, since they walk you through each step of the process and show the code and output.\nHere are some of the best examples I’ve come across:\n Text analysis of Trump’s tweets confirms he writes only the (angrier) Android half (with a follow-up) Bob Ross - Joy of Painting Bechdel analysis using the tidyverse: There are also a bunch of other examples using data from FiveThirtyEight. Sexism on the Silver Screen: Exploring film’s gender divide Comparison of Quentin Tarantino Movies by Box Office and the Bechdel Test Who came to vote in Utah’s caucuses? Health care indicators in Utah counties Song lyrics across the United States A decade (ish) of listening to Sigur Rós When is Tom peeping these days?: There are a also bunch of final projects from other R and data visualization classes here and here. Mapping Fall Foliage General (Attys) Distributions Disproving Approval    If you want to be really fancy, you can use blogdown, which makes a complete website with R Markdown files. That’s actually how this site is built (see the source code). You can build your own site with this tutorial.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"0f6270d48011ac62645a8455a86a24bf","permalink":"https://evalf20.classes.andrewheiss.com/resource/r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/r/","section":"resource","summary":"Learning R R in the wild   Learning R I highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.\nSearching for help with R on Google can sometimes be tricky because the program name is a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.","tags":null,"title":"R","type":"docs"},{"authors":null,"categories":null,"content":"   R style conventions Main style things to pay attention to for this class  Spacing Long lines Pipes (%\u0026gt;%) and ggplot layers (+) Comments    R style conventions R is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:\nmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty\u0026gt;10, class==\u0026quot;compact\u0026quot;) filter(mpg,cty\u0026gt;10,class==\u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) filter ( mpg,cty\u0026gt;10, class==\u0026quot;compact\u0026quot; ) But you’ll notice that only a few of those iterations (the first three) are easily readable.\nTo help improve readability and make it easier to share code with others, there’s an unofficial style guide for writing R code. It’s fairly short and just has lots of examples of good and bad ways of writing code (naming variables, dealing with long lines, using proper indentation levels, etc.)—you should glance through it some time.\nRStudio has a built-in way of cleaning up your code. Select some code, press ctrl + i (on Windows) or ⌘ + i (on macOS), and R will reformat the code for you. It’s not always perfect, but it’s really helpful for getting indentation right without having to manually hit space a billion times.\n Main style things to pay attention to for this class  Important note: I won’t ever grade you on any of this! If you submit something like filter(mpg,cty\u0026gt;10,class==\"compact\"), I might recommend adding spaces, but it won’t affect your grade or points or anything.\n Spacing  See the “Spacing” section in the tidyverse style guide.\n Put spaces after commas (like in regular English):\n# Good filter(mpg, cty \u0026gt; 10) # Bad filter(mpg , cty \u0026gt; 10) filter(mpg ,cty \u0026gt; 10) filter(mpg,cty \u0026gt; 10) Put spaces around operators like +, -, \u0026gt;, =, etc.:\n# Good filter(mpg, cty \u0026gt; 10) # Bad filter(mpg, cty\u0026gt;10) filter(mpg, cty\u0026gt; 10) filter(mpg, cty \u0026gt;10) Don’t put spaces around parentheses that are parts of functions:\n# Good filter(mpg, cty \u0026gt; 10) # Bad filter (mpg, cty \u0026gt; 10) filter ( mpg, cty \u0026gt; 10) filter( mpg, cty \u0026gt; 10 )  Long lines  See the “Long lines” section in the tidyverse style guide.\n It’s generally good practice to not have really long lines of code. A good suggestion is to keep lines at a maximum of 80 characters. Instead of counting characters by hand (ew), in RStudio go to “Tools” \u0026gt; “Global Options” \u0026gt; “Code” \u0026gt; “Display” and check the box for “Show margin”. You should now see a really thin line indicating 80 characters. Again, you can go beyond this—that’s fine. It’s just good practice to avoid going too far past it.\nYou can add line breaks inside longer lines of code. Line breaks should come after commas, and things like function arguments should align within the function:\n# Good filter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) # Good filter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) # Good filter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) # Bad filter(mpg, cty \u0026gt; 10, class %in% c(\u0026quot;compact\u0026quot;, \u0026quot;pickup\u0026quot;, \u0026quot;midsize\u0026quot;, \u0026quot;subcompact\u0026quot;, \u0026quot;suv\u0026quot;, \u0026quot;2seater\u0026quot;, \u0026quot;minivan\u0026quot;)) # Good filter(mpg, cty \u0026gt; 10, class %in% c(\u0026quot;compact\u0026quot;, \u0026quot;pickup\u0026quot;, \u0026quot;midsize\u0026quot;, \u0026quot;subcompact\u0026quot;, \u0026quot;suv\u0026quot;, \u0026quot;2seater\u0026quot;, \u0026quot;minivan\u0026quot;))  Pipes (%\u0026gt;%) and ggplot layers (+) Put each layer of a ggplot plot on separate lines, with the + at the end of the line, indented with two spaces:\n# Good ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() # Bad ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() # Super bad ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() # Super bad and won\u0026#39;t even work ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() Put each step in a dplyr pipeline on separate lines, with the %\u0026gt;% at the end of the line, indented with two spaces:\n# Good mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy)) # Bad mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy)) # Super bad mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy)) # Super bad and won\u0026#39;t even work mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy))  Comments  See the “Comments” section in the tidyverse style guide.\n Comments should start with a comment symbol and a single space: #\n# Good #Bad #Bad If the comment is really short (and won’t cause you to go over 80 characters in the line), you can include it in the same line as the code, separated by at least two spaces (it works with one space, but using a couple can enhance readability):\nmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% # Only rows where cty is 10 + group_by(class) %\u0026gt;% # Divide into class groups summarize(avg_hwy = mean(hwy)) # Find the average hwy in each group You can add extra spaces to get inline comments to align, if you want:\nmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% # Only rows where cty is 10 + group_by(class) %\u0026gt;% # Divide into class groups summarize(avg_hwy = mean(hwy)) # Find the average hwy in each group If the comment is really long, you can break it into multiple lines. RStudio can do this for you if you go to “Code” \u0026gt; “Reflow comment”\n# Good # Happy families are all alike; every unhappy family is unhappy in its own way. # Everything was in confusion in the Oblonskys’ house. The wife had discovered # that the husband was carrying on an intrigue with a French girl, who had been # a governess in their family, and she had announced to her husband that she # could not go on living in the same house with him. This position of affairs # had now lasted three days, and not only the husband and wife themselves, but # all the members of their family and household, were painfully conscious of it. # Bad # Happy families are all alike; every unhappy family is unhappy in its own way. Everything was in confusion in the Oblonskys’ house. The wife had discovered that the husband was carrying on an intrigue with a French girl, who had been a governess in their family, and she had announced to her husband that she could not go on living in the same house with him. This position of affairs had now lasted three days, and not only the husband and wife themselves, but all the members of their family and household, were painfully conscious of it. Though, if you’re dealing with comments that are that long, consider putting the text in R Markdown instead and having it be actual prose.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"f4734e734c67442efdc8d228e91ad766","permalink":"https://evalf20.classes.andrewheiss.com/resource/style/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/style/","section":"resource","summary":"R style conventions Main style things to pay attention to for this class  Spacing Long lines Pipes (%\u0026gt;%) and ggplot layers (+) Comments    R style conventions R is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:\nmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty\u0026gt;10, class==\u0026quot;compact\u0026quot;) filter(mpg,cty\u0026gt;10,class==\u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) filter ( mpg,cty\u0026gt;10, class==\u0026quot;compact\u0026quot; ) But you’ll notice that only a few of those iterations (the first three) are easily readable.","tags":null,"title":"R style suggestions","type":"docs"},{"authors":null,"categories":null,"content":"   Unzipping files on macOS Unzipping files on Windows   Because RStudio projects typically consist of multiple files (R scripts, datasets, graphical output, etc.) the easiest way to distribute them to you for examples, assignments, and projects is to combine all the different files in to a single compressed collection called a zip file. When you unzip a zipped file, your operating system extracts all the files that are contained inside into a new folder on your computer.\nUnzipping files on macOS is trivial, but unzipping files on Windows can mess you up if you don’t pay careful attention. Here’s a helpful guide to unzipping files on both macOS and Windows.\nUnzipping files on macOS Double click on the downloaded .zip file. macOS will automatically create a new folder with the same name as the .zip file, and all the file’s contents will be inside. Double click on the RStudio Project file (.Rproj) to get started.\n Unzipping files on Windows tl;dr: Right click on the .zip file, select “Extract All…”, and work with the resulting unzipped folder.\nUnlike macOS, Windows does not automatically unzip things for you. If you double click on the .zip file, Windows will show you what’s inside, but it will do so without actually extracting anything. This can be is incredibly confusing! Here’s what it looks like—the only clues that this folder is really a .zip file are that there’s a “Compressed Folder Tools” tab at the top, and there’s a “Ratio” column that shows how much each file is compressed.\nIt is very tempting to try to open files from this view. However, if you do, things will break and you won’t be able to correctly work with any of the files in the zipped folder. If you open the R Project file, for instance, RStudio will point to a bizarre working directory buried deep in some temporary folder:\nYou most likely won’t be able to open any data files or save anything, which will be frustrating.\nInstead, you need to right click on the .zip file and select “Extract All…”:\nThen choose where you want to unzip all the files and click on “Extract”\nYou should then finally have a real folder with all the contents of the zipped file. Open the R Project file and RStudio will point to the correct working directory and everything will work.\n ","date":1588723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"c14c352fd4c4ab8c12a3cd60b30b9d8c","permalink":"https://evalf20.classes.andrewheiss.com/resource/unzipping/","publishdate":"2020-05-06T00:00:00Z","relpermalink":"/resource/unzipping/","section":"resource","summary":"Unzipping files on macOS Unzipping files on Windows   Because RStudio projects typically consist of multiple files (R scripts, datasets, graphical output, etc.) the easiest way to distribute them to you for examples, assignments, and projects is to combine all the different files in to a single compressed collection called a zip file. When you unzip a zipped file, your operating system extracts all the files that are contained inside into a new folder on your computer.","tags":null,"title":"Unzipping files","type":"docs"},{"authors":null,"categories":null,"content":"  There are a ton of places to find data related to public policy and administration (as well as data on pretty much any topic you want) online:\n Data is Plural newsletter: Jeremy Singer-Vine sends a weekly newsletter of the most interesting public datasets he’s found. You should subscribe to it. He also has an archive of all the datasets he’s highlighted.\n Google Dataset Search: Google indexes thousands of public datasets; search for them here.\n Kaggle: Kaggle hosts machine learning competitions where people compete to create the fastest, most efficient, most predictive algorithms. A byproduct of these competitions is a host of fascinating datasets that are generally free and open to the public. See, for example, the European Soccer Database, the Salem Witchcraft Dataset or results from an Oreo flavors taste test.\n 360Giving: Dozens of British foundations follow a standard file format for sharing grant data and have made that data available online.\n US City Open Data Census: More than 100 US cities have committed to sharing dozens of types of data, including data about crime, budgets, campaign finance, lobbying, transit, and zoning. This site from the Sunlight Foundation and Code for America collects this data and rates cities by how well they’re doing.\n Political science and economics datasets: There’s a wealth of data available for political science- and economics-related topics:\n François Briatte’s extensive curated lists: Includes data from/about intergovernmental organizations (IGOs), nongovernmental organizations (NGOs), public opinion surveys, parliaments and legislatures, wars, human rights, elections, and municipalities. Thomas Leeper’s list of political science datasets: Good short list of useful datasets, divided by type of data (country-level data, survey data, social media data, event data, text data, etc.). Erik Gahner’s list of political science datasets: Huge list of useful datasets, divided by topic (governance, elections, policy, political elites, etc.)   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"2210aa8aeb5724b04bdf63d813d61030","permalink":"https://evalf20.classes.andrewheiss.com/resource/data/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/data/","section":"resource","summary":"There are a ton of places to find data related to public policy and administration (as well as data on pretty much any topic you want) online:\n Data is Plural newsletter: Jeremy Singer-Vine sends a weekly newsletter of the most interesting public datasets he’s found. You should subscribe to it. He also has an archive of all the datasets he’s highlighted.\n Google Dataset Search: Google indexes thousands of public datasets; search for them here.","tags":null,"title":"Data","type":"docs"},{"authors":null,"categories":null,"content":"  ","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"24a52df01efb615d8b4c5c0f16afc27e","permalink":"https://evalf20.classes.andrewheiss.com/resource/regtables/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/resource/regtables/","section":"resource","summary":"  ","tags":null,"title":"Side-by-side regression tables","type":"docs"},{"authors":null,"categories":null,"content":"  You can download a BibTeX file of all the non-web-based readings in the course:\n  references.bib  You can open the file in BibDesk on macOS, JabRef on Windows, or Zotero or Mendeley online.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"68be32a8da6a38dd54a9e724ab3904a0","permalink":"https://evalf20.classes.andrewheiss.com/resource/citations/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/citations/","section":"resource","summary":"You can download a BibTeX file of all the non-web-based readings in the course:\n  references.bib  You can open the file in BibDesk on macOS, JabRef on Windows, or Zotero or Mendeley online.","tags":null,"title":"Citations and bibliography","type":"docs"},{"authors":null,"categories":null,"content":"   Resources Suggested outline  Introduction Program overview Program theory and implementation Outcome and causation Data and methods Synthetic analysis Conclusion    Evaluation research is tricky and costly. If you begin an intervention or launch a study prematurely, you can waste time and money—and potentially lives.\nEven if you have a well designed program with an impeccable logic model and a perfect DAG, you might discover (too late!) that you forgot to collect some critical variables or realize that your identification strategy will not work.\nFrom a more cynical perspective, you might (unethically) engage in the practice of p-hacking—running all sorts of different model specifications until you find the results you want, and then claim in your report that you had intended to run that model all along.\nOne increasingly popular method for (1) ensuring that your data and methods work before launching a study or intervention, and (2) declaring and committing to your hypotheses and methods and models before analyzing your data is to pre-register your research or evaluation. A pre-registered study contains all the background work—an introduction, literature review, theory, hypotheses, and proposed analysis—but without the actual data. Authors post their expectations and hypotheses publicly so they can be held publicly accountable for any deviations from their proposed design.1\nThe best preregistered studies use simulated data that has the same structure as the data that will be collected (i.e. same columns, sometimes the same correlations and relationships researchers expect to see in the collected data, etc.). Because there’s no data yet (or just fake data), you have more freedom when developing a preregistered study. You can experiment with different models, play with different approaches, manipulate data in different ways, and so on. If you realize that you need a new variable, or that you need to rearrange questions on a survey, or make any other kinds of changes, you can—you haven’t collected the data yet!\n(Additionally, using synthetic data is extremely useful if you’re working with proprietary or private data that you cannot make public. You can make a synthetic version of the real data instead; see this too.)\nOnce you finalize your plan and know all the data you need to collect, and once you’ve written out the different models you’ll run, all you have to do is collect the real data, plop it into your script (replacing the fake data you’d been using), and run the analysis script again to generate the actual, real results. In the results section, you get to either say “As predicted, we found…”, or “Contrary to expectations, we found that…”.\nFor your final project in this class, you will write a pre-registered analysis of a public or nonprofit social program that you’re interested in. You don’t need to worry about collecting data—you’ll create a synthetic dataset for your pre-analysis.\nYou will submit three things via iCollege:\nA PDF of your preregistered report (see the outline below for details of what this needs to contain). You should compile this with R Markdown. You might want to write the prose-heavy sections in a word processor like Word or Google Docs and copy/paste the text into your R Markdown document, since RStudio doesn’t have a nice spell checker or grammar checker. This should have no visible R code, warnings, or messages in it (set echo = FALSE at the beginning of your document before you knit). The same PDF as above, but with all the R code in it (set echo = TRUE at the beginning of your document and reknit the file). A CSV file of your fake data.  This project is due by 7:00 PM on Monday, December 14, 2020. No late work will be accepted.\nYou can either run the analysis in RStudio locally on your computer (highly recommended(!!), since you won’t have to worry about keeping all your work on RStudio’s servers), or use an RStudio.cloud project. You can make a copy of this RStudio.cloud project—it doesn’t have anything in it, but I have preinstalled all the packages we’ve used over the course of the semester, so you don’t have to.\n  Empty RStudio.cloud project  Empty RStudio project you can download and unzip on your computer (doesn’t include any packages, since you’re responsible for installing those)  Resources Most importantly, do not hesitate to work with classmates. You all must choose different programs, but you can work in groups of up to 4 people on your own projects. Also, absolutely do not hesitate to ask me questions! I’m here to help!\nYou might find this evaluation (and its proposal) of a truancy program in the Provo School District in Utah helpful as an example for the first half of this assignment (program overview, theory, implementation, threats to validity, and outcomes). The PSD evaluation doesn’t have DAGs or fancy econometrics models like RCTs, diff-in-diff, RDD, IVs, or anything like that, so you can’t use it as an example of that part, but these should provide a good template for the program-specific sections. This is longer than expected for this class. I provide suggested word counts in the outline below.\n  psd-proposal-2011  psd-final-report-2012   Suggested outline Here’s an outline of what you’ll need to do. You did lots of this work in your evaluation assignments. Please don’t just copy/paste those assignments as is into this final project—you’ll want to polish it up for this final report. You can download this as an RMarkdown file and change the text if you want. I’ve also included this as an RMarkdown file in the empty RStudio.cloud project.\n  final-project-template.Rmd  Introduction Describe the motivation for this evaluation, briefly describe the program to be evaluated, and explain why it matters for society. (≈150 words)\n Program overview Provide in-depth background about the program. Include details about (1) when it was started, (2) why it was started, (3) what it was designed to address in society. If the program hasn’t started yet, explain why it’s under consideration. (≈300 words)\n Program theory and implementation Program theory and impact theory graph Explain and explore the program’s underlying theory. Sometimes programs will explain why they exist in a mission statement, but often they don’t and you have to infer the theory from what the program looks like when implemented. What did the program designers plan on occurring? Was this theory based on existing research? If so, cite it. (≈300 words)\nInclude a simple impact theory graph showing the program’s basic activities and outcomes. Recall from class and your reading that this is focused primarily on the theory and mechanisms, not on the implementation of the program.\n Logic model Describe the program’s inputs, activities, outputs, and outcomes. Pay careful attention to how they are linked—remember that every input needs to flow into an activity and every output must flow out of an activity. (≈150 words)\nUse flowchart software to connect the inputs, activities, outputs, and outcomes and create a complete logic model. Include this as a figure.\n  Outcome and causation Main outcome Select one of the program’s outcomes to evaluate. Explain why you’ve chosen this (is it the most important? easiest to measure? has the greatest impact on society?) (≈50 words)\n Measurement Using the concept of the “ladder of abstraction” that we discussed in class (e.g. identifying a witch, measuring poverty, etc.), make a list of all the possible attributes of the outcome. Narrow this list down to 3-4 key attributes. Discuss how you decided to narrow the concepts and justify why you think these attributes capture the outcome. Then, for each of these attributes, answer these questions:\n Measurable definition: How would you specifically define this attribute? (i.e. if the attribute is “reduced crime”, define it as “The percent change in crime in a specific neighborhood during a certain time frame” or something similar) Ideal measurement: How would you measure this attribute in an ideal world? Feasible measurement: How would you measure this given reality and given limitations in budget, time, etc.? Measurement of program effect: How would to connect this measure to people in the program? How would you check to see if the program itself had an effect?  (≈150 words in this section)\n Causal theory Given your measurement approach, describe and draw a causal diagram (DAG) that shows how your program causes the outcome. Note that this is not the same thing as the logic model—you’ll likely have nodes in the DAG that aren’t related to the program at all (like socioeconomic status, gender, experience, or other factors). The logic model provides the framework for the actual implementation of your program and connects all the moving parts to the outcomes. The DAG is how you can prove causation with statistical approaches. (≈150 words)\n Hypotheses Make predictions of your program’s effect. Declare what you think will happen. (≈50 words)\n  Data and methods Identification strategy How will you measure the actual program effect? Will you rely on an RCT? Differences-in-differences? Regression discontinuity? Instrumental variables? How does your approach account for selection bias and endogeneity? How does your approach isolate the causal effect of the program on the outcome?\nAlso briefly describe what kinds of threats to internal and external validity you face in your study.\n(≈300 words)\n Data Given your measurement approach, limits on feasibility, and identification strategy, describe the data you will use. Will you rely on administrative data collected by a government agency or nonprofit? Will you collect your own data? If so, what variables will you measure, and how? Will you conduct a survey or rely on outside observers or do something else? What does this data look like? What variables does it (or should it) include?\n(≈100 words)\n  Synthetic analysis Generate a synthetic (fake) dataset in R with all the variables you’ll need for the real life analysis. Analyze the data using your identification strategy. For instance:\n If you’re relying on observational data, close all the backdoors with matching or inverse probability weighting, don’t adjust for colliders, and make a strong argument for isolation of the causal effect in the absence of treatment/control groups If you’re doing an RCT, test the differences in means in the treatment and control groups (and follow all other best practices listed in the World Bank book, checking for balance across groups, etc.) If you’re doing diff-in-diff, run a regression model with an interaction term to show the diff-in-diff If you’re doing regression discontinuity, check for a jump in the outcome variable at the cutoff in the running variable If you’re using instrumental variables, check the validity of your instrument and run a 2SLS model  Include robustness checks to ensure the validity of your effect (i.e. if you’re doing regression discontinuity, test different bandwidths and kernel types; etc.)\n(As many words as you need to fully describe your analysis and results)\n Conclusion What would the findings from this analysis mean for your selected program? What would it mean if you found an effect? What would it mean if you didn’t find an effect? Why does any of this matter? (≈75 words)\n   See the Center for Open Science’s directory of preregistrations, or AsPredicted list for examples of this in real life. Here’s one by me!↩︎\n   ","date":1607904000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"8d16837a0c729f9c31150a71deaf1f1e","permalink":"https://evalf20.classes.andrewheiss.com/assignment/final-project/","publishdate":"2020-12-14T00:00:00Z","relpermalink":"/assignment/final-project/","section":"assignment","summary":"Resources Suggested outline  Introduction Program overview Program theory and implementation Outcome and causation Data and methods Synthetic analysis Conclusion    Evaluation research is tricky and costly. If you begin an intervention or launch a study prematurely, you can waste time and money—and potentially lives.\nEven if you have a well designed program with an impeccable logic model and a perfect DAG, you might discover (too late!","tags":null,"title":"Final project","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Causal inference and data science Storytelling Ethics  Slides Videos   Readings This looks like a lot, but most of these are quite short!\nCausal inference and data science   Miguel A. Hernán, “The C-Word: Scientific Euphemisms Do Not Improve Causal Inference From Observational Data” Miguel A. Hernán, “The c-Word: Scientific Euphemisms Do Not Improve Causal Inference from Observational Data,” American Journal of Publich Health 108, no. 5 (May 2018): 616–19, doi:10.2105/AJPH.2018.304337.  Hannah Fresques and Meg Marco, “‘Your Default Position Should Be Skepticism’ and Other Advice for Data Journalists From Hadley Wickham,” ProPublica, June 10, 2019   Storytelling   Chapter 14 in Impact Evaluation in Practice Paul J. Gertler et al., Impact Evaluation in Practice, 2nd ed. (Inter-American Development Bank; World Bank, 2016), https://openknowledge.worldbank.org/handle/10986/25030.  Martin Krzywinski and Alberto Cairo, “Storytelling”  Ben Wellington, “Making data mean more through storytelling”  Will Schoder, “Every Story is the Same”   Ethics Keep in mind throughout all these readings that an “algorithm” in these contexts is typically some fancy type of regression model where the outcome variable is something binary like “Safe babysitter/unsafe babysitter,” “Gave up seat in past/didn’t give up seat in past”, or “Violated probation in past/didn’t violate probation in past”, and the explanatory variables are hundreds of pieces of data that might predict those outcomes (social media history, flight history, race, etc.).\nData scientists build a (sometimes proprietary and complex) model based on existing data, plug in values for any given new person, multiply that person’s values by the coefficients in the model, and get a final score in the end for how likely someone is to be a safe babysitter or how likely someone is to return to jail.\n  DJ Patil, “A Code of Ethics for Data Science” (if your’re interested in this, also check out Mike Loukides, Hilary Mason, and DJ Patil, Ethics and Data Science  “AI in 2018: A Year in Review”  “How Big Data Is ‘Automating Inequality’”  “In ‘Algorithms of Oppression,’ Safiya Noble finds old stereotypes persist in new media”  99% Invisible, “The Age of the Algorithm”: Note that this is a podcast, or a 20ish minute audio story. Listen to this.  On the Media, “Biased Algorithms, Biased World”  “Wanted: The ‘perfect babysitter.’ Must pass AI scan for respect and attitude.”  “Companies are on the hook if their hiring algorithms are biased”  “Courts use algorithms to help determine sentencing, but random people get the same results”  David Heinemeier Hansson’s rant on the Apple Card  And Jamie Heinemeier Hansson’s response     Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nIntroduction  What did we just learn?  Ethics of data analyitcs (a)  Ethics of data analytics (b)  Ethics of data analytics (c)  Ethics of storytelling (a)  Ethics of storytelling (b)  Ethics of storytelling (c)  Ethics of storytelling (d)  Curiosity                                  Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction What did we just learn? Ethics of data analyitcs (a) Ethics of data analytics (b) Ethics of data analytics (c) Ethics of storytelling (a) Ethics of storytelling (b) Ethics of storytelling (c) Ethics of storytelling (d) Curiosity  You can also watch the playlist (and skip around to different sections) here:\n   ","date":1606694400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"6c1d7292f14fb9f873bbf517670d292d","permalink":"https://evalf20.classes.andrewheiss.com/content/14-content/","publishdate":"2020-11-30T00:00:00Z","relpermalink":"/content/14-content/","section":"content","summary":"Readings  Causal inference and data science Storytelling Ethics  Slides Videos   Readings This looks like a lot, but most of these are quite short!\nCausal inference and data science   Miguel A. Hernán, “The C-Word: Scientific Euphemisms Do Not Improve Causal Inference From Observational Data” Miguel A. Hernán, “The c-Word: Scientific Euphemisms Do Not Improve Causal Inference from Observational Data,” American Journal of Publich Health 108, no.","tags":null,"title":"Ethics, stories, and curiosity","type":"docs"},{"authors":null,"categories":null,"content":"  This assignment will give you practice generating synthetic data and building in causal effects.\nThese two examples will be incredibly helpful:\n Generating random numbers The ultimate guide to generating synthetic data for causal inference  You’ll be doing all your R work in R Markdown. You can download a zipped file of a pre-made project here:\n  problem-set-9.zip  And as always, if you’re struggling, please talk to me. Work with classmates too (especially for this assignment!). Don’t suffer in silence!\nInstructions If you’re using R on your own computer, download this file, unzip it, and double click on the file named problem-set-9.Rproj:  problem-set-9.zip\nYou’ll need to make sure you have these packages installed on your computer: tidyverse, broom, ggdag, and scales. If you try to load one of those packages with library(tidyverse) or library(ggdag), etc., and R gives an error that the package is missing, use the “Packages” panel in RStudio to install it.\n(Alternatively, you can open the project named “Problem Set 9” on RStudio.cloud and complete the assignment in your browser without needing to install anything. This link should take you to the project—if it doesn’t, log in and look for the project named “Problem Set 9.”)\n Rename the R Markdown file named your-name_problem-set-9.Rmd to something that matches your name and open it in RStudio.\n Complete the tasks given in the R Markdown file. You can remove any of the question text if you want.\nYou can definitely copy, paste, and adapt from other code in the document or the example guide—don’t try to write everything from scratch!.\nYou’ll need to insert your own code chunks. Rather than typing them by hand (that’s tedious!), use the “Insert” button at the top of the editing window, or press ⌥ + ⌘ + I on macOS, or ctrl + alt + I on Windows.\nRemember that you can run an entire chunk by clicking on the green play arrow in the top right corner of the chunk. You can also run lines of code line-by-line if you place your cursor on some R code and press ⌘ + enter (for macOS users) or ctrl + enter (for Windows users).\nMake sure you run each chunk sequentially. If you run a chunk in the middle of the document without running previous ones, it might not work, since previous chunks might do things that later chunks depend on.\n When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex) of your document. Upload that file to iCollege.\n   ","date":1606694400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"740a5cce8b2bc6fc3b5b61f8edb5ff83","permalink":"https://evalf20.classes.andrewheiss.com/assignment/09-problem-set/","publishdate":"2020-11-30T00:00:00Z","relpermalink":"/assignment/09-problem-set/","section":"assignment","summary":"This assignment will give you practice generating synthetic data and building in causal effects.\nThese two examples will be incredibly helpful:\n Generating random numbers The ultimate guide to generating synthetic data for causal inference  You’ll be doing all your R work in R Markdown. You can download a zipped file of a pre-made project here:\n  problem-set-9.zip  And as always, if you’re struggling, please talk to me.","tags":null,"title":"Problem set 9","type":"docs"},{"authors":null,"categories":null,"content":"   Readings Slides Videos   Readings   “Generating random numbers”  “The ultimate guide to generating synthetic data for causal inference”  Paul Hünermund and Beyers Louw, “On the Nuisance of Control Variables in Regression Analysis” (October 1, 2020), https://arxiv.org/abs/2005.10314. (click on the PDF link in the right sidebar)  “Types of Evaluation,” National Center for HIV/AIDS, Viral Hepatitis, STD, and TB Prevention, Centers for Disease Control (CDC)  Chapters 11–13 in Impact Evaluation in Practice Paul J. Gertler et al., Impact Evaluation in Practice, 2nd ed. (Inter-American Development Bank; World Bank, 2016), https://openknowledge.worldbank.org/handle/10986/25030.   Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nIntroduction  Types of evaluations  Model- and design-based inference  Ethics and open science                Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Types of evaluations Model- and design-based inference Ethics and open science  You can also watch the playlist (and skip around to different sections) here:\n   ","date":1605484800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"5b7a67b9a0df14e8aac6e2750a796e6e","permalink":"https://evalf20.classes.andrewheiss.com/content/13-content/","publishdate":"2020-11-16T00:00:00Z","relpermalink":"/content/13-content/","section":"content","summary":"Readings Slides Videos   Readings   “Generating random numbers”  “The ultimate guide to generating synthetic data for causal inference”  Paul Hünermund and Beyers Louw, “On the Nuisance of Control Variables in Regression Analysis” (October 1, 2020), https://arxiv.org/abs/2005.10314. (click on the PDF link in the right sidebar)  “Types of Evaluation,” National Center for HIV/AIDS, Viral Hepatitis, STD, and TB Prevention, Centers for Disease Control (CDC)  Chapters 11–13 in Impact Evaluation in Practice Paul J.","tags":null,"title":"Choosing and planning ethical evaluations","type":"docs"},{"authors":null,"categories":null,"content":"  This assignment is a review of all the causal inference methods we’ve learned this semester: RCTs, matching and inverse probability weighting, diff-in-diff, RDD, and IVs. Refer to your past assignments and all the examples for guidance:\n RCTs: Example + Problem Set 3 Matching and inverse probability weighting: Example + Problem Set 3 Diff-in-diff: Example + Problem Sets 4 and 5 RDD: Example + Problem Set 6 Instrumental variables: Example + Problem Set 7  You’ll be doing all your R work in R Markdown. You can download a zipped file of a pre-made project here:\n  problem-set-8.zip  And as always, if you’re struggling, please talk to me. Work with classmates too (especially for this assignment!). Don’t suffer in silence!\nInstructions If you’re using R on your own computer, download this file, unzip it, and double click on the file named problem-set-8.Rproj:  problem-set-8.zip\nYou’ll need to make sure you have these packages installed on your computer: tidyverse, broom, estimatr, modelsummary, MatchIt, rdrobust, rddensity, and haven. If you try to load one of those packages with library(tidyverse) or library(MatchIt), etc., and R gives an error that the package is missing, use the “Packages” panel in RStudio to install it.\n(Alternatively, you can open the project named “Problem Set 8” on RStudio.cloud and complete the assignment in your browser without needing to install anything. This link should take you to the project—if it doesn’t, log in and look for the project named “Problem Set 8.”)\n Rename the R Markdown file named your-name_problem-set-8.Rmd to something that matches your name and open it in RStudio.\n Complete the tasks given in the R Markdown file. You can remove any of the question text if you want.\nYou can definitely copy, paste, and adapt from other code in the document or the different example pages—don’t try to write everything from scratch!.\nYou’ll need to insert your own code chunks. Rather than typing them by hand (that’s tedious!), use the “Insert” button at the top of the editing window, or press ⌥ + ⌘ + I on macOS, or ctrl + alt + I on Windows.\nRemember that you can run an entire chunk by clicking on the green play arrow in the top right corner of the chunk. You can also run lines of code line-by-line if you place your cursor on some R code and press ⌘ + enter (for macOS users) or ctrl + enter (for Windows users).\nMake sure you run each chunk sequentially. If you run a chunk in the middle of the document without running previous ones, it might not work, since previous chunks might do things that later chunks depend on.\n When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex) of your document. Upload that file to iCollege.\n   ","date":1605484800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"886eafe889ebe564eaba0f371a06b0e3","permalink":"https://evalf20.classes.andrewheiss.com/assignment/08-problem-set/","publishdate":"2020-11-16T00:00:00Z","relpermalink":"/assignment/08-problem-set/","section":"assignment","summary":"This assignment is a review of all the causal inference methods we’ve learned this semester: RCTs, matching and inverse probability weighting, diff-in-diff, RDD, and IVs. Refer to your past assignments and all the examples for guidance:\n RCTs: Example + Problem Set 3 Matching and inverse probability weighting: Example + Problem Set 3 Diff-in-diff: Example + Problem Sets 4 and 5 RDD: Example + Problem Set 6 Instrumental variables: Example + Problem Set 7  You’ll be doing all your R work in R Markdown.","tags":null,"title":"Problem set 8","type":"docs"},{"authors":null,"categories":null,"content":"   Readings Slides Videos   Readings  The example page on complier average causal effects shows how to use R to disentangle complier average causal effects The example page on fuzzy regression discontinuity shows how to use R to use instrumental variables in fuzzy regression discontinuity, both parametrically and nonparametrically   Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nIntroduction  Treatment effects and compliance  Randomized promotion  Fuzzy regression discontinuity                Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Treatment effects and compliance Randomized promotion Fuzzy regression discontinuity  You can also watch the playlist (and skip around to different sections) here:\n   ","date":1604880000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"9eee4da015dc05af8a4bee2b0e8794b9","permalink":"https://evalf20.classes.andrewheiss.com/content/12-content/","publishdate":"2020-11-09T00:00:00Z","relpermalink":"/content/12-content/","section":"content","summary":"Readings Slides Videos   Readings  The example page on complier average causal effects shows how to use R to disentangle complier average causal effects The example page on fuzzy regression discontinuity shows how to use R to use instrumental variables in fuzzy regression discontinuity, both parametrically and nonparametrically   Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later).","tags":null,"title":"Instrumental variables II + Regression discontinuity II","type":"docs"},{"authors":null,"categories":null,"content":"  For this problem set, you’ll practice using instrumental variables with both real and simulated data. This example page will be incredibly useful for you:\n Instrumental variables  You’ll be doing all your R work in R Markdown. You can download a zipped file of a pre-made project here:\n  problem-set-7.zip  And as always, if you’re struggling, please talk to me. Work with classmates too (especially for this assignment!). Don’t suffer in silence!\nInstructions If you’re using R on your own computer, download this file, unzip it, and double click on the file named problem-set-7.Rproj:  problem-set-7.zip\nYou’ll need to make sure you have these packages installed on your computer: tidyverse, broom, estimatr, and modelsummary. If you try to load one of those packages with library(tidyverse) or library(estimatr), etc., and R gives an error that the package is missing, use the “Packages” panel in RStudio to install it.\n(Alternatively, you can open the project named “Problem Set 7” on RStudio.cloud and complete the assignment in your browser without needing to install anything. This link should take you to the project—if it doesn’t, log in and look for the project named “Problem Set 7.”)\n Rename the R Markdown file named your-name_problem-set-7.Rmd to something that matches your name and open it in RStudio.\n Complete the tasks given in the R Markdown file. There are questions scattered throughout the document—your job is to answer those questions. You don’t need to put your answers in bold or ALL CAPS or anything, and you can remove the question text if you want.\nFill out code in the empty chunks provided (you can definitely copy, paste, and adapt from other code in the document or the example page on instrumental variables—don’t try to write everything from scratch!).\nYou’ll need to insert your own code chunks. Rather than typing them by hand (that’s tedious!), use the “Insert” button at the top of the editing window, or press ⌥ + ⌘ + I on macOS, or ctrl + alt + I on Windows.\nRemember that you can run an entire chunk by clicking on the green play arrow in the top right corner of the chunk. You can also run lines of code line-by-line if you place your cursor on some R code and press ⌘ + enter (for macOS users) or ctrl + enter (for Windows users).\nMake sure you run each chunk sequentially. If you run a chunk in the middle of the document without running previous ones, it might not work, since previous chunks might do things that later chunks depend on.\n When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex) of your document. Upload that file to iCollege.\n   ","date":1604880000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"7977a29170029ede9c580f61f64b581a","permalink":"https://evalf20.classes.andrewheiss.com/assignment/07-problem-set/","publishdate":"2020-11-09T00:00:00Z","relpermalink":"/assignment/07-problem-set/","section":"assignment","summary":"For this problem set, you’ll practice using instrumental variables with both real and simulated data. This example page will be incredibly useful for you:\n Instrumental variables  You’ll be doing all your R work in R Markdown. You can download a zipped file of a pre-made project here:\n  problem-set-7.zip  And as always, if you’re struggling, please talk to me. Work with classmates too (especially for this assignment!","tags":null,"title":"Problem set 7","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Instrumental variables  Slides Videos   Readings   Chapter 5 in Impact Evaluation in Practice Paul J. Gertler et al., Impact Evaluation in Practice, 2nd ed. (Inter-American Development Bank; World Bank, 2016), https://openknowledge.worldbank.org/handle/10986/25030.  Chapter 3 in Mastering ’Metrics Joshua D. Angrist and Jörn-Steffen Pischke, Mastering ’Metrics: The Path from Cause to Effect (Princeton, NJ: Princeton University Press, 2015).  “Instrumental variables” in Causal Inference: The Mixtape Scott Cunningham, Causal Inference: The Mixtape, 2018, https://www.scunning.com/mixtape.html.  Instrumental variables  The example page on instrumental variables shows how to use R to analyze and estimate causal effects with instrumental variables    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nIntroduction  Endogeneity and exogeneity  Instruments  Using instruments                Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Endogeneity and exogeneity Instruments Using instruments  You can also watch the playlist (and skip around to different sections) here:\n   ","date":1604275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"d9c34011e152a6099f13882fb144b965","permalink":"https://evalf20.classes.andrewheiss.com/content/11-content/","publishdate":"2020-11-02T00:00:00Z","relpermalink":"/content/11-content/","section":"content","summary":"Readings  Instrumental variables  Slides Videos   Readings   Chapter 5 in Impact Evaluation in Practice Paul J. Gertler et al., Impact Evaluation in Practice, 2nd ed. (Inter-American Development Bank; World Bank, 2016), https://openknowledge.worldbank.org/handle/10986/25030.  Chapter 3 in Mastering ’Metrics Joshua D. Angrist and Jörn-Steffen Pischke, Mastering ’Metrics: The Path from Cause to Effect (Princeton, NJ: Princeton University Press, 2015).  “Instrumental variables” in Causal Inference: The Mixtape Scott Cunningham, Causal Inference: The Mixtape, 2018, https://www.","tags":null,"title":"Instrumental variables I","type":"docs"},{"authors":null,"categories":null,"content":"  For this problem set, you’ll practice doing regression discontinuity analysis with simulated data from a hypothetical program. This example page will be incredibly useful for you:\n Regression discontinuity  You’ll be doing all your R work in R Markdown. You can download a zipped file of a pre-made project here:\n  problem-set-6.zip  And as always, if you’re struggling, please talk to me. Work with classmates too (especially for this assignment!). Don’t suffer in silence!\nInstructions If you’re using R on your own computer, download this file, unzip it, and double click on the file named problem-set-6.Rproj:  problem-set-6.zip\nYou’ll need to make sure you have these packages installed on your computer: tidyverse, broom, rdrobust, rddensity, and modelsummary. If you try to load one of those packages with library(tidyverse) or library(rdrobust), etc., and R gives an error that the package is missing, use the “Packages” panel in RStudio to install it.\n(Alternatively, you can open the project named “Problem Set 6” on RStudio.cloud and complete the assignment in your browser without needing to install anything. This link should take you to the project—if it doesn’t, log in and look for the project named “Problem Set 6.”)\n Rename the R Markdown file named your-name_problem-set-6.Rmd to something that matches your name and open it in RStudio.\n Complete the tasks given in the R Markdown file. There are questions marked in bold. Your job is to answer those questions. You don’t need to put your answers in bold or ALL CAPS or anything, and you can remove the question text if you want.\nFill out code in the empty chunks provided (you can definitely copy, paste, and adapt from other code in the document or the example page on regression discontinuity—don’t try to write everything from scratch!).\nYou’ll need to insert your own code chunks. Rather than typing them by hand (that’s tedious!), use the “Insert” button at the top of the editing window, or press ⌥ + ⌘ + I on macOS, or ctrl + alt + I on Windows.\nRemember that you can run an entire chunk by clicking on the green play arrow in the top right corner of the chunk. You can also run lines of code line-by-line if you place your cursor on some R code and press ⌘ + enter (for macOS users) or ctrl + enter (for Windows users).\nMake sure you run each chunk sequentially. If you run a chunk in the middle of the document without running previous ones, it might not work, since previous chunks might do things that later chunks depend on.\n When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex) of your document. Upload that file to iCollege.\n   ","date":1604275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"6e2e28be24f74db1a7d3e76a944f890d","permalink":"https://evalf20.classes.andrewheiss.com/assignment/06-problem-set/","publishdate":"2020-11-02T00:00:00Z","relpermalink":"/assignment/06-problem-set/","section":"assignment","summary":"For this problem set, you’ll practice doing regression discontinuity analysis with simulated data from a hypothetical program. This example page will be incredibly useful for you:\n Regression discontinuity  You’ll be doing all your R work in R Markdown. You can download a zipped file of a pre-made project here:\n  problem-set-6.zip  And as always, if you’re struggling, please talk to me. Work with classmates too (especially for this assignment!","tags":null,"title":"Problem set 6","type":"docs"},{"authors":null,"categories":null,"content":"    Seeds Distributions  Uniform distribution Normal distribution Truncated normal distribution Beta distribution Binomial distribution Poisson distribution  Rescaling numbers Summary Example   In your final project, you will generate a synthetic dataset and use it to conduct an evaluation of some social program. Generating fake or simulated data is an incredibly powerful skill, but it takes some practice. Here are a bunch of helpful resources and code examples of how to use different R functions to generate random numbers that follow specific distributions (or probability shapes).\nThis example focuses primarily on distributions. Each of the columns you’ll generate will be completely independent from each other and there will be no correlation between them. The example for generating synthetic data provides code and a bunch of examples of how to build in correlations between columns.\nFirst, make sure you load the libraries we’ll use throughout the example:\nlibrary(tidyverse) library(patchwork) Seeds When R (or any computer program, really) generates random numbers, it uses an algorithm to simulate randomness. This algorithm always starts with an initial number, or seed. Typically it will use something like the current number of milliseconds since some date, so that every time you generate random numbers they’ll be different. Look at this, for instance:\n# Choose 3 numbers between 1 and 10 sample(1:10, 3) ## [1] 9 4 7 # Choose 3 numbers between 1 and 10 sample(1:10, 3) ## [1] 5 6 9 They’re different both times.\nThat’s ordinarily totally fine, but if you care about reproducibility (like having a synthetic dataset with the same random values, or having jittered points in a plot be in the same position every time you knit), it’s a good idea to set your own seed. This ensures that the random numbers you generate are the same every time you generate them.\nDo this by feeding set.seed() some numbers. It doesn’t matter what number you use—it just has to be a whole number. People have all sorts of favorite seeds:\n 1 13 42 1234 12345 20201101 (i.e. the current date) 8675309  You could even go to random.org and use atmospheric noise to generate a seed, and then use that in R.\nHere’s what happens when you generate random numbers after setting a seed:\n# Set a seed set.seed(1234) # Choose 3 numbers between 1 and 10 sample(1:10, 3) ## [1] 10 6 5 # Set a seed set.seed(1234) # Choose another 3 numbers between 1 and 10 sample(1:10, 3) ## [1] 10 6 5 They’re the same!\nOnce you set a seed, it influences any function that does anything random, but it doesn’t reset. For instance, if you set a seed once and then run sample() twice, you’ll get different numbers the second time, but you’ll get the same different numbers every time:\n# Set a seed set.seed(1234) # Choose 3 numbers between 1 and 10 sample(1:10, 3) ## [1] 10 6 5 sample(1:10, 3) # This will be different! ## [1] 9 5 6 # Set a seed again set.seed(1234) # Choose 3 numbers between 1 and 10 sample(1:10, 3) ## [1] 10 6 5 sample(1:10, 3) # This will be different, but the same as before! ## [1] 9 5 6 Typically it’s easiest to just include set.seed(SOME_NUMBER) at the top of your script after you load all the libraries. Some functions have a seed argument, and it’s a good idea to use it: position_jitter(..., seed = 1234).\n Distributions Remember in elementary school when you’d decide on playground turns by saying “Pick a number between 1 and 10” and whoever was the closest would win? When you generate random numbers in R, you’re essentially doing the same thing, only with some fancier bells and whistles.\nWhen you ask someone to choose a number between 1 and 10, any of those numbers should be equally likely. 1 isn’t really less common than 5 or anything. In some situations, though, there are numbers that are more likely to appear than others (i.e. when you roll two dice, it’s pretty rare to get a 2, but pretty common to get a 7). These different kinds of likelihood change the shape of the distribution of possible values. There are hundreds of different distributions, but for the sake of generating data, there are only a few that you need to know.\nUniform distribution In a uniform distribution, every number is equally likely. This is the “pick a number between 1 and 10” scenario, or rolling a single die. There are a couple ways to work with a uniform distribution in R: (1) sample() and (2) runif().\nsample() The sample() function chooses an element from a list.\nFor instance, let’s pretend we have six possible numbers (like a die, or like 6 categories on a survey), like this:\npossible_answers \u0026lt;- c(1, 2, 3, 4, 5, 6) # We could also write this as 1:6 instead If we want to randomly choose from this list, you’d use sample(). The size argument defines how many numbers to choose.\n# Choose 1 random number sample(possible_answers, size = 1) ## [1] 4 # Choose 3 random numbers sample(possible_answers, size = 3) ## [1] 2 6 5 One important argument you can use is replace, which essentially puts the number back into the pool of possible numbers. Imagine having a bowl full of ping pong balls with the numbers 1–6 on them. If you take the number “3” out, you can’t draw it again. If you put it back in, you can pull it out again. The replace argument puts the number back after it’s drawn:\n# Choose 10 random numbers, with replacement sample(possible_answers, size = 10, replace = TRUE) ## [1] 6 4 6 6 6 4 4 5 4 3 If you don’t specify replace = TRUE, and you try to choose more numbers than are in the set, you’ll get an error:\n# Choose 8 numbers between 1 and 6, but don\u0026#39;t replace them. # This won\u0026#39;t work! sample(possible_answers, size = 8) ## Error in sample.int(length(x), size, replace, prob): cannot take a sample larger than the population when \u0026#39;replace = FALSE\u0026#39; It’s hard to see patterns in the outcomes when generating just a handful of numbers, but easier when you do a lot. Let’s roll a die 1,000 times:\nset.seed(1234) die \u0026lt;- tibble(value = sample(possible_answers, size = 1000, replace = TRUE)) die %\u0026gt;% count(value) ## # A tibble: 6 x 2 ## value n ## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 1 161 ## 2 2 153 ## 3 3 188 ## 4 4 149 ## 5 5 157 ## 6 6 192 ggplot(die, aes(x = value)) + geom_bar() + labs(title = \u0026quot;1,000 rolls of a single die\u0026quot;) In this case, 3 and 6 came up more often than the others, but that’s just because of randomness. If we rolled the die 100,000 times, the bars should basically be the same:\nset.seed(1234) die \u0026lt;- tibble(value = sample(possible_answers, size = 100000, replace = TRUE)) ggplot(die, aes(x = value)) + geom_bar() + labs(title = \u0026quot;100,000 rolls of a single die\u0026quot;)  runif() Another way to generate uniformly distributed numbers is to use the runif() function (which is short for “random uniform”, and which took me years to realize, and for years I wondered why people used a function named “run if” when there’s no if statement anywhere??)\nrunif() will choose numbers between a minimum and a maximum. These numbers will not be whole numbers. By default, the min and max are 0 and 1:\nrunif(5) ## [1] 0.09862 0.96294 0.88655 0.05623 0.44452 Here are 5 numbers between 35 and 56:\nrunif(5, min = 35, max = 56) ## [1] 46.83 42.89 37.75 53.22 46.13 Since these aren’t whole numbers, you can round them to make them look more realistic (like, if you were generating a column for age, you probably don’t want people who are 21.5800283 years old):\n# Generate 5 people between the ages of 18 and 35 round(runif(5, min = 18, max = 35), 0) ## [1] 21 28 33 34 31 You can confirm that each number has equal probability if you make a histogram. Here are 5,000 random people between 18 and 35:\nset.seed(1234) lots_of_numbers \u0026lt;- tibble(x = runif(5000, min = 18, max = 35)) ggplot(lots_of_numbers, aes(x = x)) + geom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;, boundary = 18)   Normal distribution The whole “choose a number between 1 and 10” idea of a uniform distribution is neat and conceptually makes sense, but most numbers that exist in the world tend to have higher probabilities around certain values—almost like gravity around a specific point. For instance, income in the United States is not uniformly distributed—a handful of people are really really rich, lots are very poor, and most are kind of clustered around an average.\nThe idea of having possible values clustered around an average is how the rest of these distributions work (uniform distributions don’t have any sort of central gravity point; all these others do). Each distribution is defined by different things called parameters, or values that determine the shape of the probabilities and locations of the clusters.\nA super common type of distribution is the normal distribution. This is the famous “bell curve” you learn about in earlier statistics classes. A normal distribution has two parameters:\nA mean (the center of the cluster) A standard deviation (how much spread there is around the mean).  In R, you can generate random numbers from a normal distribution with the rnorm() function. It takes three arguments: the number of numbers you want to generate, the mean, and the standard deviation. It defaults to a mean of 0 and a standard deviation of 1, which means most numbers will cluster around 0, with a lot between −1 and 1, and some going up to −2 and 2 (technically 67% of numbers will be between −1 and 1, while 95% of numbers will be between −2–2ish)\nrnorm(5) ## [1] -1.3662 0.5392 -1.3219 -0.2813 -2.1049 # Cluster around 10, with an SD of 4 rnorm(5, mean = 10, sd = 4) ## [1] 3.530 7.105 11.227 10.902 13.743 When working with uniform distributions, it’s easy to know how high or low your random values might go, since you specify a minimum and maximum number. With a normal distribution, you don’t specify starting and ending points—you specify a middle and a spread, so it’s harder to guess the whole range. Plotting random values is thus essential. Here’s 1,000 random numbers clustered around 10 with a standard deviation of 4:\nset.seed(1234) plot_data \u0026lt;- tibble(x = rnorm(1000, mean = 10, sd = 4)) head(plot_data) ## # A tibble: 6 x 1 ## x ## \u0026lt;dbl\u0026gt; ## 1 5.17 ## 2 11.1 ## 3 14.3 ## 4 0.617 ## 5 11.7 ## 6 12.0 ggplot(plot_data, aes(x = x)) + geom_histogram(binwidth = 1, boundary = 0, color = \u0026quot;white\u0026quot;) Neat. Most numbers are around 10; lots are between 5 and 15; some go as high as 25 and as low as −5.\nWatch what happens if you change the standard deviation to 10 to make the spread wider:\nset.seed(1234) plot_data \u0026lt;- tibble(x = rnorm(1000, mean = 10, sd = 10)) head(plot_data) ## # A tibble: 6 x 1 ## x ## \u0026lt;dbl\u0026gt; ## 1 -2.07 ## 2 12.8 ## 3 20.8 ## 4 -13.5 ## 5 14.3 ## 6 15.1 ggplot(plot_data, aes(x = x)) + geom_histogram(binwidth = 1, boundary = 0, color = \u0026quot;white\u0026quot;) It’s still centered around 10, but now you get values as high as 40 and as low as −20. The data is more spread out now.\nWhen simulating data, you’ll most often use a normal distribution just because it’s easy and lots of things follow that pattern in the real world. Incomes, ages, education, etc. all have a kind of gravity to them, and a normal distribution is a good way of showing that gravity. For instance, here are 1,000 simulated people with reasonable random incomes, ages, and years of education:\nset.seed(1234) fake_people \u0026lt;- tibble(income = rnorm(1000, mean = 40000, sd = 15000), age = rnorm(1000, mean = 25, sd = 8), education = rnorm(1000, mean = 16, sd = 4)) head(fake_people) ## # A tibble: 6 x 3 ## income age education ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 21894. 15.4 12.1 ## 2 44161. 27.4 15.6 ## 3 56267. 12.7 15.6 ## 4 4815. 30.1 20.8 ## 5 46437. 30.6 9.38 ## 6 47591. 9.75 11.8 fake_income \u0026lt;- ggplot(fake_people, aes(x = income)) + geom_histogram(binwidth = 5000, color = \u0026quot;white\u0026quot;, boundary = 0) + labs(title = \u0026quot;Simulated income\u0026quot;) fake_age \u0026lt;- ggplot(fake_people, aes(x = age)) + geom_histogram(binwidth = 2, color = \u0026quot;white\u0026quot;, boundary = 0) + labs(title = \u0026quot;Simulated age\u0026quot;) fake_education \u0026lt;- ggplot(fake_people, aes(x = education)) + geom_histogram(binwidth = 2, color = \u0026quot;white\u0026quot;, boundary = 0) + labs(title = \u0026quot;Simulated education\u0026quot;) fake_income + fake_age + fake_education These three columns all have different centers and spreads. Income is centered around $45,000, going up to almost $100,000 and as low as −$10,000; age is centered around 25, going as low as 0 and as high as 50; education is centered around 16, going as low as 3 and as high as 28. Cool.\nAgain, when generating these numbers, it’s really hard to know how high or low these ranges will be, so it’s a good idea to plot them constantly. I settled on sd = 4 for education only because I tried things like 1 and 10 and got wild looking values (everyone basically at 16 with little variation, or everyone ranging from −20 to 50, which makes no sense when thinking about years of education). Really it’s just a process of trial and error until the data looks good and reasonable.\n Truncated normal distribution Sometimes you’ll end up with negative numbers that make no sense. Look at income in the plot above, for instance. Some people are earning −$10,000 year. The rest of the distribution looks okay, but those negative values are annoying.\nTo fix this, you can use something called a truncated normal distribution, which lets you specify a mean and standard deviation, just like a regular normal distribution, but also lets you specify a minimum and/or maximum so you don’t get values that go too high or too low.\nR doesn’t have a truncated normal function built-in, but you can install the truncnorm package and use the rtruncnorm() function. A truncated normal distribution has four parameters:\nA mean (mean) A standard deviation (sd) A minimum (optional) (a) A maximum (optional) (b)  For instance, let’s pretend you have a youth program designed to target people who are between 12 and 21 years old, with most around 14. You can generate numbers with a mean of 14 and a standard deviation of 5, but you’ll create people who are too old, too young, or even negatively aged!\nset.seed(1234) plot_data \u0026lt;- tibble(fake_age = rnorm(1000, mean = 14, sd = 5)) head(plot_data) ## # A tibble: 6 x 1 ## fake_age ## \u0026lt;dbl\u0026gt; ## 1 7.96 ## 2 15.4 ## 3 19.4 ## 4 2.27 ## 5 16.1 ## 6 16.5 ggplot(plot_data, aes(x = fake_age)) + geom_histogram(binwidth = 2, color = \u0026quot;white\u0026quot;, boundary = 0) To fix this, truncate the range at 12 and 21:\nlibrary(truncnorm) # For rtruncnorm() set.seed(1234) plot_data \u0026lt;- tibble(fake_age = rtruncnorm(1000, mean = 14, sd = 5, a = 12, b = 21)) head(plot_data) ## # A tibble: 6 x 1 ## fake_age ## \u0026lt;dbl\u0026gt; ## 1 15.4 ## 2 19.4 ## 3 16.1 ## 4 16.5 ## 5 14.3 ## 6 18.8 ggplot(plot_data, aes(x = fake_age)) + geom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;, boundary = 0) And voila! A bunch of people between 12 and 21, with most around 14, with no invalid values.\n Beta distribution Normal distributions are neat, but they’re symmetrical around the mean (unless you truncate them). What if your program involves a test with a maximum of 100 points where most people score around 85, but a sizable portion score below that. In other words, it’s not centered at 85, but is skewed left.\nTo simulate this kind of distribution, we can use a Beta distribution. Beta distributions are neat because they naturally only range between 0 and 1—they’re perfect for things like percentages or proportions or or 100-based exams.\nUnlike a normal distribution, where you use the mean and standard deviation as parameters, Beta distributions take two non-intuitive parameters:\nshape1 shape2  What the heck are these shapes though?! This answer at Cross Validated does an excellent job of explaining the intuition behind Beta distributions and it’d be worth it to read it.\nBasically, Beta distributions are good at modeling probabilities of things, and shape1 and shape2 represent specific parts of a probability formula.\nLet’s say that there’s an exam with 10 points where most people score a 6/10. Another way to think about this is that an exam is a collection of correct answers and incorrect answers, and that the percent correct follows this equation:\n\\[ \\frac{\\text{Number correct}}{\\text{Number correct} + \\text{Number incorrect}} \\]\nIf you scored a 6, you could write that as:\n\\[ \\frac{6}{6 + 4} \\]\nTo make it more general, we can use Greek variable names: \\(\\alpha\\) for the number correct and \\(\\beta\\) for the number incorrect, leaving us with this:\n\\[ \\frac{\\alpha}{\\alpha + \\beta} \\]\nNeat.\nIn a Beta distribution, the \\(\\alpha\\) and \\(\\beta\\) in that equation correspond to shape1 and shape2. If we want to generate random scores for this test where most people get 6/10, we can use rbeta():\nset.seed(1234) plot_data \u0026lt;- tibble(exam_score = rbeta(1000, shape1 = 6, shape2 = 4)) %\u0026gt;% # rbeta() generates numbers between 0 and 1, so multiply everything by 10 to # scale up the exam scores mutate(exam_score = exam_score * 10) ggplot(plot_data, aes(x = exam_score)) + geom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;) + scale_x_continuous(breaks = 0:10) Most people score around 6, with a bunch at 5 and 7, and fewer in the tails. Importantly, it’s not centered at 6—the distribution is asymmetric.\nThe magic of—and most confusing part about—Beta distributions is that you can get all sorts of curves by just changing the shape parameters. To make this easier to see, we can make a bunch of different Beta distributions. Instead of plotting them with histograms, we’ll use density plots (and instead of generating random numbers, we’ll plot the actual full range of the distribution (that’s what dbeta and geom_function() do in all these examples)).\nHere’s what we saw before, with \\(\\alpha\\) (shape1) = 6 and \\(\\beta\\) (shape2) = 4:\nggplot() + geom_function(fun = dbeta, args = list(shape1 = 6, shape2 = 4)) Again, there’s a peak at 0.6 (or 6), which is what we expected.\nWe can make the distribution narrower if we scale the shapes up. Here pretty much everyone scores around 50% and 75%.\nggplot() + geom_function(fun = dbeta, args = list(shape1 = 60, shape2 = 40)) So far all these curves look like normal distributions, just slightly skewed. But when if most people score 90–100%? Or most fail? A Beta distribution can handle that too:\nggplot() + geom_function(fun = dbeta, args = list(shape1 = 9, shape2 = 1), color = \u0026quot;blue\u0026quot;) + geom_function(fun = dbeta, args = list(shape1 = 1, shape2 = 9), color = \u0026quot;red\u0026quot;) With shape1 = 9 and shape2 = 1 (or \\(\\frac{9}{9 + 1}\\)) we get most around 90%, while shape1 = 1 and shape2 = 9 (or \\(\\frac{1}{1 + 9}\\)) gets us most around 10%.\nCheck out all these other shapes too:\nggplot() + geom_function(fun = dbeta, args = list(shape1 = 5, shape2 = 5), color = \u0026quot;blue\u0026quot;) + geom_function(fun = dbeta, args = list(shape1 = 2, shape2 = 5), color = \u0026quot;red\u0026quot;) + geom_function(fun = dbeta, args = list(shape1 = 80, shape2 = 23), color = \u0026quot;orange\u0026quot;) + geom_function(fun = dbeta, args = list(shape1 = 13, shape2 = 17), color = \u0026quot;brown\u0026quot;) In real life, if I don’t want to figure out the math behind the \\(\\frac{\\alpha}{\\alpha + \\beta}\\) shape values, I end up just choosing different numbers until it looks like the shape I want, and then I use rbeta() with those parameter values. Like, how about we generate some numbers based on the red line above, with shape1 = 2 and shape2 = 5, which looks like it should be centered around 0.2ish (\\(\\frac{2}{2 + 5} = 0.2857\\)):\nset.seed(1234) plot_data \u0026lt;- tibble(thing = rbeta(1000, shape1 = 2, shape2 = 5)) %\u0026gt;% mutate(thing = thing * 100) head(plot_data) ## # A tibble: 6 x 1 ## thing ## \u0026lt;dbl\u0026gt; ## 1 10.1 ## 2 34.5 ## 3 55.3 ## 4 2.19 ## 5 38.0 ## 6 39.9 ggplot(plot_data, aes(x = thing)) + geom_histogram(binwidth = 2, color = \u0026quot;white\u0026quot;, boundary = 0) It worked! Most values are around 20ish, but some go up to 60–80.\n Binomial distribution Often you’ll want to generate a column that only has two values: yes/no, treated/untreated, before/after, big/small, red/blue, etc. You’ll also likely want to control the proportions (25% treated, 62% blue, etc.). You can do this in two different ways: (1) sample() and (2) rbinom().\nsample() We already saw sample() when we talked about uniform distributions. To generate a binary variable with sample(), just feed it a list of two possible values:\nset.seed(1234) # Choose 5 random T/F values possible_things \u0026lt;- c(TRUE, FALSE) sample(possible_things, 5, replace = TRUE) ## [1] FALSE FALSE FALSE FALSE TRUE R will choose these values with equal/uniform probability by default, but you can change that in sample() with the prob argument. For instance, pretend you want to simulate an election. According to the latest polls, one candidate has an 80% chance of winning. You want to randomly choose a winner based on that chance. Here’s how to do that with sample():\nset.seed(1234) candidates \u0026lt;- c(\u0026quot;Person 1\u0026quot;, \u0026quot;Person 2\u0026quot;) sample(candidates, size = 1, prob = c(0.8, 0.2)) ## [1] \u0026quot;Person 1\u0026quot; Person 1 wins!\nIt’s hard to see the weighted probabilities when you just choose one, so let’s pretend there are 1,000 elections:\nset.seed(1234) fake_elections \u0026lt;- tibble(winner = sample(candidates, size = 1000, prob = c(0.8, 0.2), replace = TRUE)) fake_elections %\u0026gt;% count(winner) ## # A tibble: 2 x 2 ## winner n ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 Person 1 792 ## 2 Person 2 208 ggplot(fake_elections, aes(x = winner)) + geom_bar() Person 1 won 792 of the elections. Neat.\n(This is essentially what election forecasting websites like FiveThirtyEight do! They just do it with way more sophisticated simulations.)\n rbinom() Instead of using sample(), you can use a formal distribution called the binomial distribution. This distribution is often used for things that might have “trials” or binary outcomes that are like success/failure or yes/no or true/false\nThe binomial distribution takes two parameters:\nsize: The number of “trials”, or times that an event happens prob: The probability of success in each trial  It’s easiest to see some examples of this. Let’s say you have a program that has a 60% success rate and it is tried on groups of 20 people 5 times. The parameters are thus size = 20 (since there are twenty people per group) and prob = 0.6 (since there is a 60% chance of success):\nset.seed(1234) rbinom(5, size = 20, prob = 0.6) ## [1] 15 11 11 11 10 The results here mean that in group 1, 15/20 (75%) people had success, in group 2, 11/20 (55%) people had success, and so on. Not every group will have exactly 60%, but they’re all kind of clustered around that.\nHOWEVER, I don’t like using rbinom() like this, since this is all group-based, and when you’re generating fake people you generally want to use individuals, or groups of 1. So instead, I assume that size = 1, which means that each “group” is only one person large. This forces the generated numbers to either be 0 or 1:\nset.seed(1234) rbinom(5, size = 1, prob = 0.6) ## [1] 1 0 0 0 0 Here, only 1 of the 5 people were 1/TRUE/yes, which is hardly close to a 60% chance overall, but that’s because we only generated 5 numbers. If we generate lots, we can see the probability of yes emerge:\nset.seed(12345) plot_data \u0026lt;- tibble(thing = rbinom(2000, 1, prob = 0.6)) %\u0026gt;% # Make this a factor since it\u0026#39;s basically a yes/no categorical variable mutate(thing = factor(thing)) plot_data %\u0026gt;% count(thing) %\u0026gt;% mutate(proportion = n / sum(n)) ## # A tibble: 2 x 3 ## thing n proportion ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0 840 0.42 ## 2 1 1160 0.580 ggplot(plot_data, aes(x = thing)) + geom_bar() 58% of the 2,000 fake people here were 1/TRUE/yes, which is close to the goal of 60%. Perfect.\n  Poisson distribution One last common distribution that you might find helpful when simulating data is the Poisson distribution (in French, “poisson” = fish, but here it’s not actually named after the animal, but after French mathematician Siméon Denis Poisson).\nA Poisson distribution is special because it generates whole numbers (i.e. nothing like 1.432) that follow a skewed pattern (i.e. more smaller values than larger values). There’s all sorts of fancy math behind it that you don’t need to worry about so much—all you need to know is that it’s good at modeling things called Poisson processes.\nFor instance, let’s say you’re sitting at the front door of a coffee shop (in pre-COVID days) and you count how many people are in each arriving group. You’ll see something like this:\n 1 person 1 person 2 people 1 person 3 people 2 people 1 person  Lots of groups of one, some groups of two, fewer groups of three, and so on. That’s a Poisson process: a bunch of independent random events that combine into grouped events.\nThat sounds weird and esoteric (and it is!), but it reflects lots of real world phenomena, and things you’ll potentially want to measure in a program. For instance, the number of kids a family has follows a type of Poisson process. Lots of families have 1, some have 2, fewer have 3, even fewer have 4, and so on. The number of cars in traffic, the number of phone calls received by an office, arrival times in a line, and even the outbreak of wars are all examples of Poisson processes.\nYou can generate numbers from a Poisson distribution with the rpois() function in R. This distribution only takes a single parameter:\n lambda (\\(\\lambda\\))  The \\(\\lambda\\) value controls the rate or speed that a Poisson process increases (i.e. jumps from 1 to 2, from 2 to 3, from 3 to 4, etc.). I have absolutely zero mathematical intuition for how it works. The two shape parameters for a Beta distribution at least fit in a fraction and you can wrap your head around that, but the lambda in a Poisson distribution is just a mystery to me. So whenever I use a Poisson distribution for something, I just play with the lambda until the data looks reasonable.\nLet’s assume that the number of kids a family has follows a Poisson process. Here’s how we can use rpois() to generate that data:\nset.seed(123) # 10 different families rpois(10, lambda = 1) ## [1] 0 2 1 2 3 0 1 2 1 1 Cool. Most families have 0–1 kids; some have 2; one has 3.\nIt’s easier to see these patterns with a plot:\nset.seed(1234) plot_data \u0026lt;- tibble(num_kids = rpois(500, lambda = 1)) head(plot_data) ## # A tibble: 6 x 1 ## num_kids ## \u0026lt;int\u0026gt; ## 1 0 ## 2 1 ## 3 1 ## 4 1 ## 5 2 ## 6 1 plot_data %\u0026gt;% group_by(num_kids) %\u0026gt;% summarize(count = n()) %\u0026gt;% mutate(proportion = count / sum(count)) ## # A tibble: 6 x 3 ## num_kids count proportion ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0 180 0.36 ## 2 1 187 0.374 ## 3 2 87 0.174 ## 4 3 32 0.064 ## 5 4 11 0.022 ## 6 5 3 0.006 ggplot(plot_data, aes(x = num_kids)) + geom_bar() Here 75ish% of families have 0–1 kids (36% + 37.4%), 17% have 2 kids, 6% have 3, 2% have 4, and only 0.6% have 5.\nWe can play with the \\(\\lambda\\) to increase the rate of kids per family:\nset.seed(1234) plot_data \u0026lt;- tibble(num_kids = rpois(500, lambda = 2)) head(plot_data) ## # A tibble: 6 x 1 ## num_kids ## \u0026lt;int\u0026gt; ## 1 0 ## 2 2 ## 3 2 ## 4 2 ## 5 4 ## 6 2 plot_data %\u0026gt;% group_by(num_kids) %\u0026gt;% summarize(count = n()) %\u0026gt;% mutate(proportion = count / sum(count)) ## # A tibble: 8 x 3 ## num_kids count proportion ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0 62 0.124 ## 2 1 135 0.27 ## 3 2 145 0.290 ## 4 3 88 0.176 ## 5 4 38 0.076 ## 6 5 19 0.038 ## 7 6 10 0.02 ## 8 7 3 0.006 ggplot(plot_data, aes(x = num_kids)) + geom_bar() Now most families have 1–2 kids. Cool.\n  Rescaling numbers All these different distributions are good at generating general shapes:\n Uniform: a bunch of random numbers with no central gravity Normal: an average ± some variation Beta: different shapes and skews and gravities between 0 and 1 Binomial: yes/no outcomes that follow some probability  The shapes are great, but you also care about the values of these numbers. This can be tricky. As we saw earlier with a normal distribution, sometimes you’ll get values that go below zero or above some value you care about. We fixed that with a truncated normal distribution, but not all distributions have truncated versions. Additionally, if you’re using a Beta distribution, you’re stuck in a 0–1 scale (or 0–10 or 0–100 if you multiply the value by 10 or 100 or whatever).\nWhat if you want a fun skewed Beta shape for a variable like income or some other value that doesn’t fit within a 0–1 range? You can rescale any set of numbers after-the-fact using the rescale() function from the scales library and rescale things to whatever range you want.\nFor instance, let’s say that income isn’t normally distributed, but is right-skewed with a handful of rich people. This might look like a Beta distribution with shape1 = 2 and shape2 = 5:\nggplot() + geom_function(fun = dbeta, args = list(shape1 = 2, shape2 = 5)) If we generate random numbers from this distribution, they’ll all be stuck between 0 and 1:\nset.seed(1234) fake_people \u0026lt;- tibble(income = rbeta(1000, shape1 = 2, shape2 = 5)) ggplot(fake_people, aes(x = income)) + geom_histogram(binwidth = 0.1, color = \u0026quot;white\u0026quot;, boundary = 0) We can take those underling 0–1 values and rescale them to some other range using the rescale() function. We can specify the minimum and maximum values in the to argument. Here we’ll scale it up so that 0 = $10,000 and 1 = $100,000. Our rescaled version follows the same skewed Beta distribution shape, but now we’re using better values!\nlibrary(scales) fake_people_scaled \u0026lt;- fake_people %\u0026gt;% mutate(income_scaled = rescale(income, to = c(10000, 100000))) head(fake_people_scaled) ## # A tibble: 6 x 2 ## income income_scaled ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.101 21154. ## 2 0.345 49014. ## 3 0.553 72757. ## 4 0.0219 12176. ## 5 0.380 53036. ## 6 0.399 55162. ggplot(fake_people_scaled, aes(x = income_scaled)) + geom_histogram(binwidth = 5000, color = \u0026quot;white\u0026quot;, boundary = 0) This works for anything, really. For instance, instead of specifying a mean and standard deviation for a normal distribution and hoping that the generated values don’t go too high or too low, you can generate a normal distribution with a mean of 0 and standard deviation of 1 and then rescale it to the range you want:\nset.seed(1234) fake_data \u0026lt;- tibble(age_not_scaled = rnorm(1000, mean = 0, sd = 1)) %\u0026gt;% mutate(age = rescale(age_not_scaled, to = c(18, 65))) head(fake_data) ## # A tibble: 6 x 2 ## age_not_scaled age ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 -1.21 33.6 ## 2 0.277 44.2 ## 3 1.08 49.9 ## 4 -2.35 25.5 ## 5 0.429 45.3 ## 6 0.506 45.8 plot_unscaled \u0026lt;- ggplot(fake_data, aes(x = age_not_scaled)) + geom_histogram(binwidth = 0.5, color = \u0026quot;white\u0026quot;, boundary = 0) plot_scaled \u0026lt;- ggplot(fake_data, aes(x = age)) + geom_histogram(binwidth = 5, color = \u0026quot;white\u0026quot;, boundary = 0) plot_unscaled + plot_scaled This gives you less control over the center of the distribution (here it happens to be 40 because that’s in the middle of 18 and 65), but it gives you more control over the edges of the distribution.\nRescaling things is really helpful when building in effects and interacting columns with other columns, since multiplying variables by different coefficients can make the values go way out of the normal range. You’ll see a lot more of that in the synthetic data example.\n Summary Phew. We covered a lot here, and we barely scratched the surface of all the distributions that exist. Here’s a helpful summary of the main distribtuions you should care about:\n   Distribution  Description  Situations  Parameters  Code      Uniform  Numbers between a minimum and maximum; everything equally likely  ID numbers, age  min, max  sample() or runif()    Normal  Numbers bunched up around an average with a surrounding spread; numbers closer to average more likely  Income, education, most types of numbers that have some sort of central tendency  mean, sd  rnorm()    Truncated normal  Normal distribution + constraints on minimum and/or maximum values  Anything with a normal distribution  mean, sd, a (minimum), b (maximum)  truncnorm::rtruncnorm()    Beta  Numbers constrained between 0 and 1  Anything with percents; anything on a 0–1(00) scale; anything, really, if you use rescale() to rescale it  shape1 (\\(\\alpha\\)), shape2 (\\(\\beta\\)) (\\(\\frac{\\alpha}{\\alpha + \\beta}\\))  rbeta()    Binomial  Binary variables  Treatment/control, yes/no, true/false, 0/1  size, prob  sample(..., prob = 0.5) or rbinom()    Poisson  Whole numbers that represent counts of things  Number of kids, number of cities lived in, arrival times  lambda  rpois()      Example And here’s an example dataset of 1,000 fake people and different characteristics. One shortcoming of this fake data is that each of these columns is completely independent—there’s no relationship between age and education and family size and income. You can see how to make these columns correlated (and make one cause another!) in the example for synthetic data.\nset.seed(1234) # Set the number of people here once so it\u0026#39;s easier to change later n_people \u0026lt;- 1000 example_fake_people \u0026lt;- tibble( id = 1:n_people, opinion = sample(1:5, n_people, replace = TRUE), age = runif(n_people, min = 18, max = 80), income = rnorm(n_people, mean = 50000, sd = 10000), education = rtruncnorm(n_people, mean = 16, sd = 6, a = 8, b = 24), happiness = rbeta(n_people, shape1 = 2, shape2 = 1), treatment = sample(c(TRUE, FALSE), n_people, replace = TRUE, prob = c(0.3, 0.7)), size = rbinom(n_people, size = 1, prob = 0.5), family_size = rpois(n_people, lambda = 1) + 1 # Add one so there are no 0s ) %\u0026gt;% # Adjust some of these columns mutate(opinion = recode(opinion, \u0026quot;1\u0026quot; = \u0026quot;Strongly disagree\u0026quot;, \u0026quot;2\u0026quot; = \u0026quot;Disagree\u0026quot;, \u0026quot;3\u0026quot; = \u0026quot;Neutral\u0026quot;, \u0026quot;4\u0026quot; = \u0026quot;Agree\u0026quot;, \u0026quot;5\u0026quot; = \u0026quot;Strongly agree\u0026quot;)) %\u0026gt;% mutate(size = recode(size, \u0026quot;0\u0026quot; = \u0026quot;Small\u0026quot;, \u0026quot;1\u0026quot; = \u0026quot;Large\u0026quot;)) %\u0026gt;% mutate(happiness = rescale(happiness, to = c(1, 8))) head(example_fake_people) ## # A tibble: 6 x 9 ## id opinion age income education happiness treatment size family_size ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 Agree 31.7 43900. 18.3 7.20 TRUE Large 1 ## 2 2 Disagree 52.9 34696. 17.1 4.73 TRUE Large 2 ## 3 3 Strongly agree 45.3 43263. 17.1 7.32 FALSE Large 4 ## 4 4 Agree 34.9 40558. 11.7 4.18 FALSE Small 2 ## 5 5 Strongly disagree 50.3 41392. 13.3 2.61 TRUE Small 2 ## 6 6 Strongly agree 63.6 69917. 11.2 4.36 FALSE Small 2 plot_opinion \u0026lt;- ggplot(example_fake_people, aes(x = opinion)) + geom_bar() + guides(fill = FALSE) + labs(title = \u0026quot;Opinion (uniform with sample())\u0026quot;) plot_age \u0026lt;- ggplot(example_fake_people, aes(x = age)) + geom_histogram(binwidth = 5, color = \u0026quot;white\u0026quot;, boundary = 0) + labs(title = \u0026quot;Age (uniform with runif())\u0026quot;) plot_income \u0026lt;- ggplot(example_fake_people, aes(x = income)) + geom_histogram(binwidth = 5000, color = \u0026quot;white\u0026quot;, boundary = 0) + labs(title = \u0026quot;Income (normal)\u0026quot;) plot_education \u0026lt;- ggplot(example_fake_people, aes(x = education)) + geom_histogram(binwidth = 2, color = \u0026quot;white\u0026quot;, boundary = 0) + labs(title = \u0026quot;Education (truncated normal)\u0026quot;) plot_happiness \u0026lt;- ggplot(example_fake_people, aes(x = happiness)) + geom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;) + scale_x_continuous(breaks = 1:8) + labs(title = \u0026quot;Happiness (Beta, rescaled to 1-8)\u0026quot;) plot_treatment \u0026lt;- ggplot(example_fake_people, aes(x = treatment)) + geom_bar() + labs(title = \u0026quot;Treatment (binary with sample())\u0026quot;) plot_size \u0026lt;- ggplot(example_fake_people, aes(x = size)) + geom_bar() + labs(title = \u0026quot;Size (binary with rbinom())\u0026quot;) plot_family \u0026lt;- ggplot(example_fake_people, aes(x = family_size)) + geom_bar() + scale_x_continuous(breaks = 1:7) + labs(title = \u0026quot;Family size (Poisson)\u0026quot;) (plot_opinion + plot_age) / (plot_income + plot_education) (plot_happiness + plot_treatment) / (plot_size + plot_family)  ","date":1604102400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"b67105a508aa83bebfd7ccac18a3d502","permalink":"https://evalf20.classes.andrewheiss.com/example/random-numbers/","publishdate":"2020-10-31T00:00:00Z","relpermalink":"/example/random-numbers/","section":"example","summary":"Seeds Distributions  Uniform distribution Normal distribution Truncated normal distribution Beta distribution Binomial distribution Poisson distribution  Rescaling numbers Summary Example   In your final project, you will generate a synthetic dataset and use it to conduct an evaluation of some social program. Generating fake or simulated data is an incredibly powerful skill, but it takes some practice. Here are a bunch of helpful resources and code examples of how to use different R functions to generate random numbers that follow specific distributions (or probability shapes).","tags":null,"title":"Generating random numbers","type":"docs"},{"authors":null,"categories":null,"content":"   Basic example  Relationships and regression Explanatory variables linked to outcome; no connection between explanatory variables Explanatory variables linked to outcome; connection between explanatory variables Adding extra noise  Visualizing variables and relationships  Visualizing one variable Visualizing two continuous variables Visualizing a binary variable and a continuous variable  Specific examples  tl;dr: The general process Creating an effect in an observational DAG Brief pep talk intermission Creating an effect for RCTs Creating an effect for diff-in-diff Creating an effect for regression discontinuity Creating an effect for instrumental variables  Use synthetic data packages  fabricatr wakefield faux    In the example guide for generating random numbers, we explored how to use a bunch of different statistical distributions to create variables that had reasonable values. However, each of the columns that we generated there were completely independent of each other. In the final example, we made some data with columns like age, education, and income, but none of those were related, though in real life they would be.\nGenerating random variables is fairly easy: choose some sort of distributional shape, set parameters like a mean and standard deviation, and let randomness take over. Forcing variables to be related is a little trickier and involves a little math. But don’t worry! That math is all just regression stuff!\nlibrary(tidyverse) library(broom) library(patchwork) library(scales) library(ggdag) Basic example Relationships and regression Let’s pretend we want to predict someone’s happiness on a 10-point scale based on the number of cookies they’ve eaten and whether or not their favorite color is blue.\n\\[ \\text{Happiness} = \\beta_0 + \\beta_1 \\text{Cookies eaten} + \\beta_2 \\text{Favorite color is blue} + \\varepsilon \\]\nWe can generate a fake dataset with columns for happiness (Beta distribution clustered around 7ish), cookies (Poisson distribution), and favorite color (binomial distribution for blue/not blue):\nset.seed(1234) n_people \u0026lt;- 1000 happiness_simple \u0026lt;- tibble( id = 1:n_people, happiness = rbeta(n_people, shape1 = 7, shape2 = 3), cookies = rpois(n_people, lambda = 1), color_blue = sample(c(\u0026quot;Blue\u0026quot;, \u0026quot;Not blue\u0026quot;), n_people, replace = TRUE) ) %\u0026gt;% # Adjust some of the columns mutate(happiness = round(happiness * 10, 1), cookies = cookies + 1, color_blue = fct_relevel(factor(color_blue), \u0026quot;Not blue\u0026quot;)) head(happiness_simple) ## # A tibble: 6 x 4 ## id happiness cookies color_blue ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; ## 1 1 8.7 2 Blue ## 2 2 6.5 2 Not blue ## 3 3 4.8 2 Blue ## 4 4 9.6 3 Not blue ## 5 5 6.2 1 Not blue ## 6 6 6.1 2 Blue We have a neat dataset now, so let’s run a regression. Is eating more cookies or liking blue associated with greater happiness?\nmodel_happiness1 \u0026lt;- lm(happiness ~ cookies + color_blue, data = happiness_simple) tidy(model_happiness1) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 7.06 0.105 67.0 0 ## 2 cookies -0.00884 0.0419 -0.211 0.833 ## 3 color_blueBlue -0.0202 0.0861 -0.235 0.815 Not really. The coefficients for both cookies and color_blueBlue are basically 0 and not statistically significant. That makes sense since the three columns are completely independent of each other. If there were any significant effects, that’d be strange and solely because of random chance.\nFor the sake of your final project, you can just leave all the columns completely independent of each other if you want. None of your results will be significant and you won’t see any effects anywhere, but you can still build models, run all the pre-model diagnostics, and create graphs and tables based on this data.\nHOWEVER, it will be far more useful to you if you generate relationships. The whole goal of this class is to find causal effects in observational, non-experimental data. If you can generate synthetic non-experimental data and bake in a known causal effect, you can know if your different methods for recovering that effect are working.\nSo how do we bake in correlations and causal effects?\n Explanatory variables linked to outcome; no connection between explanatory variables To help with the intuition of how to link these columns, think about the model we’re building:\n\\[ \\text{Happiness} = \\beta_0 + \\beta_1 \\text{Cookies eaten} + \\beta_2 \\text{Favorite color is blue} + \\varepsilon \\]\nThis model provides estimates for all those betas. Throughout the semester, we’ve used the analogy of sliders and switches to describe regression coefficients. Here we have both:\n \\(\\beta_0\\): The average baseline happiness. \\(\\beta_1\\): The additional change in happiness that comes from eating one cookie. This is a slider: move cookies up by one and happiness changes by \\(\\beta_1\\). \\(\\beta_2\\): The change in happiness that comes from having your favorite color be blue. This is a switch: turn on “blue” for someone and their happiness changes by \\(\\beta_2\\).  We can invent our own coefficients and use some math to build them into the dataset. Let’s use these numbers as our targets:\n \\(\\beta_0\\): Average happiness is 7 \\(\\beta_1\\): Eating one more cookie boosts happiness by 0.25 points \\(\\beta_2\\): People who like blue have 0.75 higher happiness  When generating the data, we can’t just use rbeta() by itself to generate happiness, since happiness depends on both cookies and favorite color (that’s why we call it a dependent variable). To build in this effect, we can add a new column that uses math and modifies the underlying rbeta()-based happiness score:\nhappiness_with_effect \u0026lt;- happiness_simple %\u0026gt;% # Turn the categorical favorite color column into TRUE/FALSE so we can do math with it mutate(color_blue_binary = ifelse(color_blue == \u0026quot;Blue\u0026quot;, TRUE, FALSE)) %\u0026gt;% # Make a new happiness column that uses coefficients for cookies and favorite color mutate(happiness_modified = happiness + (0.25 * cookies) + (0.75 * color_blue_binary)) head(happiness_with_effect) ## # A tibble: 6 x 6 ## id happiness cookies color_blue color_blue_binary happiness_modified ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 8.7 2 Blue TRUE 9.95 ## 2 2 6.5 2 Not blue FALSE 7 ## 3 3 4.8 2 Blue TRUE 6.05 ## 4 4 9.6 3 Not blue FALSE 10.4 ## 5 5 6.2 1 Not blue FALSE 6.45 ## 6 6 6.1 2 Blue TRUE 7.35 Now that we have a new happiness_modified column we can run a model using it as the outcome:\nmodel_happiness2 \u0026lt;- lm(happiness_modified ~ cookies + color_blue, data = happiness_with_effect) tidy(model_happiness2) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 7.06 0.105 67.0 0. ## 2 cookies 0.241 0.0419 5.76 1.13e- 8 ## 3 color_blueBlue 0.730 0.0861 8.48 8.25e-17 Whoa! Look at those coefficients! They’re exactly what we tried to build in! The baseline happiness (intercept) is ≈7, eating one cookie is associated with a ≈0.25 increase in happiness, and liking blue is associated with a ≈0.75 increase in happiness.\nHowever, we originally said that happiness was a 0-10 point scale. After boosting it with extra happiness for cookies and liking blue, there are some people who score higher than 10:\n# Original scale ggplot(happiness_with_effect, aes(x = happiness)) + geom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;) + scale_x_continuous(breaks = 0:11) + coord_cartesian(xlim = c(0, 11)) # Scaled up ggplot(happiness_with_effect, aes(x = happiness_modified)) + geom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;) + scale_x_continuous(breaks = 0:11) + coord_cartesian(xlim = c(0, 11)) To fix that, we can use the rescale() function from the scales package to force the new happiness_modified variable to fit back in its original range:\nhappiness_with_effect \u0026lt;- happiness_with_effect %\u0026gt;% mutate(happiness_rescaled = rescale(happiness_modified, to = c(3, 10))) ggplot(happiness_with_effect, aes(x = happiness_rescaled)) + geom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;) + scale_x_continuous(breaks = 0:11) + coord_cartesian(xlim = c(0, 11)) Everything is back in the 3–10 range now. However, the rescaling also rescaled our built-in effects. Look what happens if we use the happiness_rescaled in the model:\nmodel_happiness3 \u0026lt;- lm(happiness_rescaled ~ cookies + color_blue, data = happiness_with_effect) tidy(model_happiness3) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 6.34 0.0910 69.6 0. ## 2 cookies 0.208 0.0362 5.76 1.13e- 8 ## 3 color_blueBlue 0.631 0.0744 8.48 8.25e-17 Now the baseline happiness is 6.3, the cookies effect is 0.2, and the blue effect is 0.63. These effects shrunk because we shrunk the data back down to have a maximum of 10.\nThere are probably fancy mathy ways to rescale data and keep the coefficients the same size, but rather than figure that out (who even wants to do that?!), my strategy is just to play with numbers until the results look good. Instead of using a 0.25 cookie effect and 0.75 blue effect, I make those effects bigger so that the rescaled version is roughly what I really want. There’s no systematic way to do this—I ran this chunk below a bunch of times until the numbers worked.\nset.seed(1234) n_people \u0026lt;- 1000 happiness_real_effect \u0026lt;- tibble( id = 1:n_people, happiness_baseline = rbeta(n_people, shape1 = 7, shape2 = 3), cookies = rpois(n_people, lambda = 1), color_blue = sample(c(\u0026quot;Blue\u0026quot;, \u0026quot;Not blue\u0026quot;), n_people, replace = TRUE) ) %\u0026gt;% # Adjust some of the columns mutate(happiness_baseline = round(happiness_baseline * 10, 1), cookies = cookies + 1, color_blue = fct_relevel(factor(color_blue), \u0026quot;Not blue\u0026quot;)) %\u0026gt;% # Turn the categorical favorite color column into TRUE/FALSE so we can do math with it mutate(color_blue_binary = ifelse(color_blue == \u0026quot;Blue\u0026quot;, TRUE, FALSE)) %\u0026gt;% # Make a new happiness column that uses coefficients for cookies and favorite color mutate(happiness_effect = happiness_baseline + (0.31 * cookies) + # Cookie effect (0.91 * color_blue_binary)) %\u0026gt;% # Blue effect # Rescale to 3-10, since that\u0026#39;s what the original happiness column looked like mutate(happiness = rescale(happiness_effect, to = c(3, 10))) model_does_this_work_yet \u0026lt;- lm(happiness ~ cookies + color_blue, data = happiness_real_effect) tidy(model_does_this_work_yet) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 6.15 0.0886 69.4 0. ## 2 cookies 0.253 0.0352 7.19 1.25e-12 ## 3 color_blueBlue 0.749 0.0724 10.3 7.44e-24 There’s nothing magical about the 0.31 and 0.91 numbers I used here; I just kept changing those to different things until the regression coefficients ended up at ≈0.25 and ≈0.75. Also, I gave up on trying to make the baseline happiness 7. It’s possible to do—you’d just need to also shift the underlying Beta distribution up (like shape1 = 9, shape2 = 2 or something). But then you’d also need to change the coefficients more. You’ll end up with 3 moving parts and it can get complicated, so I don’t worry too much about it, since what we care about the most here is the effect of cookies and favorite color, not baseline levels of happiness.\nPhew. We successfully connected cookies and favorite color to happiness and we have effects that are measurable with regression! One last thing that I would do is get rid of some of the intermediate columns like color_blue_binary or happiness_effect—we only used those for the behind-the-scenes math of creating the effect. Here’s our final synthetic dataset:\nhappiness \u0026lt;- happiness_real_effect %\u0026gt;% select(id, happiness, cookies, color_blue) head(happiness) ## # A tibble: 6 x 4 ## id happiness cookies color_blue ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; ## 1 1 8.81 2 Blue ## 2 2 6.20 2 Not blue ## 3 3 5.53 2 Blue ## 4 4 9.07 3 Not blue ## 5 5 5.68 1 Not blue ## 6 6 6.63 2 Blue We can save that as a CSV file with write_csv():\nwrite_csv(happiness, \u0026quot;data/happiness_fake_data.csv\u0026quot;)  Explanatory variables linked to outcome; connection between explanatory variables In that cookie example, we assumed that both cookie consumption and favorite color are associated with happiness. We also assumed that cookie consumption and favorite color are not related to each other. But what if they are? What if people who like blue eat more cookies?\nWe’ve already used regression-based math to connect explanatory variables to outcome variables. We can use that same intuition to connect explanatory variables to each other.\nThe easiest way to think about this is with DAGs. Here’s the DAG for the model we just ran:\nhappiness_dag1 \u0026lt;- dagify(hap ~ cook + blue, coords = list(x = c(hap = 3, cook = 1, blue = 2), y = c(hap = 1, cook = 1, blue = 2))) ggdag(happiness_dag1) + theme_dag() Both cookies and favorite color cause happiness, but there’s no link between them. Notice that dagify() uses the same model syntax that lm() does: hap ~ cook + blue. If we think of this just like a regression model, we can pretend that there are coefficients there too: hap ~ 0.25*cook + 0.75*blue. We don’t actually include any coefficients in the DAG or anything, but it helps with the intuition.\nBut what if people who like blue eat more cookies on average? For whatever reason, let’s pretend that liking blue causes you to eat 0.5 more cookies, on average. Here’s the new DAG:\nhappiness_dag2 \u0026lt;- dagify(hap ~ cook + blue, cook ~ blue, coords = list(x = c(hap = 3, cook = 1, blue = 2), y = c(hap = 1, cook = 1, blue = 2))) ggdag(happiness_dag2) + theme_dag() Now we have two different equations: hap ~ cook + blue and cook ~ blue. Conveniently, these both translate to models, and we know the coefficients we want!\n hap ~ 0.25*cook + 0.75*blue: This is what we built before—cookies boost happiness by 0.25 and liking blue boosts happiness by 0.75 cook ~ 0.3*blue: This is what we just proposed—liking blue boosts cookies by 0.5  We can follow the same process we did when building the cookie and blue effects into happiness to also build a blue effect into cookies!\nset.seed(1234) n_people \u0026lt;- 1000 happiness_cookies_blue \u0026lt;- tibble( id = 1:n_people, happiness_baseline = rbeta(n_people, shape1 = 7, shape2 = 3), cookies = rpois(n_people, lambda = 1), color_blue = sample(c(\u0026quot;Blue\u0026quot;, \u0026quot;Not blue\u0026quot;), n_people, replace = TRUE) ) %\u0026gt;% # Adjust some of the columns mutate(happiness_baseline = round(happiness_baseline * 10, 1), cookies = cookies + 1, color_blue = fct_relevel(factor(color_blue), \u0026quot;Not blue\u0026quot;)) %\u0026gt;% # Turn the categorical favorite color column into TRUE/FALSE so we can do math with it mutate(color_blue_binary = ifelse(color_blue == \u0026quot;Blue\u0026quot;, TRUE, FALSE)) %\u0026gt;% # Make blue have an effect on cookie consumption mutate(cookies = cookies + (0.5 * color_blue_binary)) %\u0026gt;% # Make a new happiness column that uses coefficients for cookies and favorite color mutate(happiness_effect = happiness_baseline + (0.31 * cookies) + # Cookie effect (0.91 * color_blue_binary)) %\u0026gt;% # Blue effect # Rescale to 3-10, since that\u0026#39;s what the original happiness column looked like mutate(happiness = rescale(happiness_effect, to = c(3, 10))) head(happiness_cookies_blue) ## # A tibble: 6 x 7 ## id happiness_baseline cookies color_blue color_blue_binary happiness_effect happiness ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 8.7 2.5 Blue TRUE 10.4 8.84 ## 2 2 6.5 2 Not blue FALSE 7.12 6.14 ## 3 3 4.8 2.5 Blue TRUE 6.48 5.61 ## 4 4 9.6 3 Not blue FALSE 10.5 8.96 ## 5 5 6.2 1 Not blue FALSE 6.51 5.63 ## 6 6 6.1 2.5 Blue TRUE 7.78 6.69 Notice now that people who like blue eat partial cookies, as expected. We can verify that there’s a relationship between liking blue and cookies by running a model:\nlm(cookies ~ color_blue, data = happiness_cookies_blue) %\u0026gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 2.07 0.0451 45.9 3.51e-248 ## 2 color_blueBlue 0.460 0.0651 7.07 2.96e- 12 Yep. Liking blue is associated with 0.46 more cookies on average (it’s not quite 0.5, but that’s because of randomness).\nNow let’s do some neat DAG magic. Let’s say we’re interested in the causal effect of cookies on happiness. We could run a naive model:\nmodel_happiness_naive \u0026lt;- lm(happiness ~ cookies, data = happiness_cookies_blue) tidy(model_happiness_naive) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 6.27 0.0894 70.1 0. ## 2 cookies 0.325 0.0354 9.18 2.40e-19 Based on this, eating a cookie causes you to have 0.325 more happiness points. But that’s wrong! Liking the color blue is a confounder and opens a path between cookies and happiness. You can see the confounding both in the DAG (since blue points to both the cookie node and the happiness node) and in the math (liking blue boosts happiness + liking blue boosts cookie consumption, which boosts happiness).\nTo fix this confounding, we need to statistically adjust for liking blue and close the backdoor path. Ordinarily we’d do this with something like matching or inverse probability weighting, but here we can just include liking blue as a control variable (since it’s linearly related to both cookies and happiness):\nmodel_happiness_ate \u0026lt;- lm(happiness ~ cookies + color_blue, data = happiness_cookies_blue) tidy(model_happiness_ate) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 6.09 0.0870 70.0 0. ## 2 cookies 0.249 0.0346 7.19 1.25e-12 ## 3 color_blueBlue 0.739 0.0729 10.1 4.70e-23 After adjusting for backdoor confounding, eating one additional cookie causes a 0.249 point increase in happiness. This is the effect we originally built into the data!\nIf you wanted, we could rescale the number of cookies just like we rescaled happiness before, since sometimes adding effects to columns changes their reasonable ranges.\nNow that we have a good working dataset, we can keep the columns we care about and save it as a CSV file for later use:\nhappiness \u0026lt;- happiness_cookies_blue %\u0026gt;% select(id, happiness, cookies, color_blue) head(happiness) ## # A tibble: 6 x 4 ## id happiness cookies color_blue ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; ## 1 1 8.84 2.5 Blue ## 2 2 6.14 2 Not blue ## 3 3 5.61 2.5 Blue ## 4 4 8.96 3 Not blue ## 5 5 5.63 1 Not blue ## 6 6 6.69 2.5 Blue write_csv(happiness, \u0026quot;data/happiness_fake_data.csv\u0026quot;)  Adding extra noise We’ve got columns that follow specific distributions, and we’ve got columns that are statistically related to each other. We can add one more wrinkle to make our fake data even more fun (and even more reflective of real life). We can add some noise.\nRight now, the effects we’re finding are too perfect. When we used mutate() to add a 0.25 boost in happiness for every cookie people ate, we added exactly 0.25 happiness points. If someone ate 2 cookies, they got 0.5 more happiness; if they ate 5, they got 1.25 more.\nWhat if the cookie effect isn’t exactly 0.25, but somewhere around 0.25? For some people it’s 0.1, for others it’s 0.3, for others it’s 0.22. We can use the same ideas we talked about in the random numbers example to generate a distribution of an effect. For instance, let’s say that the average cookie effect is 0.25, but it can vary somewhat with a standard deviation of 0.15:\ntemp_data \u0026lt;- tibble(x = rnorm(10000, mean = 0.25, sd = 0.15)) ggplot(temp_data, aes(x = x)) + geom_histogram(binwidth = 0.05, boundary = 0, color = \u0026quot;white\u0026quot;) Sometimes it can go as low as −0.25; sometimes it can go as high as 0.75; normally it’s around 0.25.\nNothing in the model explains why it’s higher or lower for some people—it’s just random noise. Remember that the model accounts for that! This random variation is what the \\(\\varepsilon\\) is for in this model equation:\n\\[ \\text{Happiness} = \\beta_0 + \\beta_1 \\text{Cookies eaten} + \\beta_2 \\text{Favorite color is blue} + \\varepsilon \\]\nWe can build that uncertainty into the fake column! Instead of using 0.31 * cookies when generating happiness (which is technically 0.25, but shifted up to account for rescaling happiness back down after), we’ll make a column for the cookie effect and then multiply that by the number of cookies.\nset.seed(1234) n_people \u0026lt;- 1000 happiness_cookies_noisier \u0026lt;- tibble( id = 1:n_people, happiness_baseline = rbeta(n_people, shape1 = 7, shape2 = 3), cookies = rpois(n_people, lambda = 1), cookie_effect = rnorm(n_people, mean = 0.31, sd = 0.2), color_blue = sample(c(\u0026quot;Blue\u0026quot;, \u0026quot;Not blue\u0026quot;), n_people, replace = TRUE) ) %\u0026gt;% # Adjust some of the columns mutate(happiness_baseline = round(happiness_baseline * 10, 1), cookies = cookies + 1, color_blue = fct_relevel(factor(color_blue), \u0026quot;Not blue\u0026quot;)) %\u0026gt;% # Turn the categorical favorite color column into TRUE/FALSE so we can do math with it mutate(color_blue_binary = ifelse(color_blue == \u0026quot;Blue\u0026quot;, TRUE, FALSE)) %\u0026gt;% # Make blue have an effect on cookie consumption mutate(cookies = cookies + (0.5 * color_blue_binary)) %\u0026gt;% # Make a new happiness column that uses coefficients for cookies and favorite # color. Importantly, instead of using 0.31 * cookies, we\u0026#39;ll use the random # cookie effect we generated earlier mutate(happiness_effect = happiness_baseline + (cookie_effect * cookies) + (0.91 * color_blue_binary)) %\u0026gt;% # Rescale to 3-10, since that\u0026#39;s what the original happiness column looked like mutate(happiness = rescale(happiness_effect, to = c(3, 10))) head(happiness_cookies_noisier) ## # A tibble: 6 x 8 ## id happiness_baseline cookies cookie_effect color_blue color_blue_binary happiness_effect happiness ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 8.7 2.5 0.124 Blue TRUE 9.92 8.16 ## 2 2 6.5 2.5 0.370 Blue TRUE 8.34 7.02 ## 3 3 4.8 2.5 0.326 Blue TRUE 6.53 5.72 ## 4 4 9.6 3.5 0.559 Blue TRUE 12.5 9.98 ## 5 5 6.2 1.5 0.0631 Blue TRUE 7.20 6.21 ## 6 6 6.1 2 0.222 Not blue FALSE 6.54 5.73 Now let’s look at the cookie effect in this noisier data:\nmodel_noisier \u0026lt;- lm(happiness ~ cookies + color_blue, data = happiness_cookies_noisier) tidy(model_noisier) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 6.11 0.0779 78.4 0. ## 2 cookies 0.213 0.0314 6.79 1.92e-11 ## 3 color_blueBlue 0.650 0.0671 9.68 3.01e-21 The effect is a little smaller now because of the extra noise, so we’d need to mess with the 0.31 and 0.91 coefficients more to get those numbers back up to 0.25 and 0.75.\nWhile this didn’t influence the findings too much here, it can have consequences for other variables. For instance, in the previous section we said that the color blue influences cookie consumption. If the blue effect on cookies isn’t precisely 0.5 but follows some sort of distribution (sometimes small, sometimes big, sometimes negative, sometimes zero), that will influence cookies differently. That random effect on cookie consumption will then work together with the random effect of cookies on happiness, resulting in multiple varied values.\nFor instance, imagine the average effect of liking blue on cookies is 0.5, and the average effect of cookies on happiness is 0.25. For one person, their blue-on-cookie effect might be 0.392, which changes the number of cookies they eat. Their cookie-on-happiness effect is 0.573, which changes their happiness. Both of those random effects work together to generate the final happiness.\nIf you want more realistic-looking synthetic data, it’s a good idea to add some random noise wherever you can.\n  Visualizing variables and relationships Going through this process requires a ton of trial and error. You will change all sorts of numbers to make sure the relationships you’re building work. This is especially the case if you rescale things, since that rescales your effects. There are a lot of moving parts and this is a complicated process.\nYou’ll run your data generation chunks lots and lots and lots of times, tinkering with the numbers as you go. This example makes it look easy, since it’s the final product, but I ran all these chunks over and over again until I got the causal effect and relationships just right.\nIt’s best if you also create plots and models to see what the relationships look like\nVisualizing one variable We covered a bunch of distributions in the random number generation example, but it’s hard to think about what a standard deviation of 2 vs 10 looks like, or what happens when you mess with the shape parameters in a Beta distribution.\nIt’s best to visualize these variables. You could build the variable into your official dataset and then look at it, but I find it’s often faster to just look at what a general distribution looks like first. The easiest way to do this is generate a dataset with just one column in it and look at it, either with a histogram or a density plot.\nFor instance, what does a Beta distribution with shape1 = 3 and shape2 = 16 look like? The math says it should peak around 0.15ish (\\(\\frac{3}{3 + 16}\\)), and that looks like the case:\ntemp_data \u0026lt;- tibble(x = rbeta(10000, shape1 = 3, shape2 = 16)) plot1 \u0026lt;- ggplot(temp_data, aes(x = x)) + geom_histogram(binwidth = 0.05, boundary = 0, color = \u0026quot;white\u0026quot;) plot2 \u0026lt;- ggplot(temp_data, aes(x = x)) + geom_density() plot1 + plot2 What if we want a normal distribution centered around 100, with most values range from 50 to 150. That’s range of ±50, but that doesn’t mean the sd will be 50—it’ll be much smaller than that, like 25ish. Tinker with the numbers until it looks right.\ntemp_data \u0026lt;- tibble(x = rnorm(10000, mean = 100, sd = 25)) plot1 \u0026lt;- ggplot(temp_data, aes(x = x)) + geom_histogram(binwidth = 10, boundary = 0, color = \u0026quot;white\u0026quot;) plot2 \u0026lt;- ggplot(temp_data, aes(x = x)) + geom_density() plot1 + plot2  Visualizing two continuous variables If you have two continuous/numeric columns, it’s best to use a scatterplot. For instance, let’s make two columns based on the Beta and normal distributions above, and we’ll make it so that y goes up by 0.25 for every increase in x, along with some noise:\nset.seed(1234) temp_data \u0026lt;- tibble( x = rnorm(1000, mean = 100, sd = 25) ) %\u0026gt;% mutate(y = rbeta(1000, shape1 = 3, shape2 = 16) + # Baseline distribution (0.25 * x) + # Effect of x rnorm(1000, mean = 0, sd = 10)) # Add some noise ggplot(temp_data, aes(x = x, y = y)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;) We can confirm the effect with a model:\nlm(y ~ x, data = temp_data) %\u0026gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 1.98 1.28 1.54 1.24e- 1 ## 2 x 0.235 0.0125 18.8 1.49e-67  Visualizing a binary variable and a continuous variable If you have one binary column and one continuous/numeric column, it’s generally best to not use a scatterplot. Instead, either look at the distribution of the continuous variable across the binary variable with a faceted histogram or overlaid density plot, or look at the average of the continuous variable across the different values of the binary variable with a point range.\nLet’s make two columns: a continuous outcome (y) and a binary treatment (x). Being in the treatment group causes an increase of 20 points, on average.\nset.seed(1234) temp_data \u0026lt;- tibble( treatment = rbinom(1000, size = 1, prob = 0.5) # Make 1000 0/1 values with 50% chance of each ) %\u0026gt;% mutate(outcome = rbeta(1000, shape1 = 3, shape2 = 16) + # Baseline distribution (20 * treatment) + # Effect of treatment rnorm(1000, mean = 0, sd = 20)) %\u0026gt;% # Add some noise mutate(treatment = factor(treatment)) # Make treatment a factor/categorical variable We can check the numbers with a model:\nlm(outcome ~ treatment, data = temp_data) %\u0026gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 0.244 0.935 0.261 7.94e- 1 ## 2 treatment1 20.9 1.30 16.1 4.70e-52 Here’s what that looks like as a histogram:\nggplot(temp_data, aes(x = outcome, fill = treatment)) + geom_histogram(binwidth = 5, color = \u0026quot;white\u0026quot;, boundary = 0) + guides(fill = FALSE) + # Turn off the fill legend since it\u0026#39;s redundant facet_wrap(vars(treatment), ncol = 1) And as overlapping densities:\nggplot(temp_data, aes(x = outcome, fill = treatment)) + geom_density(alpha = 0.5) And with a point range:\n# hahaha these error bars are tiny ggplot(temp_data, aes(x = treatment, y = outcome, color = treatment)) + stat_summary(geom = \u0026quot;pointrange\u0026quot;, fun.data = \u0026quot;mean_se\u0026quot;) + guides(color = FALSE) # Turn off the color legend since it\u0026#39;s redundant   Specific examples tl;dr: The general process Those previous sections go into a lot of detail. In general, here’s the process you should follow when building relationships in synthetic data:\nDraw a DAG that maps out how all the columns you care about are related. Specify how those nodes are measured. Specify the relationships between the nodes based on the DAG equations. Generate random columns that stand alone. Generate related columns using regression math. Consider adding random noise. This is an entirely trial and error process until you get numbers that look good. Rely heavily on plots as you try different coefficients and parameters. Optionally rescale any columns that go out of reasonable bounds. If you rescale, you’ll need to tinker with the coefficients you used since the final effects will also get rescaled. Verify all relationships with plots and models. Try it out! Save the data.   Creating an effect in an observational DAG Draw a DAG that maps out how all the columns you care about are related.\nHere’s a simple DAG that shows the causal effect of mosquito net usage on malaria risk. Income and health both influence and confound net use and malaria risk, and income also influences health.\nmosquito_dag \u0026lt;- dagify(mal ~ net + inc + hlth, net ~ inc + hlth, hlth ~ inc, coords = list(x = c(mal = 4, net = 1, inc = 2, hlth = 3), y = c(mal = 1, net = 1, inc = 2, hlth = 2))) ggdag(mosquito_dag) + theme_dag()  Specify how those nodes are measured.\nFor the sake of this example, we’ll measure these nodes like so. See the random number example for more details about the distributions.\n Malaria risk: scale from 0–100, mostly around 40, but ranging from 10ish to 80ish. Best to use a Beta distribution.\n Net use: binary 0/1, TRUE/FALSE variable, where 50% of people use nets. Best to use a binomial distribution. However, since we want to use other variables that increase the likelihood of using a net, we’ll do some cool tricky stuff, explained later.\n Income: weekly income, measured in dollars, mostly around 500 ± 300. Best to use a normal distribution.\n Health: scale from 0–100, mostly around 70, but ranging from 50ish to 100. Best to use a Beta distribution.\n  Specify the relationships between the nodes based on the DAG equations.\nThere are three models in this DAG:\n hlth ~ inc: Income influences health. We’ll assume that every 10 dollars/week is associated with a 1 point increase in health (so a 1 dollar incrrease is associated with a 0.02 point increase in health)\n net ~ inc + hlth: Income and health both increase the probability of net usage. This is where we do some cool tricky stuff.\nBoth income and health have an effect on the probability of bed net use, but bed net use is measured as a 0/1, TRUE/FALSE variable. If we run a regression with net as the outcome, we can’t really interpret the coefficients like “a 1 point increase in health is associated with a 0.42 point increase in bed net being TRUE.” That doesn’t even make sense.\nOrdinarily, when working with binary outcome variables, you use logistic regression models (see the crash course we had when talking about propensity scores here). In this kind of regression, the coefficients in the model represent changes in the log odds of using a net. As we discuss in the crash course section, log odds are typically impossible to interpet. If you exponentiate them, you get odds ratios, which let you say things like “a 1 point increase in health is associated with a 15% increase in the likelihood of using a net.” Technically we could include coefficients for a logistic regression model and simulate probabilities of using a net or not using log odds and odds ratios (and that’s what I do in the rain barrel data from Problem Set 3 (see code here)), but that’s really hard to wrap your head around since you’re dealing with strange uninterpretable coefficients. So we won’t do that here.\nInstead, we’ll do some fun trickery. We’ll create something called a “bed net score” that gets bigger as income and health increase. We’ll say that a 1 point increase in health score is associated with a 1.5 point increase in bed net score, and a 1 dollar increase in income is associated with a 0.5 point increase in bed net score. This results in a column that ranges all over the place, from 200 to 500 (in this case; that won’t always be true). This column definitely doesn’t look like a TRUE/FALSE binary column—it’s just a bunch of numbers. That’s okay!\nWe’ll then use the rescale() function from the scales package to take this bed net score and scale it down so that it goes from 0.05 to 0.95. This represents a person’s probability of using a bed net.\nFinally, we’ll use that probability in the rbinom() function to generate a 0 or 1 for each person. Some people will have a high probability because of their income and health, like 0.9, and will most likely use a net. Some people might have a 0.15 probability and will likely not use a net.\nWhen you generate binary variables like this, it’s hard to know the exact effect you’ll get, so it’s best to tinker with the numbers until you see relationships that you want.\n mal ~ net + inc + hlth: Finally net use, income, and health all have an effect on the risk of malaria. Building this relationship is easy since it’s just a regular linear regression model (since malaria risk is not binary). We’ll say that a 1 dollar increase in income is associated with a decrease in risk, a 1 point increase in health is associated with a decrease in risk, and using a net is associated with a 15 point decrease in risk. That’s the casual effect we’re building in to the DAG.\n  Generate random columns that stand alone. Generate related columns using regression math. Consider adding random noise. This is an entirely trial and error process until you get numbers that look good. Rely heavily on plots as you try different coefficients and parameters. Optionally rescale any columns that go out of reasonable bounds. If you rescale, you’ll need to tinker with the coefficients you used since the final effects will also get rescaled.\nHere we go! Let’s make some data. I’ll comment the code below so you can see what’s happening at each step.\n# Make this randomness consistent set.seed(1234) # Simulate 1138 people (just for fun) n_people \u0026lt;- 1138 net_data \u0026lt;- tibble( # Make an ID column (not necessary, but nice to have) id = 1:n_people, # Generate income variable: normal, 500 ± 300 income = rnorm(n_people, mean = 500, sd = 75) ) %\u0026gt;% # Generate health variable: beta, centered around 70ish mutate(health_base = rbeta(n_people, shape1 = 7, shape2 = 4) * 100, # Health increases by 0.02 for every dollar in income health_income_effect = income * 0.02, # Make the final health score and add some noise health = health_base + health_income_effect + rnorm(n_people, mean = 0, sd = 3), # Rescale so it doesn\u0026#39;t go above 100 health = rescale(health, to = c(min(health), 100))) %\u0026gt;% # Generate net variable based on income, health, and random noise mutate(net_score = (0.5 * income) + (1.5 * health) + rnorm(n_people, mean = 0, sd = 15), # Scale net score down to 0.05 to 0.95 to create a probability of using a net net_probability = rescale(net_score, to = c(0.05, 0.95)), # Randomly generate a 0/1 variable using that probability net = rbinom(n_people, 1, net_probability)) %\u0026gt;% # Finally generate a malaria risk variable based on income, health, net use, # and random noise mutate(malaria_risk_base = rbeta(n_people, shape1 = 4, shape2 = 5) * 100, # Risk goes down by 10 when using a net. Because we rescale things, # though, we have to make the effect a lot bigger here so it scales # down to -10. Risk also decreases as health and income go up. I played # with these numbers until they created reasonable coefficients. malaria_effect = (-30 * net) + (-1.9 * health) + (-0.1 * income), # Make the final malaria risk score and add some noise malaria_risk = malaria_risk_base + malaria_effect + rnorm(n_people, 0, sd = 3), # Rescale so it doesn\u0026#39;t go below 0, malaria_risk = rescale(malaria_risk, to = c(5, 70))) # Look at all these columns! head(net_data) ## # A tibble: 6 x 11 ## id income health_base health_income_effect health net_score net_probability net malaria_risk_base malaria_effect malaria_risk ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 409. 61.2 8.19 63.1 301. 0.369 0 37.9 -161. 45.1 ## 2 2 521. 83.9 10.4 83.5 409. 0.684 1 55.0 -241. 23.4 ## 3 3 581. 64.6 11.6 73.0 426. 0.735 0 53.0 -197. 36.5 ## 4 4 324. 62.0 6.48 60.6 255. 0.235 0 68.4 -148. 58.7 ## 5 5 532. 69.2 10.6 73.4 373. 0.578 1 63.2 -223. 32.7 ## 6 6 538. 36.6 10.8 42.6 295. 0.351 0 38.6 -135. 52.5 Verify all relationships with plots and models.\nLet’s see if we have the relationships we want. Income looks like it’s associated with health:\nggplot(net_data, aes(x = income, y = health)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;) lm(health ~ income, data = net_data) %\u0026gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 59.1 2.54 23.3 2.24e-98 ## 2 income 0.0169 0.00504 3.36 8.15e- 4 It looks like richer and healthier people are more likely to use nets:\nnet_income \u0026lt;- ggplot(net_data, aes(x = income, fill = as.factor(net))) + geom_density(alpha = 0.7) + theme(legend.position = \u0026quot;bottom\u0026quot;) net_health \u0026lt;- ggplot(net_data, aes(x = health, fill = as.factor(net))) + geom_density(alpha = 0.7) + theme(legend.position = \u0026quot;bottom\u0026quot;) net_income + net_health Income increasing makes it 1% more likely to use a net; health increasing make it 2% more likely to use a net:\nglm(net ~ income + health, family = binomial(link = \u0026quot;logit\u0026quot;), data = net_data) %\u0026gt;% tidy(exponentiate = TRUE) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 0.0186 0.532 -7.49 6.64e-14 ## 2 income 1.01 0.000864 6.47 9.93e-11 ## 3 health 1.02 0.00491 3.89 9.92e- 5 Try it out!\nIs the effect in there? Let’s try finding it by controlling for our two backdoors: health and income. Ordinarily we should do something like matching or inverse probability weighting, but we’ll just do regular regression here (which is okay-ish, since all these variables are indeed linearly related with each other—we made them that way!)\nIf we just look at the effect of nets on malaria risk without any statistical adjustment, we see that net cause a decrease of 13 points in malaria risk. This is wrong though becuase there’s confounding.\n# Wrong correlation-is-not-causation effect model_net_naive \u0026lt;- lm(malaria_risk ~ net, data = net_data) tidy(model_net_naive) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 41.9 0.413 102. 0. ## 2 net -13.6 0.572 -23.7 2.90e-101 If we control for the confounders, we get the 10 point ATE. It works!\n# Correctly adjusted ATE effect model_net_ate \u0026lt;- lm(malaria_risk ~ net + health + income, data = net_data) tidy(model_net_ate) ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 97.3 1.28 76.1 0. ## 2 net -10.5 0.317 -33.2 1.32e-169 ## 3 health -0.608 0.0123 -49.4 1.25e-284 ## 4 income -0.0320 0.00213 -15.0 1.20e- 46 Save the data.\nSince it works, let’s save it:\n# In the end, all we need is id, income, health, net, and malaria risk: net_data_final \u0026lt;- net_data %\u0026gt;% select(id, income, health, net, malaria_risk) head(net_data_final) ## # A tibble: 6 x 5 ## id income health net malaria_risk ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 409. 63.1 0 45.1 ## 2 2 521. 83.5 1 23.4 ## 3 3 581. 73.0 0 36.5 ## 4 4 324. 60.6 0 58.7 ## 5 5 532. 73.4 1 32.7 ## 6 6 538. 42.6 0 52.5 # Save it as a CSV file write_csv(net_data_final, \u0026quot;data/bed_nets.csv\u0026quot;)   Brief pep talk intermission Generating data for a full complete observational DAG like the example above is complicated and hard. These other forms of causal inference are design-based (i.e. tied to specific contexts like before/after treatment/control or arbitrary cutoffs) instead of model-based, so they’re actually a lot easier to simulate! So don’t be scared away yet!\n Creating an effect for RCTs Draw a DAG that maps out how all the columns you care about are related.\nRCTs are great because they make DAGs really easy! In a well-randomized RCT, you get to delete all arrows going into the treatment node in a DAG. We’ll stick with the same mosquito net situation we just used, but make it randomized:\nrct_dag \u0026lt;- dagify(mal ~ net + inc + hlth, hlth ~ inc, coords = list(x = c(mal = 4, net = 1, inc = 2, hlth = 3), y = c(mal = 1, net = 1, inc = 2, hlth = 2))) ggdag(rct_dag) + theme_dag()  Specify how those nodes are measured.\nWe’ll measure these nodes the same way as before:\n Malaria risk: scale from 0–100, mostly around 40, but ranging from 10ish to 80ish. Best to use a Beta distribution.\n Net use: binary 0/1, TRUE/FALSE variable, where 50% of people use nets. Best to use a binomial distribution.\n Income: weekly income, measured in dollars, mostly around 500 ± 300. Best to use a normal distribution.\n Health: scale from 0–100, mostly around 70, but ranging from 50ish to 100. Best to use a Beta distribution.\n  Specify the relationships between the nodes based on the DAG equations.\nThis is where RCTs are great. Because we removed all the arrows going into net, we don’t need to build any relationships that influence net use. Net use is randomized! We don’t need to make strange “bed net scores” and give people boosts according to income or health or anything. There are only two models in this DAG:\n hlth ~ inc: Income influences health. We’ll assume that every 10 dollars/week is associated with a 1 point increase in health (so a 1 dollar incrrease is associated with a 0.02 point increase in health)\n mal ~ net + inc + hlth: Net use, income, and health all have an effect on the risk of malaria. We’ll say that a 1 dollar increase in income is associated with a decrease in risk, a 1 point increase in health is associated with a decrease in risk, and using a net is associated with a 15 point decrease in risk. That’s the casual effect we’re building in to the DAG.\n  Generate random columns that stand alone. Generate related columns using regression math. Consider adding random noise. This is an entirely trial and error process until you get numbers that look good. Rely heavily on plots as you try different coefficients and parameters. Optionally rescale any columns that go out of reasonable bounds. If you rescale, you’ll need to tinker with the coefficients you used since the final effects will also get rescaled.\nLet’s make this data. It’ll be a lot easier than the full DAG we did before. Again, I’ll comment the code below so you can see what’s happening at each step.\n# Make this randomness consistent set.seed(1234) # Simulate 793 people (just for fun) n_people \u0026lt;- 793 rct_data \u0026lt;- tibble( # Make an ID column (not necessary, but nice to have) id = 1:n_people, # Generate income variable: normal, 500 ± 300 income = rnorm(n_people, mean = 500, sd = 75) ) %\u0026gt;% # Generate health variable: beta, centered around 70ish mutate(health_base = rbeta(n_people, shape1 = 7, shape2 = 4) * 100, # Health increases by 0.02 for every dollar in income health_income_effect = income * 0.02, # Make the final health score and add some noise health = health_base + health_income_effect + rnorm(n_people, mean = 0, sd = 3), # Rescale so it doesn\u0026#39;t go above 100 health = rescale(health, to = c(min(health), 100))) %\u0026gt;% # Randomly assign people to use a net (this is nice and easy!) mutate(net = rbinom(n_people, 1, 0.5)) %\u0026gt;% # Finally generate a malaria risk variable based on income, health, net use, # and random noise mutate(malaria_risk_base = rbeta(n_people, shape1 = 4, shape2 = 5) * 100, # Risk goes down by 10 when using a net. Because we rescale things, # though, we have to make the effect a lot bigger here so it scales # down to -10. Risk also decreases as health and income go up. I played # with these numbers until they created reasonable coefficients. malaria_effect = (-35 * net) + (-1.9 * health) + (-0.1 * income), # Make the final malaria risk score and add some noise malaria_risk = malaria_risk_base + malaria_effect + rnorm(n_people, 0, sd = 3), # Rescale so it doesn\u0026#39;t go below 0, malaria_risk = rescale(malaria_risk, to = c(5, 70))) # Look at all these columns! head(rct_data) ## # A tibble: 6 x 9 ## id income health_base health_income_effect health net malaria_risk_base malaria_effect malaria_risk ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 409. 57.2 8.19 61.3 1 37.4 -192. 35.1 ## 2 2 521. 63.3 10.4 69.4 0 33.0 -184. 37.6 ## 3 3 581. 61.8 11.6 71.9 1 36.4 -230. 24.4 ## 4 4 324. 42.2 6.48 45.5 1 52.7 -154. 52.8 ## 5 5 532. 72.1 10.6 78.5 1 41.9 -237. 23.6 ## 6 6 538. 82.0 10.8 89.1 0 46.6 -223. 29.9 Verify all relationships with plots and models.\nIncome still looks like it’s associated with health (which isn’t surprising, since it’s the same code we used for the full DAG earlier):\nggplot(net_data, aes(x = income, y = health)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;) lm(health ~ income, data = net_data) %\u0026gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 59.1 2.54 23.3 2.24e-98 ## 2 income 0.0169 0.00504 3.36 8.15e- 4 Try it out!\nIs the effect in there? With an RCT, all we really need to do is compare the outcome across treatment and control groups—because there’s no confounding, we don’t need to control for anything. Ordinarily we should check for balance across characteristics like health and income (and maybe generate other demographic columns) like we did in the RCT example, but we’ll skip all that here since we’re just checking to see if the effect is there.\nIt looks like using nets causes an average decrease of 10 risk points. Great!\n# Correct RCT-based ATE model_rct \u0026lt;- lm(malaria_risk ~ net, data = rct_data) tidy(model_rct) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 40.8 0.463 88.0 0. ## 2 net -10.2 0.653 -15.7 2.22e-48 Just for fun, if we control for health and income, we’ll get basically the same effect, since they don’t actualy confound the relationship and don’t really explain anything useful.\n# Controlling for stuff even though we don\u0026#39;t need to model_rct_controls \u0026lt;- lm(malaria_risk ~ net + health + income, data = rct_data) tidy(model_rct_controls) ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 97.7 1.45 67.3 0. ## 2 net -10.8 0.344 -31.3 2.27e-140 ## 3 health -0.586 0.0140 -41.8 1.67e-202 ## 4 income -0.0310 0.00230 -13.5 1.75e- 37 Save the data.\nThe data works, so let’s get rid of the intermediate columns we don’t need and save it as a CSV file.\n# In the end, all we need is id, income, health, net, and malaria risk: rct_data_final \u0026lt;- rct_data %\u0026gt;% select(id, income, health, net, malaria_risk) head(rct_data_final) ## # A tibble: 6 x 5 ## id income health net malaria_risk ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 409. 61.3 1 35.1 ## 2 2 521. 69.4 0 37.6 ## 3 3 581. 71.9 1 24.4 ## 4 4 324. 45.5 1 52.8 ## 5 5 532. 78.5 1 23.6 ## 6 6 538. 89.1 0 29.9 # Save it as a CSV file write_csv(rct_data_final, \u0026quot;data/bed_nets_rct.csv\u0026quot;)   Creating an effect for diff-in-diff Draw a DAG that maps out how all the columns you care about are related.\nDifference-in-differences approaches to causal inference are not based on models but on context or research design. You need comparable treatment and control groups before and after some policy or program is implemented.\nWe’ll keep with our mosquito net example and pretend that two cities in some country are dealing with malaria infections. City B rolls out a free net program in 2017; City A does not. Here’s what the DAG looks like:\ndid_dag \u0026lt;- dagify(mal ~ net + year + city, net ~ year + city, coords = list(x = c(mal = 3, net = 1, year = 2, city = 2), y = c(mal = 2, net = 2, year = 3, city = 1))) ggdag(did_dag) + theme_dag()  Specify how those nodes are measured.\nHere’s how we’ll measure these nodes:\n Malaria risk: scale from 0–100, mostly around 60, but ranging from 30ish to 90ish. Best to use a Beta distribution.\n Net use: binary 0/1, TRUE/FALSE variable. This is technically binomial, but we don’t need to simulate it since it will only happen for people who are in the treatment city after the universal net rollout.\n Year: year ranging from 2013 to 2020. Best to use a uniform distribution.\n City: binary 0/1, City A/City B variable. Best to use a binomial distribution.\n  Specify the relationships between the nodes based on the DAG equations.\nThere are two models in the DAG:\n net ~ year + city: Net use is determined by being in City B and being after 2017. We’ll assume perfect compliance here (but it’s fairly easy to simulate non-compliance and have some people in City A use nets after 2017, and some people in both cities use nets before 2017).\n mal ~ net + year + city: Malaria risk is determined by net use, year, and city. It’s determined by lots of other things too (like we saw in the previous DAGs), but since we’re assuming that the two cities are comparable treatment and control groups, we don’t need to worry about things like health, income, age, etc.\nWe’ll pretend that in general, City B has historicallly had a problem with malaria and people there have had higher risk: being in City B increases malaria risk by 5 points, on average. Over time, both cities have worked on mosquito abatement, so average malaria risk has decreased by 2 points per year (in both cities, because we believe in parallel trends). Using a mosquito net causes a decrease of 10 points on average. That’s our causal effect.\n  Generate random columns that stand alone. Generate related columns using regression math. Consider adding random noise. This is an entirely trial and error process until you get numbers that look good. Rely heavily on plots as you try different coefficients and parameters. Optionally rescale any columns that go out of reasonable bounds. If you rescale, you’ll need to tinker with the coefficients you used since the final effects will also get rescaled.\nGeneration time! Heavily annotated code below:\n# Make this randomness consistent set.seed(1234) # Simulate 2567 people (just for fun) n_people \u0026lt;- 2567 did_data \u0026lt;- tibble( # Make an ID column (not necessary, but nice to have) id = 1:n_people, # Generate year variable: uniform, between 2013 and 2020. Round so it\u0026#39;s whole. year = round(runif(n_people, min = 2013, max = 2020), 0), # Generate city variable: binomial, 50% chance of being in a city. We\u0026#39;ll use # sample() instead of rbinom() city = sample(c(\u0026quot;City A\u0026quot;, \u0026quot;City B\u0026quot;), n_people, replace = TRUE) ) %\u0026gt;% # Generate net variable. We\u0026#39;re assuming perfect compliance, so this will only # be TRUE for people in City B after 2017 mutate(net = ifelse(city == \u0026quot;City B\u0026quot; \u0026amp; year \u0026gt; 2017, TRUE, FALSE)) %\u0026gt;% # Generate a malaria risk variable based on year, city, net use, and random noise mutate(malaria_risk_base = rbeta(n_people, shape1 = 6, shape2 = 3) * 100, # Risk goes up if you\u0026#39;re in City B because they have a worse problem. # We could just say \u0026quot;city_effect = 5\u0026quot; and give everyone in City A an # exact 5-point boost, but to add some noise, we\u0026#39;ll give people an # average boost using rnorm(). Some people might go up 7, some might go # up 1, some might go down 2 city_effect = ifelse(city == \u0026quot;City B\u0026quot;, rnorm(n_people, mean = 5, sd = 2), 0), # Risk goes down by 2 points on average every year. Creating this # effect with regression would work fine (-2 * year), except the years # are huge here (-2 * 2013 and -2 * 2020, etc.) So first we create a # smaller year column where 2013 is year 1, 2014 is year 2, and so on, # that way we can say -2 * 1 and -2 * 6, or whatever. # Also, rather than make risk go down by *exactly* 2 every year, we\u0026#39;ll # add some noise with rnorm(), so for some people it\u0026#39;ll go down by 1 or # 4 or up by 1, and so on year_smaller = year - 2012, year_effect = rnorm(n_people, mean = -2, sd = 0.1) * year_smaller, # Using a net causes a decrease of 10 points, on average. Again, rather # than use exactly 10, we\u0026#39;ll use a distribution around 10. People only # get a net boost if they\u0026#39;re in City B after 2017. net_effect = ifelse(city == \u0026quot;City B\u0026quot; \u0026amp; year \u0026gt; 2017, rnorm(n_people, mean = -10, sd = 1.5), 0), # Finally combine all these effects to create the malaria risk variable malaria_risk = malaria_risk_base + city_effect + year_effect + net_effect, # Rescale so it doesn\u0026#39;t go below 0 or above 100 malaria_risk = rescale(malaria_risk, to = c(0, 100))) %\u0026gt;% # Make an indicator variable showing if the row is after 2017 mutate(after = year \u0026gt; 2017) head(did_data) ## # A tibble: 6 x 11 ## id year city net malaria_risk_base city_effect year_smaller year_effect net_effect malaria_risk after ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;lgl\u0026gt; ## 1 1 2014 City B FALSE 53.0 6.25 2 -3.97 0 54.2 FALSE ## 2 2 2017 City B FALSE 89.7 3.32 5 -9.17 0 84.1 FALSE ## 3 3 2017 City A FALSE 49.5 0 5 -10.1 0 37.6 FALSE ## 4 4 2017 City B FALSE 37.4 7.11 5 -9.44 0 33.1 FALSE ## 5 5 2019 City A FALSE 76.6 0 7 -14.7 0 61.1 TRUE ## 6 6 2017 City B FALSE 65.7 5.38 5 -10.4 0 59.9 FALSE Verify all relationships with plots and models.\nIs risk higher in City B? Yep.\nggplot(did_data, aes(x = city, y = malaria_risk, color = city)) + stat_summary(geom = \u0026quot;pointrange\u0026quot;, fun.data = \u0026quot;mean_se\u0026quot;) + guides(color = FALSE) Does risk decrease over time? And are the trends parallel? There was a weird random spike in City B in 2017 for whatever reason, but in general, the trends in the two cities are pretty parallel from 2013 to 2017.\nplot_data \u0026lt;- did_data %\u0026gt;% group_by(year, city) %\u0026gt;% summarize(mean_risk = mean(malaria_risk), se_risk = sd(malaria_risk) / sqrt(n()), upper = mean_risk + (1.96 * se_risk), lower = mean_risk + (-1.96 * se_risk)) ggplot(plot_data, aes(x = year, y = mean_risk, color = city)) + geom_vline(xintercept = 2017.5) + geom_ribbon(aes(ymin = lower, ymax = upper, fill = city), alpha = 0.3, color = FALSE) + geom_line() + theme(legend.position = \u0026quot;bottom\u0026quot;)  Try it out!\nLet’s see if it works! For diff-in-diff we need to use this model:\n\\[ \\text{Malaria risk} = \\alpha + \\beta\\ \\text{City B} + \\gamma\\ \\text{After 2017} + \\delta\\ (\\text{City B} \\times \\text{After 2017}) + \\varepsilon \\]\nmodel_did \u0026lt;- lm(malaria_risk ~ city + after + city*after, data = did_data) tidy(model_did) ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 59.2 0.549 108. 0. ## 2 cityCity B 5.17 0.778 6.64 3.71e-11 ## 3 afterTRUE -7.47 0.939 -7.96 2.61e-15 ## 4 cityCity B:afterTRUE -10.2 1.32 -7.79 9.67e-15 It works! Being in City B is associated with a 5-point higher risk on average; being after 2017 is associated with a 7.5-point lower risk on average, and being in City B after 2017 causes risk to drop by −10. The number isn’t exactly −10 here, since we rescaled the malaria_risk column a little, but still, it’s close. It’d probably be a good idea to build in some more noise and noncompliance, since the p-values are really really tiny here, but this is good enough for now.\nHere’s an obligatory diff-in-diff visualization:\nplot_data \u0026lt;- did_data %\u0026gt;% group_by(after, city) %\u0026gt;% summarize(mean_risk = mean(malaria_risk), se_risk = sd(malaria_risk) / sqrt(n()), upper = mean_risk + (1.96 * se_risk), lower = mean_risk + (-1.96 * se_risk)) # Extract parts of the model results for adding annotations model_results \u0026lt;- tidy(model_did) before_treatment \u0026lt;- filter(model_results, term == \u0026quot;(Intercept)\u0026quot;)$estimate + filter(model_results, term == \u0026quot;cityCity B\u0026quot;)$estimate diff_diff \u0026lt;- filter(model_results, term == \u0026quot;cityCity B:afterTRUE\u0026quot;)$estimate after_treatment \u0026lt;- before_treatment + diff_diff + filter(model_results, term == \u0026quot;afterTRUE\u0026quot;)$estimate ggplot(plot_data, aes(x = after, y = mean_risk, color = city, group = city)) + geom_pointrange(aes(ymin = lower, ymax = upper)) + geom_line() + annotate(geom = \u0026quot;segment\u0026quot;, x = FALSE, xend = TRUE, y = before_treatment, yend = after_treatment - diff_diff, linetype = \u0026quot;dashed\u0026quot;, color = \u0026quot;grey50\u0026quot;) + annotate(geom = \u0026quot;segment\u0026quot;, x = 2.1, xend = 2.1, y = after_treatment, yend = after_treatment - diff_diff, linetype = \u0026quot;dotted\u0026quot;, color = \u0026quot;blue\u0026quot;) + theme(legend.position = \u0026quot;bottom\u0026quot;)  Save the data.\nThe data works, so let’s get rid of the intermediate columns we don’t need and save it as a CSV file.\ndid_data_final \u0026lt;- did_data %\u0026gt;% select(id, year, city, net, malaria_risk) head(did_data_final) ## # A tibble: 6 x 5 ## id year city net malaria_risk ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 2014 City B FALSE 54.2 ## 2 2 2017 City B FALSE 84.1 ## 3 3 2017 City A FALSE 37.6 ## 4 4 2017 City B FALSE 33.1 ## 5 5 2019 City A FALSE 61.1 ## 6 6 2017 City B FALSE 59.9 # Save data write_csv(did_data_final, \u0026quot;data/diff_diff.csv\u0026quot;)   Creating an effect for regression discontinuity Draw a DAG that maps out how all the columns you care about are related.\nRegression discontinuity designs are also based on context instead of models, so the DAG is pretty simple. We’ll keep with our mosquito net example and pretend that families that earn less than $450 a week qualify for a free net. Here’s the DAG:\nrdd_dag \u0026lt;- dagify(mal ~ net + inc, net ~ cut, cut ~ inc, coords = list(x = c(mal = 4, net = 1, inc = 3, cut = 2), y = c(mal = 1, net = 1, inc = 2, cut = 1.75))) ggdag(rdd_dag) + theme_dag()  Specify how those nodes are measured.\nHere’s how we’ll measure these nodes:\n Malaria risk: scale from 0–100, mostly around 60, but ranging from 30ish to 90ish. Best to use a Beta distribution.\n Net use: binary 0/1, TRUE/FALSE variable. This is technically binomial, but we don’t need to simulate it since it will only happen for people who below the cutoff.\n Income: weekly income, measured in dollars, mostly around 500 ± 300. Best to use a normal distribution.\n Cutoff: binary 0/1, below/above $450 variable. This is technically binomial, but we don’t need to simulate it since it is entirely based on income.\n  Specify the relationships between the nodes based on the DAG equations.\nThere are three models in the DAG:\n cut ~ inc: Being above or below the cutpoint is determined by income. We know the cutoff is 450, so we just make an indicator showing if people are below that.\n net ~ cut: Net usage is determined by the cutpoint. If people are below the cutpoint, they’ll use a net; if not, they won’t. We can build in noncompliance here if we want and use fuzzy regression discontinuity. For the sake of this example, we’ll do it both ways, just so you can see both sharp and fuzzy synthetic data.\n mal ~ net + inc: Malaria risk is determined by both net usage and income. It’s also determined by lots of other things (age, education, city, etc.), but we don’t need to include those in the DAG because we’re using RDD to say that we have good treatment and control groups right around the cutoff.\nWe’ll pretend that a 1 dollar increase in income is associated with a drop in risk of 0.01, and that using a mosquito net causes a decrease of 10 points on average. That’s our causal effect.\n  Generate random columns that stand alone. Generate related columns using regression math. Consider adding random noise. This is an entirely trial and error process until you get numbers that look good. Rely heavily on plots as you try different coefficients and parameters. Optionally rescale any columns that go out of reasonable bounds. If you rescale, you’ll need to tinker with the coefficients you used since the final effects will also get rescaled.\nLet’s fake some data! Heavily annotated code below:\n# Make this randomness consistent set.seed(1234) # Simulate 5441 people (we need a lot bc we\u0026#39;re throwing most away) n_people \u0026lt;- 5441 rdd_data \u0026lt;- tibble( # Make an ID column (not necessary, but nice to have) id = 1:n_people, # Generate income variable: normal, 500 ± 300 income = rnorm(n_people, mean = 500, sd = 75) ) %\u0026gt;% # Generate cutoff variable mutate(below_cutoff = ifelse(income \u0026lt; 450, TRUE, FALSE)) %\u0026gt;% # Generate net variable. We\u0026#39;ll make two: one that\u0026#39;s sharp and has perfect # compliance, and one that\u0026#39;s fuzzy # Here\u0026#39;s the sharp one. It\u0026#39;s easy. If you\u0026#39;re below the cutoff you use a net. mutate(net_sharp = ifelse(below_cutoff == TRUE, TRUE, FALSE)) %\u0026gt;% # Here\u0026#39;s the fuzzy one, which is a little trickier. If you\u0026#39;re far away from # the cutoff, you follow what you\u0026#39;re supposed to do (like if your income is # 800, you don\u0026#39;t use the program; if your income is 200, you definitely use # the program). But if you\u0026#39;re close to the cutoff, we\u0026#39;ll pretend that there\u0026#39;s # an 80% chance that you\u0026#39;ll do what you\u0026#39;re supposed to do. mutate(net_fuzzy = case_when( # If your income is between 450 and 500, there\u0026#39;s a 20% chance of using the program income \u0026gt;= 450 \u0026amp; income \u0026lt;= 500 ~ sample(c(TRUE, FALSE), n_people, replace = TRUE, prob = c(0.2, 0.8)), # If your income is above 500, you definitely don\u0026#39;t use the program income \u0026gt; 500 ~ FALSE, # If your income is between 400 and 450, there\u0026#39;s an 80% chance of using the program income \u0026lt; 450 \u0026amp; income \u0026gt;= 400 ~ sample(c(TRUE, FALSE), n_people, replace = TRUE, prob = c(0.8, 0.2)), # If your income is below 400, you definitely use the program income \u0026lt; 400 ~ TRUE )) %\u0026gt;% # Finally we can make the malaria risk score, based on income, net use, and # random noise. We\u0026#39;ll make two outcomes: one using the sharp net use and one # using the fuzzy net use. They have the same effect built in, we just have to # use net_sharp and net_fuzzy separately. mutate(malaria_risk_base = rbeta(n_people, shape1 = 4, shape2 = 5) * 100) %\u0026gt;% # Make the sharp version. There\u0026#39;s really a 10 point decrease, but because of # rescaling, we use 15. I only chose 15 through lots of trial and error (i.e. # I used -11, ran the RDD model, and the effect was too small; I used -20, ran # the model, and the effect was too big; I kept changing numbers until landing # on -15). Risk also goes down as income increases. mutate(malaria_effect_sharp = (-15 * net_sharp) + (-0.01 * income), malaria_risk_sharp = malaria_risk_base + malaria_effect_sharp + rnorm(n_people, 0, sd = 3), malaria_risk_sharp = rescale(malaria_risk_sharp, to = c(5, 70))) %\u0026gt;% # Do the same thing, but with net_fuzzy mutate(malaria_effect_fuzzy = (-15 * net_fuzzy) + (-0.01 * income), malaria_risk_fuzzy = malaria_risk_base + malaria_effect_fuzzy + rnorm(n_people, 0, sd = 3), malaria_risk_fuzzy = rescale(malaria_risk_fuzzy, to = c(5, 70))) %\u0026gt;% # Make a version of income that\u0026#39;s centered at the cutpoint mutate(income_centered = income - 450) head(rdd_data) ## # A tibble: 6 x 11 ## id income below_cutoff net_sharp net_fuzzy malaria_risk_base malaria_effect_sharp malaria_risk_sharp malaria_effect_fuzzy malaria_risk_fuzzy income_centered ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 409. TRUE TRUE FALSE 56.5 -19.1 38.0 -4.09 47.5 -40.5 ## 2 2 521. FALSE FALSE FALSE 26.1 -5.21 28.6 -5.21 27.6 70.8 ## 3 3 581. FALSE FALSE FALSE 84.4 -5.81 64.7 -5.81 65.5 131. ## 4 4 324. TRUE TRUE TRUE 32.9 -18.2 25.9 -18.2 23.4 -126. ## 5 5 532. FALSE FALSE FALSE 53.1 -5.32 46.5 -5.32 44.3 82.2 ## 6 6 538. FALSE FALSE FALSE 45.7 -5.38 43.2 -5.38 40.2 88.0 Verify all relationships with plots and models.\nIs there a cutoff in the running variable when we use the sharp net variable? Yep!\nggplot(rdd_data, aes(x = income, y = net_sharp, color = below_cutoff)) + geom_vline(xintercept = 450) + geom_point(alpha = 0.3, position = position_jitter(width = NULL, height = 0.2)) + guides(color = FALSE) Is there a cutoff in the running variable when we use the fuzzy net variable? Yep! There are some richer people using the program and some poorer people not using it.\nggplot(rdd_data, aes(x = income, y = net_fuzzy, color = below_cutoff)) + geom_vline(xintercept = 450) + geom_point(alpha = 0.3, position = position_jitter(width = NULL, height = 0.2)) + guides(color = FALSE)  Try it out!\nLet’s test it! For sharp RDD we need to use this model:\n\\[ \\text{Malaria risk} = \\beta_0 + \\beta_1 \\text{Income}_\\text{centered} + \\beta_2 \\text{Net} + \\varepsilon \\]\nWe’ll use a bandwidth of ±$50, because why not. In real life you’d be more careful about bandwidth selection (or use rdbwselect() from the rdrobust package to find the optimal bandwidth)\nggplot(rdd_data, aes(x = income, y = malaria_risk_sharp, color = net_sharp)) + geom_vline(xintercept = 450) + geom_point(alpha = 0.2, size = 0.5) + # Add lines for the full range geom_smooth(data = filter(rdd_data, income_centered \u0026lt;= 0), method = \u0026quot;lm\u0026quot;, se = FALSE, size = 1, linetype = \u0026quot;dashed\u0026quot;) + geom_smooth(data = filter(rdd_data, income_centered \u0026gt; 0), method = \u0026quot;lm\u0026quot;, se = FALSE, size = 1, linetype = \u0026quot;dashed\u0026quot;) + # Add lines for bandwidth = 50 geom_smooth(data = filter(rdd_data, income_centered \u0026gt;= -50 \u0026amp; income_centered \u0026lt;= 0), method = \u0026quot;lm\u0026quot;, se = FALSE, size = 2) + geom_smooth(data = filter(rdd_data, income_centered \u0026gt; 0 \u0026amp; income_centered \u0026lt;= 50), method = \u0026quot;lm\u0026quot;, se = FALSE, size = 2) + theme(legend.position = \u0026quot;bottom\u0026quot;) model_sharp \u0026lt;- lm(malaria_risk_sharp ~ income_centered + net_sharp, data = filter(rdd_data, income_centered \u0026gt;= -50 \u0026amp; income_centered \u0026lt;= 50)) tidy(model_sharp) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 40.7 0.462 88.1 0. ## 2 income_centered -0.0197 0.0144 -1.37 1.72e- 1 ## 3 net_sharpTRUE -10.6 0.815 -13.0 1.67e-37 There’s an effect! For people in the bandwidth, the local average treatment effect of nets is a 10.6 point reduction in malaria risk.\nLet’s check if it works with the fuzzy version where there are noncompliers:\nggplot(rdd_data, aes(x = income, y = malaria_risk_fuzzy, color = net_fuzzy)) + geom_vline(xintercept = 450) + geom_point(alpha = 0.2, size = 0.5) + # Add lines for the full range geom_smooth(data = filter(rdd_data, income_centered \u0026lt;= 0), method = \u0026quot;lm\u0026quot;, se = FALSE, size = 1, linetype = \u0026quot;dashed\u0026quot;) + geom_smooth(data = filter(rdd_data, income_centered \u0026gt; 0), method = \u0026quot;lm\u0026quot;, se = FALSE, size = 1, linetype = \u0026quot;dashed\u0026quot;) + # Add lines for bandwidth = 50 geom_smooth(data = filter(rdd_data, income_centered \u0026gt;= -50 \u0026amp; income_centered \u0026lt;= 0), method = \u0026quot;lm\u0026quot;, se = FALSE, size = 2) + geom_smooth(data = filter(rdd_data, income_centered \u0026gt; 0 \u0026amp; income_centered \u0026lt;= 50), method = \u0026quot;lm\u0026quot;, se = FALSE, size = 2) + theme(legend.position = \u0026quot;bottom\u0026quot;) There’s a gap, but it’s hard to measure since there are noncompliers on both sides. We can deal with the noncompliance if we use above/below the cutoff as an instrument (see the fuzzy regression discontinuity guide for a complete example). We should run this set of models:\n\\[ \\begin{aligned} \\widehat{\\text{Net}} \u0026amp;= \\gamma_0 + \\gamma_1 \\text{Income}_\\text{centered} + \\gamma_2 \\text{Below 450} + \\omega \\\\ \\text{Malaria risk} \u0026amp;= \\beta_0 + \\beta_1 \\text{Income}_\\text{centered} + \\beta_2 \\widehat{\\text{Net}} + \\epsilon \\end{aligned} \\]\nInstead of doing these two stages by hand (ugh), we’ll do the 2SLS regression with the iv_robust() function from the estimatr package:\nlibrary(estimatr) model_fuzzy \u0026lt;- iv_robust(malaria_risk_fuzzy ~ income_centered + net_fuzzy | income_centered + below_cutoff, data = filter(rdd_data, income_centered \u0026gt;= -50 \u0026amp; income_centered \u0026lt;= 50)) tidy(model_fuzzy) ## term estimate std.error statistic p.value conf.low conf.high df outcome ## 1 (Intercept) 40.73241 0.69045 58.994 0.000e+00 39.37843 42.086402 2220 malaria_risk_fuzzy ## 2 income_centered -0.01921 0.01423 -1.350 1.772e-01 -0.04711 0.008696 2220 malaria_risk_fuzzy ## 3 net_fuzzyTRUE -11.21929 1.31033 -8.562 2.034e-17 -13.78889 -8.649700 2220 malaria_risk_fuzzy The effect is slightly larger now (−11.2), but that’s because we are looking at a doubly local ATE: compliers in the bandwidth. But still, it’s close to −10, so that’s good. And we could probably get it closer if we did other mathy shenanigans like adding squared and cubed terms or using nonparametric estimation with rdrobust() in the rdrobust package.\n Save the data.\nThe data works, so let’s get rid of the intermediate columns we don’t need and save it as a CSV file. We’ll make two separate CSV files for fuzzy and sharp, just because.\nrdd_data_final_sharp \u0026lt;- rdd_data %\u0026gt;% select(id, income, net = net_sharp, malaria_risk = malaria_risk_sharp) head(rdd_data_final_sharp) ## # A tibble: 6 x 4 ## id income net malaria_risk ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 409. TRUE 38.0 ## 2 2 521. FALSE 28.6 ## 3 3 581. FALSE 64.7 ## 4 4 324. TRUE 25.9 ## 5 5 532. FALSE 46.5 ## 6 6 538. FALSE 43.2 rdd_data_final_fuzzy \u0026lt;- rdd_data %\u0026gt;% select(id, income, net = net_fuzzy, malaria_risk = malaria_risk_fuzzy) head(rdd_data_final_fuzzy) ## # A tibble: 6 x 4 ## id income net malaria_risk ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 409. FALSE 47.5 ## 2 2 521. FALSE 27.6 ## 3 3 581. FALSE 65.5 ## 4 4 324. TRUE 23.4 ## 5 5 532. FALSE 44.3 ## 6 6 538. FALSE 40.2 # Save data write_csv(rdd_data_final_sharp, \u0026quot;data/rdd_sharp.csv\u0026quot;) write_csv(rdd_data_final_fuzzy, \u0026quot;data/rdd_fuzzy.csv\u0026quot;)   Creating an effect for instrumental variables Draw a DAG that maps out how all the columns you care about are related.\nAs with diff-in-diff and regression discontinuity, instrumental variables are a design-based approach to causal inference and thus don’t require complicated models (but you can still add control variables!), so their DAGs are simpler. Once again we’ll look at the effect of mosquito nets on malaria risk, but this time we’ll say that we cannot possibly measure all the confounding factors between net use and malaria risk, so we’ll use an instrument to extract the exogeneity from net use.\nAs we talked about in Session 11, good plausible instruments are hard to find: they have to cause bed net use and not be related to malaria risk except through bed net use.\nFor this example, we’ll pretend that free bed nets are distributed from town halls around the country. We’ll use “distance to town hall” as our instrument, since it could arguably maybe work perhaps. Being closer to a town hall makes you more likely to use a net, but being closer to a town halls doesn’t make put you at higher or lower risk for malaria on its own—it does that only because it changes your likelihood of getting a net.\nThis is where the story for the instrument falls apart, actually; in real life, if you live far away from a town hall, you probably live further from health services and live in more rural places with worse mosquito abatement policies, so you’re probably at higher risk of malaria. It’s probably a bad instrument, but just go with it.\nHere’s the DAG:\niv_dag \u0026lt;- dagify(mal ~ net + U, net ~ dist + U, coords = list(x = c(mal = 4, net = 2, U = 3, dist = 1), y = c(mal = 1, net = 1, U = 2, dist = 1.5)), latent = \u0026quot;U\u0026quot;) ggdag_status(iv_dag) + guides(color = FALSE) + theme_dag()  Specify how those nodes are measured.\nHere’s how we’ll measure these nodes:\n Malaria risk: scale from 0–100, mostly around 60, but ranging from 30ish to 90ish. Best to use a Beta distribution.\n Net use: binary 0/1, TRUE/FALSE variable. However, since we want to use other variables that increase the likelihood of using a net, we’ll do some cool tricky stuff with a bed net score, like we did in the observational DAG example earlier.\n Distance: distance to nearest town hall, measured in kilometers, mostly around 3, with a left skewed long tail (i.e. most people live fairly close, some people live far away). Best to use a Beta distribution (to get the skewed shape) that we then rescale.\n Unobserved: who knows?! There are a lot of unknown confounders. We could generate columns like income, age, education, and health, make them mathematically related to malaria risk and net use, and then throw those columns away in the final data so they’re unobserved. That would be fairly easy and intuitive.\nFor the sake of simplicity here, we’ll make a column called “risk factors,” kind of like we did with the “ability” column in the instrumental variables example—it’s a magical column that is unmeasurable, but it’ll open a backdoor path between net use and malaria risk and thus create endogeneity. It’ll be normally distributed around 50, with a standard deviation of 25.\n  Specify the relationships between the nodes based on the DAG equations.\nThere are two models in the DAG:\n net ~ dist + U: Net usage is determined by both distance and our magical unobserved risk factor column. Net use is technically binomial, but in order to change the likelihood of net use based on distance to town hall and unobserved stuff, we’ll do the fancy tricky stuff we did in the observational DAG section above: we’ll create a bed net score, increase or decrease that score based on risk factors and distance, scale that score to a 0-1 scale of probabilities, and then draw a binomial 0/1 outcome using those probabilities.\nWe’ll say that a one kilometer increase in the distance to a town halls reduces the bed net score and a one point increase in risk factors reduces the bed net score.\n mal ~ net + U: Malaria risk is determined by both net usage and unkown stuff, or the magical column we’re calling “risk factors.” We’ll say that a one point increase in risk factors increases malaria risk, and that using a mosquito net causes a decrease of 10 points on average. That’s our causal effect.\n  Generate random columns that stand alone. Generate related columns using regression math. Consider adding random noise. This is an entirely trial and error process until you get numbers that look good. Rely heavily on plots as you try different coefficients and parameters. Optionally rescale any columns that go out of reasonable bounds. If you rescale, you’ll need to tinker with the coefficients you used since the final effects will also get rescaled.\nFake data time! Here’s some heavily annotated code:\n# Make this randomness consistent set.seed(1234) # Simulate 1578 people (just for fun) n_people \u0026lt;- 1578 iv_data \u0026lt;- tibble( # Make an ID column (not necessary, but nice to have) id = 1:n_people, # Generate magical unobserved risk factor variable: normal, 500 ± 300 risk_factors = rnorm(n_people, mean = 100, sd = 25), # Generate distance to town hall variable distance = rbeta(n_people, shape1 = 1, shape2 = 4) ) %\u0026gt;% # Scale up distance to be 0.1-15 instead of 0-1 mutate(distance = rescale(distance, to = c(0.1, 15))) %\u0026gt;% # Generate net variable based on distance, risk factors, and random noise # Note: These -40 and -2 effects are entirely made up and I got them through a # lot of trial and error and rerunning this stupid chunk dozens of times mutate(net_score = 0 + (-40 * distance) + # Distance effect (-2 * risk_factors) + # Risk factor effect rnorm(n_people, mean = 0, sd = 50), # Random noise net_probability = rescale(net_score, to = c(0.15, 1)), # Randomly generate a 0/1 variable using that probability net = rbinom(n_people, 1, net_probability)) %\u0026gt;% # Generate malaria risk variable based on net use, risk factors, and random noise mutate(malaria_risk_base = rbeta(n_people, shape1 = 7, shape2 = 5) * 100, # We\u0026#39;re aiming for a -10 net effect, but need to boost it because of rescaling malaria_effect = (-20 * net) + (0.5 * risk_factors), # Make the final malaria risk score malaria_risk = malaria_risk_base + malaria_effect, # Rescale so it doesn\u0026#39;t go below 0 malaria_risk = rescale(malaria_risk, to = c(5, 80))) iv_data ## # A tibble: 1,578 x 9 ## id risk_factors distance net_score net_probability net malaria_risk_base malaria_effect malaria_risk ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 69.8 3.98 -202. 0.766 1 71.1 14.9 36.5 ## 2 2 107. 2.14 -284. 0.686 1 75.4 33.5 49.5 ## 3 3 127. 5.47 -469. 0.505 0 57.4 63.6 56.4 ## 4 4 41.4 9.61 -414. 0.558 0 37.2 20.7 20.4 ## 5 5 111. 4.66 -376. 0.596 0 38.5 55.4 40.9 ## 6 6 113. 2.29 -284. 0.686 1 75.7 36.3 51.3 ## 7 7 85.6 0.922 -215. 0.753 0 28.7 42.8 28.2 ## 8 8 86.3 12.5 -608. 0.368 1 50.6 23.2 29.4 ## 9 9 85.9 3.11 -267. 0.703 1 38.4 22.9 22.4 ## 10 10 77.7 1.35 -157. 0.810 1 69.1 18.9 37.6 ## # … with 1,568 more rows Verify all relationships with plots and models.\nIs there a relationship between unobserved risk factors and malaria risk? Yep.\nggplot(iv_data, aes(x = risk_factors, y = malaria_risk)) + geom_point(aes(color = as.factor(net))) + geom_smooth(method = \u0026quot;lm\u0026quot;) Is there a relationship between distance to town hall and net use? Yeah, those who live further away are less likely to use a net.\nggplot(iv_data, aes(x = distance, fill = as.factor(net))) + geom_density(alpha = 0.7) Is there a relationship between net use and malaria risk? Haha, yeah, that’s a huge highly significant effect. Probably too perfect. We could increase those error bars if we tinker with some of the numbers in the code, but for the sake of this example, we’ll leave them like this.\nggplot(iv_data, aes(x = as.factor(net), y = malaria_risk, color = as.factor(net))) + stat_summary(geom = \u0026quot;pointrange\u0026quot;, fun.data = \u0026quot;mean_se\u0026quot;)  Try it out!\nCool, let’s see if this works. Remember, we can’t actually use the risk_factors column in real life, but we will here just to make sure the effect we built in exists. Here’s the true effect, where using a net causes a decrease of 10.9 malaria risk points\nmodel_forbidden \u0026lt;- lm(malaria_risk ~ net + risk_factors, data = iv_data) tidy(model_forbidden) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 20.6 0.873 23.6 6.98e-106 ## 2 net -10.8 0.400 -27.1 3.93e-133 ## 3 risk_factors 0.283 0.00788 35.9 1.16e-206 Since we can’t actually use that column, we’ll use distance to town hall as an instrument. We should run this set of models:\n\\[ \\begin{aligned} \\widehat{\\text{Net}} \u0026amp;= \\gamma_0 + \\gamma_1 \\text{Distance to town hall} + \\omega \\\\ \\text{Malaria risk} \u0026amp;= \\beta_0 + \\beta_1 \\widehat{\\text{Net}} + \\epsilon \\end{aligned} \\]\nWe’ll run this 2SLS model with the iv_robust() function from the estimatr package:\nlibrary(estimatr) model_iv \u0026lt;- iv_robust(malaria_risk ~ net | distance, data = iv_data) tidy(model_iv) ## term estimate std.error statistic p.value conf.low conf.high df outcome ## 1 (Intercept) 47.202 1.576 29.95 2.344e-156 44.11 50.294 1576 malaria_risk ## 2 net -8.236 2.474 -3.33 8.889e-04 -13.09 -3.385 1576 malaria_risk …and it’s relatively close, I guess, at −8.2. Getting instrumental variables to find exact causal effects is tricky, but I’m fine with this for simulated data.\n Save the data.\nThe data works well enough, so we’ll get rid of the extra intermediate columns and save it as a CSV file. We’ll keep the forbidden risk_factors column just for fun.\niv_data_final \u0026lt;- iv_data %\u0026gt;% select(id, net, distance, malaria_risk, risk_factors) head(iv_data_final) ## # A tibble: 6 x 5 ## id net distance malaria_risk risk_factors ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 1 3.98 36.5 69.8 ## 2 2 1 2.14 49.5 107. ## 3 3 0 5.47 56.4 127. ## 4 4 0 9.61 20.4 41.4 ## 5 5 0 4.66 40.9 111. ## 6 6 1 2.29 51.3 113. # Save data write_csv(iv_data_final, \u0026quot;data/bed_nets_iv.csv\u0026quot;)    Use synthetic data packages There are several R packages that let you generate synthetic data with built-in relationships in a more automatic way. They all work a little differently, and if you’re interested in trying them out, make sure you check the documentation for details.\nfabricatr The fabricatr package is a very powerful package for simulating data. It was invented specifically for using in preregistered studies, so it can handle a ton of different data structures like panel data and time series data. You can build in causal effects and force columns to be correlated with each other.\nfabricatr has exceptionally well-written documentation with like a billion detailed examples (see the right sidebar here). This is a gold standard package and you should most definitely check it out.\nHere’s a simple example of simulating a bunch of voters and making older ones more likely to vote:\nlibrary(fabricatr) set.seed(1234) fake_voters \u0026lt;- fabricate( # Make 100 people N = 100, # Age uniformly distributed between 18 and 85 age = round(runif(N, 18, 85)), # Older people more likely to vote turnout = draw_binary(prob = ifelse( age \u0026lt; 40, 0.4, 0.7), N = N) ) head(fake_voters) ## ID age turnout ## 1 001 26 0 ## 2 002 60 1 ## 3 003 59 1 ## 4 004 60 1 ## 5 005 76 1 ## 6 006 61 1 And here’s an example of country-year panel data where there are country-specific and year-specific effects on GDP:\nset.seed(1234) panel_global_data \u0026lt;- fabricate( years = add_level( N = 10, ts_year = 0:9, year_shock = rnorm(N, 0, 0.3) ), countries = add_level( N = 5, base_gdp = runif(N, 15, 22), growth_units = runif(N, 0.25, 0.5), growth_error = runif(N, 0.15, 0.5), nest = FALSE ), country_years = cross_levels( by = join(years, countries), gdp_measure = base_gdp + year_shock + (ts_year * growth_units) + rnorm(N, sd = growth_error) ) ) %\u0026gt;% # Scale up the years to be actual years instead of 1, 2, 3, etc. mutate(year = ts_year + 2010) head(panel_global_data) ## years ts_year year_shock countries base_gdp growth_units growth_error country_years gdp_measure year ## 1 01 0 -0.36212 1 17.22 0.4526 0.3096 01 17.07 2010 ## 2 02 1 0.08323 1 17.22 0.4526 0.3096 02 17.55 2011 ## 3 03 2 0.32533 1 17.22 0.4526 0.3096 03 18.72 2012 ## 4 04 3 -0.70371 1 17.22 0.4526 0.3096 04 17.99 2013 ## 5 05 4 0.12874 1 17.22 0.4526 0.3096 05 19.25 2014 ## 6 06 5 0.15182 1 17.22 0.4526 0.3096 06 19.63 2015 ggplot(panel_global_data, aes(x = year, y = gdp_measure, color = countries)) + geom_line() + labs(x = \u0026quot;Year\u0026quot;, y = \u0026quot;Log GDP\u0026quot;, color = \u0026quot;Countries\u0026quot;) That all just scratches the surface of what fabricatr can do. Again, check the examples and documentation and play around with it to see what else it can do.\n wakefield The wakefield package is jokingly named after Andrew Wakefield, the British researcher who invented fake data to show that the MMR vaccine causes autism. This package lets you quickly generate random fake datasets. It has a bunch of pre-set column possibilities, like age, color, Likert scales, political parties, religion, and so on, and you can also use standard R functions like rnorm(), rbinom(), or rbeta(). It also lets you create repeated measures (1st grade score, 2nd grade score, 3rd grade score, etc.) and build correlations between variables.\nYou should definitely look at the documentation to see a ton of examples of how it all works. Here’s a basic example:\nlibrary(wakefield) set.seed(1234) wakefield_data \u0026lt;- r_data_frame( n = 500, id, treatment = rbinom(1, 0.3), # 30% chance of being in treatment outcome = rnorm(mean = 500, sd = 100), race, age = age(x = 18:45), sex = sex_inclusive(), survey_question_1 = likert(), survey_question_2 = likert() ) head(wakefield_data) ## # A tibble: 6 x 8 ## ID treatment outcome Race age sex survey_question_1 survey_question_2 ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;ord\u0026gt; ## 1 001 0 544. White 35 Intersex Disagree Agree ## 2 002 0 606. White 38 Male Disagree Strongly Disagree ## 3 003 0 545. White 38 Female Neutral Strongly Agree ## 4 004 0 566. Black 45 Female Strongly Agree Disagree ## 5 005 1 386. Black 41 Male Disagree Agree ## 6 006 0 463. Hispanic 20 Female Disagree Agree  faux The faux package does some really neat things. We can create data that has built-in correlations without going through all the math. For instance, let’s say we have 3 variables A, B, and C that are normally distributed with these parameters:\n A: mean = 10, sd = 2 B: mean = 5, sd = 1 C: mean = 20, sd = 5  We want A to correlate with B at r = 0.8 (highly correlated), A to correlate with C at r = 0.3 (less correlated), and B to correlate with C at r = 0.4 (moderately correlated). Here’s how to create that data with faux:\nlibrary(faux) set.seed(1234) faux_data \u0026lt;- rnorm_multi(n = 100, mu = c(10, 5, 20), sd = c(2, 1, 5), r = c(0.8, 0.3, 0.4), varnames = c(\u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;C\u0026quot;), empirical = FALSE) head(faux_data) ## A B C ## 1 11.742 5.612 25.89 ## 2 9.060 4.177 18.79 ## 3 9.373 4.466 14.57 ## 4 10.913 5.341 31.88 ## 5 8.221 4.039 18.14 ## 6 10.095 4.517 17.43 # Check averages and standard deviations faux_data %\u0026gt;% # Convert to long/tidy so we can group and summarize pivot_longer(cols = everything(), names_to = \u0026quot;variable\u0026quot;, values_to = \u0026quot;value\u0026quot;) %\u0026gt;% group_by(variable) %\u0026gt;% summarize(mean = mean(value), sd = sd(value)) ## # A tibble: 3 x 3 ## variable mean sd ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 A 10.2 2.08 ## 2 B 5.02 1.00 ## 3 C 20.8 5.01 # Check correlations cor(faux_data$A, faux_data$B) ## [1] 0.808 cor(faux_data$A, faux_data$C) ## [1] 0.301 cor(faux_data$B, faux_data$C) ## [1] 0.4598 faux can do a ton of other things too, so make sure you check out the documentation and all the articles with examples here.\n  ","date":1604102400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"7ac803d99d5c43b4b876fbc82e686f11","permalink":"https://evalf20.classes.andrewheiss.com/example/synthetic-data/","publishdate":"2020-10-31T00:00:00Z","relpermalink":"/example/synthetic-data/","section":"example","summary":"Basic example  Relationships and regression Explanatory variables linked to outcome; no connection between explanatory variables Explanatory variables linked to outcome; connection between explanatory variables Adding extra noise  Visualizing variables and relationships  Visualizing one variable Visualizing two continuous variables Visualizing a binary variable and a continuous variable  Specific examples  tl;dr: The general process Creating an effect in an observational DAG Brief pep talk intermission Creating an effect for RCTs Creating an effect for diff-in-diff Creating an effect for regression discontinuity Creating an effect for instrumental variables  Use synthetic data packages  fabricatr wakefield faux    In the example guide for generating random numbers, we explored how to use a bunch of different statistical distributions to create variables that had reasonable values.","tags":null,"title":"The ultimate guide to generating synthetic data for causal inference","type":"docs"},{"authors":null,"categories":null,"content":"    Program background Noncompliance around a cutoff Visualizing a fuzzy gap Measuring a fuzzy gap  Fuzzy parametric estimation Fuzzy nonparametric estimation    Program background In this example, we’ll use the same situation that we used in the the example for regression discontinuity:\n Students take an entrance exam at the beginning of the school year If they score 70 or below, they are enrolled in a free tutoring program Students take an exit exam at the end of the year  If you want to follow along, download this dataset and put it in a folder named data:\n  tutoring_program_fuzzy.csv  library(tidyverse) # ggplot(), %\u0026gt;%, mutate(), and friends library(broom) # Convert models to data frames library(rdrobust) # For robust nonparametric regression discontinuity library(estimatr) # Run 2SLS models in one step with iv_robust() library(modelsummary) # Create side-by-side regression tables library(kableExtra) # Fancy table formatting tutoring \u0026lt;- read_csv(\u0026quot;data/tutoring_program_fuzzy.csv\u0026quot;)  Noncompliance around a cutoff In the example for regression discontinuity, it was fairly easy to measure the size of the jump at the cutoff because compliance was perfect. No people who scored above the threshold used the tutoring program, and nobody who qualified for the program didn’t use it. It was a sharp design, since program usage looked like this:\nHowever, seeing a cutoff this sharp and this perfect is fairly rare. It is possible that some people scored higher on the entrace exam and somehow used tutoring, or that some people scored below the threshold but didn’t participate in the program, either because they’re never-takers, or because they fell through bureaucratic cracks.\nMore often, you’ll see compliance that looks like this:\nggplot(tutoring, aes(x = entrance_exam, y = tutoring_text, color = entrance_exam \u0026lt;= 70)) + # Make points small and semi-transparent since there are lots of them geom_point(size = 1.5, alpha = 0.5, position = position_jitter(width = 0, height = 0.25, seed = 1234)) + # Add vertical line geom_vline(xintercept = 70) + # Add labels labs(x = \u0026quot;Entrance exam score\u0026quot;, y = \u0026quot;Participated in tutoring program\u0026quot;) + # Turn off the color legend, since it\u0026#39;s redundant guides(color = FALSE) We can see the count and percentages of compliance with group_by() and summarize():\ntutoring %\u0026gt;% group_by(tutoring, entrance_exam \u0026lt;= 70) %\u0026gt;% summarize(count = n()) %\u0026gt;% group_by(tutoring) %\u0026gt;% mutate(prop = count / sum(count)) ## # A tibble: 4 x 4 ## # Groups: tutoring [2] ## tutoring `entrance_exam \u0026lt;= 70` count prop ## \u0026lt;lgl\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 FALSE FALSE 646 0.947 ## 2 FALSE TRUE 36 0.0528 ## 3 TRUE FALSE 116 0.365 ## 4 TRUE TRUE 202 0.635 Here we have 36 people who should have used tutoring who didn’t (either because they’re never-takers and are anti-program, or because the system failed them), and we have 116 people (!!!) who somehow snuck into the program. That should probably be a big red flag for the program administrators. That means that 36.5% of people in the tutoring program shouldn’t have been there. Big yikes.\nThis is definitely not a sharp design. This is a fuzzy regression discontinuity.\n Visualizing a fuzzy gap With regular sharp RD, our goal is to measure the size of the gap or discontinuity in outcome right at the cutoff. In our sharp example we did this with different parametric regression models, as well as with the rdrobust() function for nonparametric measurement.\nRegular parametric regression won’t really work here because we have strange compliance issues:\nggplot(tutoring, aes(x = entrance_exam, y = exit_exam, color = tutoring)) + geom_point(size = 1, alpha = 0.5) + # Add a line based on a linear model for the people scoring less than 70 geom_smooth(data = filter(tutoring, entrance_exam \u0026lt;= 70), method = \u0026quot;lm\u0026quot;) + # Add a line based on a linear model for the people scoring 70 or more geom_smooth(data = filter(tutoring, entrance_exam \u0026gt; 70), method = \u0026quot;lm\u0026quot;) + geom_vline(xintercept = 70) + labs(x = \u0026quot;Entrance exam score\u0026quot;, y = \u0026quot;Exit exam score\u0026quot;, color = \u0026quot;Used tutoring\u0026quot;) There’s still a visible gap at 70, but there are people who did and did not use the program on both sides of the cutoff.\nAnother way to look at this is to make a sort of histogram that shows the probability of being in the tutoring program at different entrance exam scores. 100% of people who score between 25 and 50 on the exam used tutoring, so that’s good, but then the probability of tutoring drops to ≈80ish% up until the cutpoint at 70. After 70, there’s a 10–15% chance of using tutoring if you’re above the threshold.\nIf this were a sharp design, every single bar to the left of the cutpoint would be 100% and every single bar to the right would be 0%, but that’s not the case here. The probability of tutoring changes at the cutpoint, but it’s not 100% perfect.\n# This fun code uses cut() to split the entrance exam column into distinct # categories (0-5, 5-10, 10-15, etc.). You\u0026#39;ll see some strange syntax in the # categories it creates: (70, 75]. These ranges start with ( and end with ] for # a reason: ( means the range *does not* include the number, while ] means that # the range *does* include the number. (70, 75] thus means 71-75. You can # reverse that with an argument to cut() so taht it would do [70, 75), which # means 70-74. tutoring_with_bins \u0026lt;- tutoring %\u0026gt;% mutate(exam_binned = cut(entrance_exam, breaks = seq(0, 100, 5))) %\u0026gt;% # Group by each of the new bins and tutoring status group_by(exam_binned, tutoring) %\u0026gt;% # Count how many people are in each test bin + used/didn\u0026#39;t use tutoring summarize(n = n()) %\u0026gt;% # Make this summarized data wider so that there\u0026#39;s a column for tutoring and no tutoring pivot_wider(names_from = \u0026quot;tutoring\u0026quot;, values_from = \u0026quot;n\u0026quot;, values_fill = 0) %\u0026gt;% rename(tutor_yes = `TRUE`, tutor_no = `FALSE`) %\u0026gt;% # Find the probability of tutoring in each bin by taking # the count of yes / count of yes + count of no mutate(prob_tutoring = tutor_yes / (tutor_yes + tutor_no)) # Plot this puppy ggplot(tutoring_with_bins, aes(x = exam_binned, y = prob_tutoring)) + geom_col() + geom_vline(xintercept = 8.5) + labs(x = \u0026quot;Entrance exam score\u0026quot;, y = \u0026quot;Proportion of people participating in program\u0026quot;)  Measuring a fuzzy gap So how do we actually measure this gap, given all the compliance issues? Recall from Session 12 that instruments let us isolate causal effects for just compliers: they let us find the complier average causal effect, or CACE.\nBut what should we use as an instrument? Do we use something weird like the Scrabble score of people’s names? Something overused like rainfall?\nNo! In this case, the instrument is fairly easy and straightforward: we create a variable that indicates if someone is above or below the threshold. That’s all. This variable essentially measures what should have happened rather than what actually happened.\nSurprisingly, it meets all the qualifications of an instrument too:\n Relevance (\\(Z \\rightarrow X\\) and \\(\\operatorname{Cor}(Z, X) \\neq 0\\)): The cutoff causes access to the tutoring program. Exclusion (\\(Z \\rightarrow X \\rightarrow Y\\) and \\(Z \\nrightarrow Y\\) and \\(\\operatorname{Cor}(Z, Y | X) = 0\\)): The cutoff causes exit exam scores only through the tutoring program. Exogeneity (\\(U \\nrightarrow Z\\) and \\(\\operatorname{Cor}(Z, U) = 0\\)): Unobserved confounders between the tutoring program and exit exam scores are unrelated to the cutoff.  Fuzzy parametric estimation Let’s make an instrument! We’ll also center the running variable just like we did with sharp regression discontinuity:\ntutoring_centered \u0026lt;- tutoring %\u0026gt;% mutate(entrance_centered = entrance_exam - 70, below_cutoff = entrance_exam \u0026lt;= 70) tutoring_centered ## # A tibble: 1,000 x 7 ## id entrance_exam tutoring tutoring_text exit_exam entrance_centered below_cutoff ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;lgl\u0026gt; ## 1 1 92.4 FALSE No tutor 78.1 22.4 FALSE ## 2 2 72.8 FALSE No tutor 58.2 2.77 FALSE ## 3 3 53.7 TRUE Tutor 62.0 -16.3 TRUE ## 4 4 98.3 FALSE No tutor 67.5 28.3 FALSE ## 5 5 69.7 TRUE Tutor 54.1 -0.288 TRUE ## 6 6 68.1 TRUE Tutor 60.1 -1.93 TRUE ## 7 7 86.0 FALSE No tutor 73.1 16.0 FALSE ## 8 8 85.7 TRUE Tutor 76.7 15.7 FALSE ## 9 9 85.9 FALSE No tutor 57.8 15.9 FALSE ## 10 10 89.5 FALSE No tutor 79.9 19.5 FALSE ## # … with 990 more rows Now we have a new column named below_cutoff that we’ll use as an instrument. Most of the time this will be the same as the tutoring column, since most people are compliers. But some people didn’t comply, like person 8 here who was not below the cutoff but still used the tutoring program.\nBefore using the instrument, let’s first run a model that assumes the cutoff is sharp. As we did with the sharp parametric analysis, we’ll include two explanatory variables:\n\\[ \\text{Exit exam} = \\beta_0 + \\beta_1 \\text{Entrance exam score}_\\text{centered} + \\beta_2 \\text{Tutoring program} + \\epsilon \\]\nWe’ll use a bandwidth of 10:\n# Bandwidth ±10 model_sans_instrument \u0026lt;- lm(exit_exam ~ entrance_centered + tutoring, data = filter(tutoring_centered, entrance_centered \u0026gt;= -10 \u0026amp; entrance_centered \u0026lt;= 10)) tidy(model_sans_instrument) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 59.3 0.503 118. 9.75e-313 ## 2 entrance_centered 0.511 0.0665 7.69 1.17e- 13 ## 3 tutoringTRUE 11.5 0.744 15.4 1.77e- 42 Here, the coefficient for tutoringTRUE shows the size of the jump, which is 11.5. This means that participating in the tutoring program causes an increase of 11.5 points on the final exam for people in the bandwidth.\nBUT THIS IS WRONG. This is not a sharp discontinuity, so we can’t actually do this. Instead, we need to run a 2SLS model that includes our instrument in the first stage, which will then remove the endogeneity built into participation in the program. We’ll estimate this set of models:\n\\[ \\begin{aligned} \\widehat{\\text{Tutoring program}} \u0026amp;= \\gamma_0 + \\gamma_1 \\text{Entrance exam score}_\\text{centered} + \\gamma_2 \\text{Below cutoff} + \\omega \\\\ \\text{Exit exam} \u0026amp;= \\beta_0 + \\beta_1 \\text{Entrance exam score}_\\text{centered} + \\beta_2 \\widehat{\\text{Tutoring program}} + \\epsilon \\end{aligned} \\]\nWe could manually run the first stage model, generate predicted tutoring and then use those predicted values in the second stage model like we did in the instrumental variables example, but that’s tedious and nobody wants to do all that work. We’ll use iv_robust() from the estimatr package instead.\nmodel_fuzzy \u0026lt;- iv_robust( exit_exam ~ entrance_centered + tutoring | entrance_centered + below_cutoff, data = filter(tutoring_centered, entrance_centered \u0026gt;= -10 \u0026amp; entrance_centered \u0026lt;= 10) ) tidy(model_fuzzy) ## term estimate std.error statistic p.value conf.low conf.high df outcome ## 1 (Intercept) 60.14 1.018 59.1 9.7e-200 58.14 62.14 400 exit_exam ## 2 entrance_centered 0.44 0.099 4.4 1.4e-05 0.24 0.63 400 exit_exam ## 3 tutoringTRUE 9.74 1.912 5.1 5.4e-07 5.98 13.50 400 exit_exam Based on this model, using below_cutoff as an instrument, we can see that the coefficient for tutoringTRUE is different now! It’s 9.74, which means that the tutoring program causes an average increase of 9.74 points on the final exam for compliers in the bandwidth.\nNotice that last caveat. Because we’re working with regression discontinuity, we’re estimating a local average treatment effect (LATE) for people in the bandwidth. Because we’re working with instrumental variables, we’re estimating the LATE for compliers only. That means our fuzzy regression discontinuity result here is doubly robust.\nIf we compare this fuzzy result to the sharp result, we can see a sizable difference:\n# gof_omit here will omit goodness-of-fit rows that match any of the text. This # means \u0026#39;contains \u0026quot;IC\u0026quot; OR contains \u0026quot;Low\u0026quot; OR contains \u0026quot;Adj\u0026quot; OR contains \u0026quot;p.value\u0026quot; # OR contains \u0026quot;statistic\u0026quot; OR contains \u0026quot;se_type\u0026quot;\u0026#39;. Basically we\u0026#39;re getting rid of # all the extra diagnostic information at the bottom modelsummary(list(\u0026quot;No instrument (wrong)\u0026quot; = model_sans_instrument, \u0026quot;Fuzzy RD (bw = 10)\u0026quot; = model_fuzzy), gof_omit = \u0026#39;IC|Log|Adj|p\\\\.value|statistic|se_type\u0026#39;, stars = TRUE) %\u0026gt;% # Add a background color to row 5 row_spec(5, background = \u0026quot;#F5ABEA\u0026quot;)    No instrument (wrong)  Fuzzy RD (bw = 10)      (Intercept)  59.274***  60.141***     (0.503)  (1.018)    entrance_centered  0.511***  0.437***     (0.067)  (0.099)    tutoringTRUE  11.484***  9.741***     (0.744)  (1.912)    Num.Obs.  403     R2  0.373  0.365    F  119.103     N   403       * p \u0026lt; 0.1, ** p \u0026lt; 0.05, *** p \u0026lt; 0.01     We can (and should!) do all the other things that we talked about in the regression discontinuity example, like modifying the bandwidth, adding polynomial terms, and so forth to see how robust the finding is. But we won’t do any of that here.\n Fuzzy nonparametric estimation We can also use nonparametric methods to measure the size of the fuzzy gap at the cutoff. We’ll use rdrobust() just like we did in the sharp example. The only difference is that we have to add one extra argument. That’s it!\nTo do fuzzy estimation with rdrobust(), use the fuzzy argument to specify the treatment column (or tutoring in our case). Importantly (and confusingly! this took me waaaaay too long to figure out!), you do not need to specify an instrument (or even create one!). All you need to specify is the column that indicates treatment status—rdrobust() will do all the above/below-the-cutoff instrument stuff behind the scenes for you.\nrdrobust(y = tutoring$exit_exam, x = tutoring$entrance_exam, c = 70, fuzzy = tutoring$tutoring) %\u0026gt;% summary() ## Call: rdrobust ## ## Number of Obs. 1000 ## BW type mserd ## Kernel Triangular ## VCE method NN ## ## Number of Obs. 238 762 ## Eff. Number of Obs. 170 347 ## Order est. (p) 1 1 ## Order bias (q) 2 2 ## BW est. (h) 12.985 12.985 ## BW bias (b) 19.733 19.733 ## rho (h/b) 0.658 0.658 ## Unique Obs. 238 762 ## ## ============================================================================= ## Method Coef. Std. Err. z P\u0026gt;|z| [ 95% C.I. ] ## ============================================================================= ## Conventional 9.683 1.893 5.116 0.000 [5.973 , 13.393] ## Robust - - 4.258 0.000 [5.210 , 14.095] ## ============================================================================= That’s all! Using nonparametric methods, with a triangular kernel and a bandwidth of ±12.96, the causal effect of the tutoring program for compliers in the bandwidth is 9.683.\nWe can (and should!) do all the other nonparametric robustness checks that we talked about in the regression discontinuity example, like modifying the bandwidth (ideal, half, double) and messing with the kernel (uniform, triangular, Epanechnikov) to see how robust the finding is. But again, we won’t do any of that here.\n  ","date":1603843200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"d940cee85224f0943d5a116bb8b14e1f","permalink":"https://evalf20.classes.andrewheiss.com/example/rdd-fuzzy/","publishdate":"2020-10-28T00:00:00Z","relpermalink":"/example/rdd-fuzzy/","section":"example","summary":"Program background Noncompliance around a cutoff Visualizing a fuzzy gap Measuring a fuzzy gap  Fuzzy parametric estimation Fuzzy nonparametric estimation    Program background In this example, we’ll use the same situation that we used in the the example for regression discontinuity:\n Students take an entrance exam at the beginning of the school year If they score 70 or below, they are enrolled in a free tutoring program Students take an exit exam at the end of the year  If you want to follow along, download this dataset and put it in a folder named data:","tags":null,"title":"Fuzzy regression discontinuity","type":"docs"},{"authors":null,"categories":null,"content":"   Compliance and treatment effects Finding compliers with a mind-reading time machine Finding compliers in actual data Finding the ITT Finding the proportion of compliers Finding the CACE/LATE with IV/2SLS   Compliance and treatment effects Throughout this course, we’ve talked about the difference between the average treatment effect (ATE), or the average effect of a program for an entire population, and conditional average treatment effect (CATE), or the average effect of a program for some segment of the population. There are all sorts of CATEs: you can find the CATE for men vs. women, for people who are treated with the program (the average treatment on the treated, or ATT or TOT), for people who are not treated with the program (the average treatment on the untreated, or ATU), and so on.\nOne important type of CATE is the effect of a program on just those who comply with the program. We can call this the complier average treatment effect, but the acronym would be the same as conditional average treatment effect, so we’ll call it the complier average causal effect or CACE.\nThinking about compliance is important. You might randomly assign people to receive treatment or a program, but people might not do what you tell them. Additionally, people might do the program if assigned to do it, but they would have done it anyway. We can split the population into four types of people:\n Compliers: People who follow whatever their assignment is (if assigned to treatment, they do the program; if assigned to control, they don’t) Always takers: People who will receive or seek out the program regardless of assignment (if assigned to treatment, they do the program; if assigned to control, they still do the program) Never takers: People who will not receive or seek out the program regardless of assignment (if assigned to treatment, they don’t do the program; if assigned to control, they also don’t do it) Defiers: People who will do the opposite of whatever their assignment is (if assigned to treatment, they don’t do the program; if assigned to control, they do the program)  To simplify things, evaluators and econometricians assume that defiers don’t exist based on the idea of monotonicity, which means that we can assume that the effect of being assigned to treatment only increases the likelihood of participating in the program (and doesn’t make it more likely).\nThe tricky part about trying to find who the compliers are in a sample is that we can’t know what people would have done in the absence of treatment. If we see that someone in the experiment was assigned to be in the treatment group and they then participated in the program, they could be a complier (since they did what they were assigned to do), or they could be an always taker (they did what they were assigned to do, but they would have done it anyway). Due to the fundamental problem of causal inference, we cannot know what each person would have done in a parallel world.\nWe can use data from a hypothetical program to see how these three types of compliers distort our outcomes, and more importantly, how we can disentangle compliers from their always- and never-taker counterparts.\nIf you want to follow along with this example, you can download these two datasets:\n  bed_nets_time_machine.csv  bed_nets_observed.csv   Finding compliers with a mind-reading time machine First let’s load the data and reorder some of the categories:\nlibrary(tidyverse) # ggplot(), %\u0026gt;%, mutate(), and friends library(broom) # Convert models to data frames library(estimatr) # Run 2SLS models in one step with iv_robust() bed_nets \u0026lt;- read_csv(\u0026quot;data/bed_nets_observed.csv\u0026quot;) %\u0026gt;% # Make \u0026quot;No bed net\u0026quot; (control) come first mutate(bed_net = fct_relevel(bed_net, \u0026quot;No bed net\u0026quot;)) bed_nets_time_machine \u0026lt;- read_csv(\u0026quot;data/bed_nets_time_machine.csv\u0026quot;) %\u0026gt;% # Make \u0026quot;No bed net\u0026quot; come first and \u0026quot;Complier\u0026quot; come first mutate(bed_net = fct_relevel(bed_net, \u0026quot;No bed net\u0026quot;), status = fct_relevel(status, \u0026quot;Complier\u0026quot;)) This is what we would be able to see if we could read everyone’s minds. There are always takers who will use a bed net regardless of the program, and they’ll have higher health outcomes. However, those better outcomes are because of something endogenous—there’s something else that makes these people always pursue bed nets, and that’s likely related to health. We probably want to not consider them when looking for the program effect. There are never takers who won’t ever use a bed net, and they have worse health outcomes. Again, there’s endogeneity here—something is causing them to not use the bed nets, and it likely also causes their health level. We don’t want to look at them either.\nThe first group—the compliers—are the people we want to focus on. Here we see that the program had an effect when compared to a control group.\nset.seed(1234) # Make the jittering the same every time ggplot(bed_nets_time_machine, aes(y = health, x = treatment)) + geom_point(aes(shape = bed_net, color = status), position = position_jitter(height = NULL, width = 0.25)) + facet_wrap(vars(status)) + labs(color = \u0026quot;Type of person\u0026quot;, shape = \u0026quot;Compliance\u0026quot;, x = NULL, y = \u0026quot;Health status\u0026quot;) + scale_color_viridis_d(option = \u0026quot;plasma\u0026quot;, end = 0.85) + theme_bw()  Finding compliers in actual data This is what we actually see in the data, though. You can tell who some of the always takers are (those who used bed nets after being assigned to the control group) and who some of the never takers are (those who did not use a bed net after being assigned to the treatment group), but compliers are mixed up with the always and never takers. We have to somehow disentangle them!\nset.seed(1234) ggplot(bed_nets_time_machine, aes(y = health, x = bed_net)) + geom_point(aes(shape = bed_net, color = status), position = position_jitter(height = NULL, width = 0.25)) + facet_wrap(vars(treatment)) + labs(color = \u0026quot;Type of person\u0026quot;, shape = \u0026quot;Compliance\u0026quot;, x = NULL, y = \u0026quot;Health status\u0026quot;) + scale_color_viridis_d(option = \u0026quot;plasma\u0026quot;, end = 0.85) + theme_bw() We can do this by assuming the proportion of compliers, never takers, and always takers are equally spread across treatment and control (which we can assume through the magic of randomization). If that’s the case, we can calculate the intent to treat (ITT) effect, which is the CATE of being assigned treatment (or the effect of being assigned treatment on health status, regardless of actual compliance).\nThe ITT is actually composed of three different causal effects: the complier average causal effect (CACE), the always taker average causal effect (ATACE), and the never taker average causal effect (NTACE). In the formula below, \\(\\pi\\) stands for the proportion of people in each group. Formally, the ITT can be defined like this:\n\\[ \\begin{aligned} \\text{ITT}\\ =\\ \u0026amp; \\color{#0D0887}{\\pi_\\text{compliers} \\times (\\text{T} - \\text{C})_\\text{compliers}} + \\\\ \u0026amp;\\color{#B7318A}{\\pi_\\text{always takers} \\times (\\text{T} - \\text{C})_\\text{always takers}} + \\\\ \u0026amp;\\color{#FEBA2C}{\\pi_\\text{never takers} \\times (\\text{T} - \\text{C})_\\text{never takers}} \\end{aligned} \\]\nWe can simplify this to this acronymized version:\n\\[ \\text{ITT}\\ =\\ \\color{#0D0887}{\\pi_\\text{C} \\text{CACE}} + \\color{#B7318A}{\\pi_\\text{A} \\text{ATACE}} + \\color{#FEBA2C}{\\pi_\\text{N} \\text{NTACE}} \\]\nThe number we care about the most here is the CACE, which is stuck in the middle of the equation. But we can rescue it with some fun logical and algebraic trickery!\nIf we assume that assignment to treatment doesn’t make someone more likely to be an always taker or a never taker, we can set the ATACE and NTACE to zero, leaving us with just three variables to worry about: ITT, \\(\\pi_\\text{c}\\), and CACE:\n\\[ \\begin{aligned} \\text{ITT}\\ =\\ \u0026amp; \\color{#0D0887}{\\pi_\\text{C} \\text{CACE}} + \\color{#B7318A}{\\pi_\\text{A} \\text{ATACE}} + \\color{#FEBA2C}{\\pi_\\text{N} \\text{NTACE}} \\\\[6pt] =\\ \u0026amp; \\color{#0D0887}{\\pi_\\text{C} \\text{CACE}} + \\color{#B7318A}{\\pi_\\text{A} \\times 0} + \\color{#FEBA2C}{\\pi_\\text{N} \\times 0}\\\\[6pt] \\text{ITT}\\ =\\ \u0026amp; \\color{#0D0887}{\\pi_\\text{C} \\text{CACE}} \\end{aligned} \\]\nWe can use algebra to rearrange this formula so that we’re left with an equation that starts with CACE (since that’s the value we care about):\n\\[ \\text{CACE} = \\frac{\\text{ITT}}{\\pi_\\text{C}} \\]\nIf we can find the ITT and the proportion of compliers, we can find the complier average causal effect (CACE). Fortunately, both those pieces—ITT and \\(\\pi_\\text{C}\\)—are findable in the data we have!\n Finding the ITT The ITT is easy to find with a simple OLS model:\nitt_model \u0026lt;- lm(health ~ treatment, data = bed_nets) tidy(itt_model) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 40.9 0.444 92.1 0. ## 2 treatmentTreatment 5.99 0.630 9.51 5.36e-21 ITT \u0026lt;- tidy(itt_model) %\u0026gt;% filter(term == \u0026quot;treatmentTreatment\u0026quot;) %\u0026gt;% pull(estimate) The ITT here is ≈6—being assigned treatment increases average health status by 5.99 health points.\n Finding the proportion of compliers The proportion of compliers is a little trickier, but doable with some algebraic trickery. Recall from the graph above that the people who were in the treatment group and who complied are a combination of always takers and compliers. This means we can say:\n\\[ \\begin{aligned} \\pi_\\text{A} + \\pi_\\text{C} =\u0026amp; \\text{% yes in treatment; or} \\\\ \\pi_\\text{C} =\u0026amp; \\text{% yes in treatment} - \\pi_\\text{A} \\end{aligned} \\]\nWe actually know \\(\\pi_\\text{A}\\)—remember in the graph above that the people who were in the control group and who used bed nets are guaranteed to be always takers (none of them are compliers or never takers). If we assume that the proportion of always takers is the same in both treatment and control, we can use that percent here, giving us this final equation for \\(\\pi_\\text{C}\\):\n\\[ \\begin{aligned} \\pi_\\text{C} =\u0026amp; \\text{% yes in treatment} - \\pi_\\text{A} \\\\ =\u0026amp; \\text{% yes in treatment} - \\text{% yes in control} \\end{aligned} \\]\nSo, if we can find the percent of people assigned to treatment who used bed nets, find the percent of people assigned to control and used bed nets, and subtract the two percentages, we’ll have the proportion of compliers, or \\(\\pi_\\text{C}\\). We can do that with the data we have (61% - 19.5% = 41.5% compliers):\nbed_nets %\u0026gt;% group_by(treatment, bed_net) %\u0026gt;% summarize(n = n()) %\u0026gt;% mutate(prop = n / sum(n)) ## # A tibble: 4 x 4 ## # Groups: treatment [2] ## treatment bed_net n prop ## \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Control No bed net 808 0.805 ## 2 Control Bed net 196 0.195 ## 3 Treatment No bed net 388 0.390 ## 4 Treatment Bed net 608 0.610 # pi_c = prop yes in treatment - prop yes in control pi_c \u0026lt;- 0.6104418 - 0.1952191 Finally, now that we know both the ITT and \\(\\pi_\\text{C}\\), we can find the CACE (or the LATE):\nCACE \u0026lt;- ITT / pi_c CACE ## [1] 14.42 It’s 14.4, which means that using bed nets increased health by 14 health points for compliers (which is a lot bigger than the 6 that we found before). We successfully filtered out the always takers and the never takers, and we have our complier-specific causal effect.\n Finding the CACE/LATE with IV/2SLS Doing that is super tedious though! What if there was an easier way to find the effect of the bed net program for just the compliers? We can do this with IV/2SLS regression by using assignment to treatment as an instrument.\nAssignment to treatment works as an instrument because it’s (1) relevant, since being told to use bed nets is probably highly correlated with using bed nets, (2) exclusive, since the only way that being told to use bed nets can cause changes in health is through the actual use of the bed nets, and (3) exogenous, since being told to use bed nets probably isn’t related to other things that cause health.\nHere’s a 2SLS regression with assignment to treatment as the instrument:\nmodel_2sls \u0026lt;- iv_robust(health ~ bed_net | treatment, data = bed_nets) tidy(model_2sls) ## term estimate std.error statistic p.value conf.low conf.high df outcome ## 1 (Intercept) 38.12 0.5151 74.01 0.000e+00 37.11 39.13 1998 health ## 2 bed_netBed net 14.42 1.2538 11.50 1.087e-29 11.96 16.88 1998 health The coefficient for bed_net is identical to the CACE that we found manually! Instrumental variables are helpful for isolated program effects to only compliers when you’re dealing with noncompliance.\n ","date":1603843200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"245eb61eb69be58a4a364fd6f8b96603","permalink":"https://evalf20.classes.andrewheiss.com/example/cace/","publishdate":"2020-10-28T00:00:00Z","relpermalink":"/example/cace/","section":"example","summary":"Compliance and treatment effects Finding compliers with a mind-reading time machine Finding compliers in actual data Finding the ITT Finding the proportion of compliers Finding the CACE/LATE with IV/2SLS   Compliance and treatment effects Throughout this course, we’ve talked about the difference between the average treatment effect (ATE), or the average effect of a program for an entire population, and conditional average treatment effect (CATE), or the average effect of a program for some segment of the population.","tags":null,"title":"Complier average treatment effects","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Regression discontinuity  Slides Videos   Readings   Chapter 6 in Impact Evaluation in Practice Paul J. Gertler et al., Impact Evaluation in Practice, 2nd ed. (Inter-American Development Bank; World Bank, 2016), https://openknowledge.worldbank.org/handle/10986/25030.  Chapter 4 in Mastering ’Metrics Joshua D. Angrist and Jörn-Steffen Pischke, Mastering ’Metrics: The Path from Cause to Effect (Princeton, NJ: Princeton University Press, 2015).  “Regression discontinuity” in Causal Inference: The Mixtape Scott Cunningham, Causal Inference: The Mixtape, 2018, https://www.scunning.com/mixtape.html.  Regression discontinuity  The example page on regression discontinuity shows how to use R to analyze and estimate causal effects with regression discontinuity    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nIntroduction  Arbitrary cutoffs and causal inference  Drawing lines and measuring gaps  Main RDD concerns                Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Arbitrary cutoffs and causal inference Drawing lines and measuring gaps Main RDD concerns  You can also watch the playlist (and skip around to different sections) here:\n   ","date":1603670400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"71d614e4debdb64f2a26fc68ad217ddd","permalink":"https://evalf20.classes.andrewheiss.com/content/10-content/","publishdate":"2020-10-26T00:00:00Z","relpermalink":"/content/10-content/","section":"content","summary":"Readings  Regression discontinuity  Slides Videos   Readings   Chapter 6 in Impact Evaluation in Practice Paul J. Gertler et al., Impact Evaluation in Practice, 2nd ed. (Inter-American Development Bank; World Bank, 2016), https://openknowledge.worldbank.org/handle/10986/25030.  Chapter 4 in Mastering ’Metrics Joshua D. Angrist and Jörn-Steffen Pischke, Mastering ’Metrics: The Path from Cause to Effect (Princeton, NJ: Princeton University Press, 2015).  “Regression discontinuity” in Causal Inference: The Mixtape Scott Cunningham, Causal Inference: The Mixtape, 2018, https://www.","tags":null,"title":"Regression discontinuity I","type":"docs"},{"authors":null,"categories":null,"content":"  You’ll likely run into some errors when knitting with a modelsummary() table. That’s because the current version of the modelsummary package on CRAN doesn’t include support for robust standard errors from feols() models. You can install the latest version of modelsummary that does have support for robust standard errors by doing this:\nInstall the remotes package using the Packages panel in RStudio Run this in your console to install the latest version of modelsummary directly from GitHub: remotes::install_github('vincentarelbundock/modelsummary')   For this problem set, you’ll reproduce the results from one of the papers that you looked at in your threats to validity assignment:\n Rafael Di Tella and Ernesto Schargrodsky, “Do Police Reduce Crime? Estimates Using the Allocation of Police Forces After a Terrorist Attack,” American Economic Review 94, no. 1 (March 2004): 115–133, doi:10.1257/000282804322970733.\n The full published paper is posted on iCollege under “Content \u0026gt; Validity assignment” (since you used the paper for that assignment). It’s not posted publicly here because of copyright reasons. The data comes from Di Tella and Schargrodsky’s data appendix available at their study’s AER webpage, and I’ve included it in the .zip file for the assignment.\nThis paper uses difference-in-differences to estimate the causal effect of increased policing on car thefts. Now that you know all about diff-in-diff, you can create their same results!\nThe answer key from Problem Set 4 (where you also did diff-in-diff) + this example page will be incredibly useful for you:\n Difference-in-differences  You’ll be doing all your R work in R Markdown. You can download a zipped file of a pre-made project here:\n  problem-set-5.zip  And as always, if you’re struggling, please talk to me. Work with classmates too (especially for this assignment!). Don’t suffer in silence!\nInstructions If you’re using R on your own computer, download this file, unzip it, and double click on the file named problem-set-5.Rproj:  problem-set-5.zip\nYou’ll need to make sure you have these packages installed on your computer: tidyverse, haven, broom, fixest, and modelsummary. If you try to load one of those packages with library(tidyverse) or library(haven), etc., and R gives an error that the package is missing, use the “Packages” panel in RStudio to install it.\n(Alternatively, you can open the project named “Problem Set 5” on RStudio.cloud and complete the assignment in your browser without needing to install anything. This link should take you to the project—if it doesn’t, log in and look for the project named “Problem Set 5.”)\n Rename the R Markdown file named your-name_problem-set-5.Rmd to something that matches your name and open it in RStudio.\n Complete the tasks given in the R Markdown file. There are questions marked in bold. Your job is to answer those questions. You don’t need to put your answers in bold or ALL CAPS or anything, and you can remove the question text if you want.\nFill out code in the empty chunks provided (you can definitely copy, paste, and adapt from other code in the document or the example page on diff-in-diff—don’t try to write everything from scratch!).\nYou’ll need to insert your own code chunks. Rather than typing them by hand (that’s tedious!), use the “Insert” button at the top of the editing window, or press ⌥ + ⌘ + I on macOS, or ctrl + alt + I on Windows.\nRemember that you can run an entire chunk by clicking on the green play arrow in the top right corner of the chunk. You can also run lines of code line-by-line if you place your cursor on some R code and press ⌘ + enter (for macOS users) or ctrl + enter (for Windows users).\nMake sure you run each chunk sequentially. If you run a chunk in the middle of the document without running previous ones, it might not work, since previous chunks might do things that later chunks depend on.\n When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex) of your document. Upload that file to iCollege.\n   ","date":1603670400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"6e536602e9cf71a3c24283ef20842292","permalink":"https://evalf20.classes.andrewheiss.com/assignment/05-problem-set/","publishdate":"2020-10-26T00:00:00Z","relpermalink":"/assignment/05-problem-set/","section":"assignment","summary":"You’ll likely run into some errors when knitting with a modelsummary() table. That’s because the current version of the modelsummary package on CRAN doesn’t include support for robust standard errors from feols() models. You can install the latest version of modelsummary that does have support for robust standard errors by doing this:\nInstall the remotes package using the Packages panel in RStudio Run this in your console to install the latest version of modelsummary directly from GitHub: remotes::install_github('vincentarelbundock/modelsummary')   For this problem set, you’ll reproduce the results from one of the papers that you looked at in your threats to validity assignment:","tags":null,"title":"Problem set 5","type":"docs"},{"authors":null,"categories":null,"content":"    Video walk-through Program background Load and clean data Step 1: Determine if process of assigning treatment is rule-based Step 2: Determine if the design is fuzzy or sharp Step 3: Check for discontinuity in running variable around cutpoint Step 4: Check for discontinuity in outcome across running variable Step 5: Measure the size of the effect  Parametric estimation Nonparametric estimation  Step 6: Compare all the effects   Video walk-through If you want to follow along with this example, you can download the data below:\n  tutoring_program.csv  Important: The results will be slightly different here than they were in the video. That’s because in the data I used in the video, I didn’t round any values, so people could score a 71.8392403 on the entrance exam, which is kind of an excessive number of decimals. In this data, I rounded everything to one decimal point (e.g. 71.8), so the gaps and lines will be a tiny bit different here.\n There’s a set of videos that walks through each section below. To make it easier for you to jump around the video examples, I cut the long video into smaller pieces and included them all in one YouTube playlist.\n Getting started Sharp or fuzzy? Checking for manipulation Finding discontinuity in outcome Measuring the gap parametrically Measuring the gap nonparametrically  You can also watch the playlist (and skip around to different sections) here:\n   Program background In this hypothetical example, students take an entrance exam at the beginning of a school year. Those who score 70 or below are automatically enrolled in a free tutoring program and receive assistance throughout the year. At the end of the school year, students take a final test, or exit exam (with a maximum of 100 points) to measure how much they learned overall. Remember, this is a hypothetical example and tests like this don’t really exist, but just go with it.\nYou have a dataset with four columns in it:\n id: The ID of the student entrance_exam: The student’s entrance exam score (out of 100) exit_exam: The student’s exit exam score (out of 100) tutoring: An indicator variable showing if the student was enrolled in the tutoring program   Load and clean data First, let’s download the dataset (if you haven’t already), put in a folder named data, and load it:\n  tutoring_program.csv  library(tidyverse) # ggplot(), %\u0026gt;%, mutate(), and friends library(broom) # Convert models to data frames library(rdrobust) # For robust nonparametric regression discontinuity library(rddensity) # For nonparametric regression discontinuity density tests library(modelsummary) # Create side-by-side regression tables # Load the data. # It\u0026#39;d be a good idea to click on the \u0026quot;tutoring\u0026quot; object in the Environment # panel in RStudio to see what the data looks like after you load it tutoring \u0026lt;- read_csv(\u0026quot;data/tutoring_program.csv\u0026quot;)  Step 1: Determine if process of assigning treatment is rule-based In order to join the tutoring program, students have to score 70 points or lower on the entrance exam. Students who score higher than 70 are not eligible for the program. Since we have a clear 70-point rule, we can assume that the process of participating in the tutoring program is rule-based.\n Step 2: Determine if the design is fuzzy or sharp Since we know that the program was applied based on a rule, we next want to figure out how strictly the rule was applied. The threshold was 70 points on the test—did people who scored 68 slip through bureaucratic cracks and not participate, or did people who scored 73 sneak into the program? The easiest way to check this is with a graph, and we can get exact numbers with a table.\nggplot(tutoring, aes(x = entrance_exam, y = tutoring, color = tutoring)) + # Make points small and semi-transparent since there are lots of them geom_point(size = 0.5, alpha = 0.5, position = position_jitter(width = 0, height = 0.25, seed = 1234)) + # Add vertical line geom_vline(xintercept = 70) + # Add labels labs(x = \u0026quot;Entrance exam score\u0026quot;, y = \u0026quot;Participated in tutoring program\u0026quot;) + # Turn off the color legend, since it\u0026#39;s redundant guides(color = FALSE) This looks pretty sharp—it doesn’t look like people who scored under 70 participated in the program. We can verify this with a table. There are no people where entrance_exam is greater than 70 and tutoring is false, and no people where entrance_exam is less than 70 and tutoring is true.\ntutoring %\u0026gt;% group_by(tutoring, entrance_exam \u0026lt;= 70) %\u0026gt;% summarize(count = n()) ## # A tibble: 2 x 3 ## # Groups: tutoring [2] ## tutoring `entrance_exam \u0026lt;= 70` count ## \u0026lt;lgl\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;int\u0026gt; ## 1 FALSE FALSE 759 ## 2 TRUE TRUE 241 This is thus a sharp design.\n Step 3: Check for discontinuity in running variable around cutpoint Next we need to see if there was any manipulation in the running variable—maybe lots of people bunched up around 70 because of how the test was graded (i.e. students wanted to get into the program, so they purposely did poorly on the exam to get under 70). We can do this a couple different ways. First, we’ll make a histogram of the running variable (test scores) and see if there are any big jumps around the threshold:\nggplot(tutoring, aes(x = entrance_exam, fill = tutoring)) + geom_histogram(binwidth = 2, color = \u0026quot;white\u0026quot;, boundary = 70) + geom_vline(xintercept = 70) + labs(x = \u0026quot;Entrance exam score\u0026quot;, y = \u0026quot;Count\u0026quot;, fill = \u0026quot;In program\u0026quot;) Here it doesn’t look like there’s a jump around the cutoff—there’s a tiny visible difference between the height of the bars right before and right after the 70-point threshold, but it seems to follow the general shape of the overall distribution. We can check to see if that jump is statistically significant with a McCrary density test (explained on pp. 185-86 of Causal Inference: The Mixtape). This puts data into bins like a histogram, and then plots the averages and confidence intervals of those bins. If the confidence intervals of the density lines don’t overlap, then there’s likely something systematically wrong with how the test was scored (i.e. too many people getting 69 vs 71). If the confidence intervals overlap, there’s not any significant difference around the threshold and we’re fine.\nFirst we use rddensity() to do the actual statistical test. You need to feed it two things: the running variable and the cutoff.\n# Notice this tutoring$entrance_exam syntax. This is one way for R to access # columns in data frames---it means \u0026quot;use the entrance_exam column in the # tutoring data frame\u0026quot;. The general syntax for it is data_frame$column_name test_density \u0026lt;- rddensity(tutoring$entrance_exam, c = 70) summary(test_density) ## Manipulation testing using local polynomial density estimation. ## ## Number of obs = 1000 ## Model = unrestricted ## Kernel = triangular ## BW method = estimated ## VCE method = jackknife ## ## c = 70 Left of c Right of c ## Number of obs 237 763 ## Eff. Number of obs 208 577 ## Order est. (p) 2 2 ## Order bias (q) 3 3 ## BW est. (h) 22.444 19.966 ## ## Method T P \u0026gt; |T| ## Robust -0.5521 0.5809 We can then plot that density test:\n# The syntax for rdplotdensity is kinda wonky here. You have to feed it the # rddensity() test, and then you have to specify x, which is your running # variable (again!). The type argument tells the plot to show both points and # lines---without it, it\u0026#39;ll only show lines. # # Finally, notice how I assigned the output of rdplotdensity to a variable named # plot_density_test. In theory, this should make it show nothing---all the # output should go to that object. Because of a bug in rdplotdensity, though, it # will show a plot automatically even if assigning it to a variable. If we don\u0026#39;t # assign it to a variable you\u0026#39;ll see two identical plots when knitting, which is # annoying. So we save it as a variable to hide the output, but get the output # for a single plot anyway. Ugh. plot_density_test \u0026lt;- rdplotdensity(rdd = test_density, X = tutoring$entrance_exam, type = \u0026quot;both\u0026quot;) # This adds both points and lines There’s a lot of output here, but what we care about is the line that starts with “Robust”, which shows the t-test for the difference in the two points on either side of the cutpoint in the plot. Notice in the plot that the confidence intervals overlap substantially. The p-value for the size of that overlap is 0.5809, which is a lot larger than 0.05, so we don’t have good evidence that there’s a significant difference between the two lines. Based on this plot and the t-statistic, we’re probably safe in saying that there’s no manipulation or bunching.\n Step 4: Check for discontinuity in outcome across running variable Now that we know this is a sharp design and that there’s no bunching of test scores around the 70-point threshold, we can finally see if there’s a discontinuity in final scores based on participation in the tutoring program. Plot the running variable on the x-axis, the outcome variable on the y-axis, and color the points by whether they participated in the program.\nggplot(tutoring, aes(x = entrance_exam, y = exit_exam, color = tutoring)) + geom_point(size = 0.5, alpha = 0.5) + # Add a line based on a linear model for the people scoring 70 or less geom_smooth(data = filter(tutoring, entrance_exam \u0026lt;= 70), method = \u0026quot;lm\u0026quot;) + # Add a line based on a linear model for the people scoring more than 70 geom_smooth(data = filter(tutoring, entrance_exam \u0026gt; 70), method = \u0026quot;lm\u0026quot;) + geom_vline(xintercept = 70) + labs(x = \u0026quot;Entrance exam score\u0026quot;, y = \u0026quot;Exit exam score\u0026quot;, color = \u0026quot;Used tutoring\u0026quot;) Based on this graph, there’s a clear discontinuity! It looks like participation in the tutoring program boosted final test scores.\n Step 5: Measure the size of the effect There’s a discontinuity, but how big is it? And is it statistically significant?\nWe can check the size two different ways: parametrically (i.e. using lm() with specific parameters and coefficients), and nonparametrically (i.e. not using lm() or any kind of straight line and instead drawing lines that fit the data more precisely). We’ll do it both ways.\nParametric estimation First we’ll do it parametrically by using linear regression. Here we want to explain the variation in final scores based on the entrance exam score and participation in the tutoring program:\n\\[ \\text{Exit exam} = \\beta_0 + \\beta_1 \\text{Entrance exam score}_\\text{centered} + \\beta_2 \\text{Tutoring program} + \\epsilon \\]\nTo make it easier to interpret coefficients, we can center the entrance exam column so that instead of showing the actual test score, it shows how many points above or below 70 the student scored. That way we can use the coefficient for tutoring for the causal effect.\ntutoring_centered \u0026lt;- tutoring %\u0026gt;% mutate(entrance_centered = entrance_exam - 70) model_simple \u0026lt;- lm(exit_exam ~ entrance_centered + tutoring, data = tutoring_centered) tidy(model_simple) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 59.4 0.442 134. 0. ## 2 entrance_centered 0.510 0.0269 18.9 1.40e-68 ## 3 tutoringTRUE 10.8 0.800 13.5 3.12e-38 Here’s what these coefficients mean:\n \\(\\beta_0\\): This is the intercept. Because we centered entrance exam scores, it shows the average exit exam score at the 70-point threshold. People who scored 70.001 points on the entrance exam score an average of 59.4 points on the exit exam. (Another way to think about this is that it shows the predicted exit exam score when entrance_centered is 0 (i.e. 70) and when tutoring is FALSE.) \\(\\beta_1\\): This is the coefficient for entrance_centered. For every point above 70 that people score on the entrance exam, they score 0.51 points higher on the exit exam. We don’t really care that much about this number. \\(\\beta_2\\): This is the coefficient for the tutoring program, and this is the one we care about the most. This is the shift in intercept when tutoring is true, or the difference between scores at the threshold. Participating in the tutoring program increases exit exam scores by 10.8 points.  One advantage to using a parametric approach is that you can include other covariates like demographics. You can also use polynomial regression and include terms like entrance_centered² or entrance_centered³ or even entrance_centered⁴ to make the line fit the data as close as possible.\nHere we fit the model to the entire data, but in real life, we care most about the observations right around the threshold, which means this model is actually wrong. Scores that are super high or super low shouldn’t really influence our effect size, since we only care about the people who score just barely under and just barely over 70.\nWe can fit the same model but restrict it to people within a smaller window, or bandwidth, like ±10 points, or ±5 points:\nmodel_bw_10 \u0026lt;- lm(exit_exam ~ entrance_centered + tutoring, data = filter(tutoring_centered, entrance_centered \u0026gt;= -10 \u0026amp; entrance_centered \u0026lt;= 10)) tidy(model_bw_10) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 60.4 0.752 80.3 2.99e-249 ## 2 entrance_centered 0.388 0.114 3.40 7.45e- 4 ## 3 tutoringTRUE 9.27 1.31 7.09 6.27e- 12 model_bw_5 \u0026lt;- lm(exit_exam ~ entrance_centered + tutoring, data = filter(tutoring_centered, entrance_centered \u0026gt;= -5 \u0026amp; entrance_centered \u0026lt;= 5)) tidy(model_bw_5) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 60.6 1.12 54.3 4.78e-118 ## 2 entrance_centered 0.380 0.331 1.15 2.53e- 1 ## 3 tutoringTRUE 9.12 1.91 4.77 3.66e- 6 We can compare all these models simultaneously with modelsummary():\nmodelsummary(list(\u0026quot;Full data\u0026quot; = model_simple, \u0026quot;Bandwidth = 10\u0026quot; = model_bw_10, \u0026quot;Bandwidth = 5\u0026quot; = model_bw_5))    Full data  Bandwidth = 10  Bandwidth = 5      (Intercept)  59.411***  60.377***  60.631***     (0.442)  (0.752)  (1.117)    entrance_centered  0.510***  0.388***  0.380     (0.027)  (0.114)  (0.331)    tutoringTRUE  10.800***  9.273***  9.122***     (0.800)  (1.309)  (1.912)    Num.Obs.  1000  404  194    R2  0.268  0.162  0.222    R2 Adj.  0.267  0.158  0.214       * p \u0026lt; 0.1, ** p \u0026lt; 0.05, *** p \u0026lt; 0.01     The effect of tutoring differs a lot across these different models, from 9.1 to 10.8. Which one is right? I don’t know. Definitely not the full data one.\nAlso notice how much the sample size (N) changes across the models. As you narrow the bandwidth, you look at fewer and fewer observations.\nggplot(tutoring, aes(x = entrance_exam, y = exit_exam, color = tutoring)) + geom_point(size = 0.5, alpha = 0.2) + # Add lines for the full model (model_simple) geom_smooth(data = filter(tutoring, entrance_exam \u0026lt;= 70), method = \u0026quot;lm\u0026quot;, se = FALSE, linetype = \u0026quot;dotted\u0026quot;, size = 1) + geom_smooth(data = filter(tutoring, entrance_exam \u0026gt; 70), method = \u0026quot;lm\u0026quot;, se = FALSE, linetype = \u0026quot;dotted\u0026quot;, size = 1) + # Add lines for bandwidth = 10 geom_smooth(data = filter(tutoring, entrance_exam \u0026lt;= 70, entrance_exam \u0026gt;= 60), method = \u0026quot;lm\u0026quot;, se = FALSE, linetype = \u0026quot;dashed\u0026quot;, size = 1) + geom_smooth(data = filter(tutoring, entrance_exam \u0026gt; 70, entrance_exam \u0026lt;= 80), method = \u0026quot;lm\u0026quot;, se = FALSE, linetype = \u0026quot;dashed\u0026quot;, size = 1) + # Add lines for bandwidth = 5 geom_smooth(data = filter(tutoring, entrance_exam \u0026lt;= 70, entrance_exam \u0026gt;= 65), method = \u0026quot;lm\u0026quot;, se = FALSE, size = 2) + geom_smooth(data = filter(tutoring, entrance_exam \u0026gt; 70, entrance_exam \u0026lt;= 75), method = \u0026quot;lm\u0026quot;, se = FALSE, size = 2) + geom_vline(xintercept = 70) + # Zoom in coord_cartesian(xlim = c(50, 90), ylim = c(55, 75)) + labs(x = \u0026quot;Entrance exam score\u0026quot;, y = \u0026quot;Exit exam score\u0026quot;, color = \u0026quot;Used tutoring\u0026quot;)  Nonparametric estimation Instead of using linear regression to measure the size of the discontinuity, we can use nonparametric methods. Essentially this means that R will not try to fit a straight line to the data—instead it’ll curve around the points and try to fit everything as smoothly as possible.\nThe rdrobust() function makes it really easy to measure the gap at the cutoff with nonparametric estimation. Here’s the simplest version:\n# Notice how we have to use the tutoring$exit_exam syntax here. Also make sure # you set the cutoff with c rdrobust(y = tutoring$exit_exam, x = tutoring$entrance_exam, c = 70) %\u0026gt;% summary() ## [1] \u0026quot;Mass points detected in the running variable.\u0026quot; ## Call: rdrobust ## ## Number of Obs. 1000 ## BW type mserd ## Kernel Triangular ## VCE method NN ## ## Number of Obs. 237 763 ## Eff. Number of Obs. 144 256 ## Order est. (p) 1 1 ## Order bias (q) 2 2 ## BW est. (h) 9.969 9.969 ## BW bias (b) 14.661 14.661 ## rho (h/b) 0.680 0.680 ## Unique Obs. 155 262 ## ## ============================================================================= ## Method Coef. Std. Err. z P\u0026gt;|z| [ 95% C.I. ] ## ============================================================================= ## Conventional -8.578 1.601 -5.359 0.000 [-11.715 , -5.441] ## Robust - - -4.352 0.000 [-12.101 , -4.587] ## ============================================================================= There are a few important pieces of information to look at in this output:\n The thing you care about the most is the actual effect size. This is the coefficient in the table at the bottom, indicated with the “Conventional” method. Here it’s -8.578, which means the tutoring program causes an 8-point change in exit exam scores. The table at the bottom also includes standard errors, p-values, and confidence intervals for the coefficient, both normal estimates (conventional) and robust estimates (robust). According to both types of estimates, this 8 point bump is statistically significant (p \u0026lt; 0.001; the 95% confidence interval definitely doesn’t ever include 0). Importantly, notice that the coefficient here is actually negative (-8.578), while our previous parametric estimates were all positive. That does not mean that the program causes a drop in test scores. That negative value is just a side effect of how rdrobust() measures the gap. It looks at the value of the treatment group right before the threshold and then shows that test scores drop when shifting to the control group (while before we talked about the opposite story: test scores increase as you move from the control group to the treatment group). There’s no way to flip the sign within rdrobust(), so you need to look at the plot and see what the gap is really doing. The model used a bandwidth of 9.969 (BW est. (h) in the output), which means it only looked at people with test scores of 70 ± 10. It decided on this bandwidth automatically, but you can change it to whatever you want. The model used a triangular kernel. This is the most esoteric part of the model—the kernel decides how much weight to give to observations around the cutoff. Test scores like 69.99 or 70.01 are extremely close to 70, so they get the most weight. Scores like 67 or 73 are a little further away so they matter less. Scores like 64 or 76 matter even less, so they get even less weight. You can use different kernels too, and Wikipedia has a nice graphic showing the shapes of these different kernels and how they give different weights to observations at different distances.  We can plot this nonparametric model with rdplot().\nrdplot(y = tutoring$exit_exam, x = tutoring$entrance_exam, c = 70) Look at that 8.5 point jump at 70! Neat!\nNotice that the points here aren’t actually the observations in the dataset. The rdplot() function makes bins of points (like a histogram) and then shows the average outcome within each bin. You can control how many bins are used in the x-axis with the nbins or binselect arguments in rdplot() (run ?rdplot in your console to see all the possible options for plotting nonparametric discontinuities).\nSide note: rdplot() uses the same plotting system as ggplot(), so you can add layers like labs() or geom_point() or anything else, which is cool, but getting to that underlying ggplot() object is weird. To do it, store the results of rdplot as an object, like asdf, then use asdf$rdplot to access the plot:\nasdf \u0026lt;- rdplot(...) asdf$rdplot + labs(x = \u0026quot;Running variable\u0026quot;, y = \u0026quot;Outcome variable\u0026quot;) You don’t need to worry about all that for showing simple plots though!\n By default, rdrobust() chooses the bandwidth size automatically based on fancy algorithms that economists have developed. You can use rdbwselect() to see what that bandwidth is, and if you include the all = TRUE argument, you can see a bunch of different potential bandwidths based on a bunch of different algorithms:\n# This says to use the mserd version, which according to the help file for # rdbwselect means \u0026quot;the mean squared error-optimal bandwidth selector for RD # treatment effects\u0026quot;. Sounds great. rdbwselect(y = tutoring$exit_exam, x = tutoring$entrance_exam, c = 70) %\u0026gt;% summary() ## [1] \u0026quot;Mass points detected in the running variable.\u0026quot; ## Call: rdbwselect ## ## Number of Obs. 1000 ## BW type mserd ## Kernel Triangular ## VCE method NN ## ## Number of Obs. 237 763 ## Order est. (p) 1 1 ## Order bias (q) 2 2 ## Unique Obs. 155 262 ## ## ======================================================= ## BW est. (h) BW bias (b) ## Left of c Right of c Left of c Right of c ## ======================================================= ## mserd 9.969 9.969 14.661 14.661 ## ======================================================= What are the different possible bandwidths we could use?\nrdbwselect(y = tutoring$exit_exam, x = tutoring$entrance_exam, c = 70, all = TRUE) %\u0026gt;% summary() ## [1] \u0026quot;Mass points detected in the running variable.\u0026quot; ## Call: rdbwselect ## ## Number of Obs. 1000 ## BW type All ## Kernel Triangular ## VCE method NN ## ## Number of Obs. 237 763 ## Order est. (p) 1 1 ## Order bias (q) 2 2 ## Unique Obs. 155 262 ## ## ======================================================= ## BW est. (h) BW bias (b) ## Left of c Right of c Left of c Right of c ## ======================================================= ## mserd 9.969 9.969 14.661 14.661 ## msetwo 11.521 10.054 17.067 14.907 ## msesum 12.044 12.044 17.631 17.631 ## msecomb1 9.969 9.969 14.661 14.661 ## msecomb2 11.521 10.054 17.067 14.907 ## cerrd 7.058 7.058 14.661 14.661 ## certwo 8.156 7.118 17.067 14.907 ## cersum 8.526 8.526 17.631 17.631 ## cercomb1 7.058 7.058 14.661 14.661 ## cercomb2 8.156 7.118 17.067 14.907 ## ======================================================= Phew. There are a lot here. The best one was mserd, or ±9.69. Some say ±7, others ±12, other are asymmetric and say -11.5 before and +10 after. Try a bunch of different bandwidths as part of your sensitivity analysis and see if your effect size changes substantially as a result.\nAnother common approach to sensitivity analysis is to use the ideal bandwidth, twice the ideal, and half the ideal (so in our case 10, 20, and 5) and see if the estimate changes substantially. Use the h argument to specify your own bandwidth\nrdrobust(y = tutoring$exit_exam, x = tutoring$entrance_exam, c = 70, h = 9.969) %\u0026gt;% summary() ## [1] \u0026quot;Mass points detected in the running variable.\u0026quot; ## Call: rdrobust ## ## Number of Obs. 1000 ## BW type Manual ## Kernel Triangular ## VCE method NN ## ## Number of Obs. 237 763 ## Eff. Number of Obs. 144 256 ## Order est. (p) 1 1 ## Order bias (q) 2 2 ## BW est. (h) 9.969 9.969 ## BW bias (b) 9.969 9.969 ## rho (h/b) 1.000 1.000 ## Unique Obs. 155 262 ## ## ============================================================================= ## Method Coef. Std. Err. z P\u0026gt;|z| [ 95% C.I. ] ## ============================================================================= ## Conventional -8.578 1.601 -5.359 0.000 [-11.715 , -5.441] ## Robust - - -3.276 0.001 [-12.483 , -3.138] ## ============================================================================= rdrobust(y = tutoring$exit_exam, x = tutoring$entrance_exam, c = 70, h = 9.969 * 2) %\u0026gt;% summary() ## [1] \u0026quot;Mass points detected in the running variable.\u0026quot; ## Call: rdrobust ## ## Number of Obs. 1000 ## BW type Manual ## Kernel Triangular ## VCE method NN ## ## Number of Obs. 237 763 ## Eff. Number of Obs. 206 577 ## Order est. (p) 1 1 ## Order bias (q) 2 2 ## BW est. (h) 19.938 19.938 ## BW bias (b) 19.938 19.938 ## rho (h/b) 1.000 1.000 ## Unique Obs. 155 262 ## ## ============================================================================= ## Method Coef. Std. Err. z P\u0026gt;|z| [ 95% C.I. ] ## ============================================================================= ## Conventional -9.151 1.130 -8.100 0.000 [-11.365 , -6.937] ## Robust - - -4.980 0.000 [-11.670 , -5.078] ## ============================================================================= rdrobust(y = tutoring$exit_exam, x = tutoring$entrance_exam, c = 70, h = 9.969 / 2) %\u0026gt;% summary() ## [1] \u0026quot;Mass points detected in the running variable.\u0026quot; ## Call: rdrobust ## ## Number of Obs. 1000 ## BW type Manual ## Kernel Triangular ## VCE method NN ## ## Number of Obs. 237 763 ## Eff. Number of Obs. 82 109 ## Order est. (p) 1 1 ## Order bias (q) 2 2 ## BW est. (h) 4.984 4.984 ## BW bias (b) 4.984 4.984 ## rho (h/b) 1.000 1.000 ## Unique Obs. 155 262 ## ## ============================================================================= ## Method Coef. Std. Err. z P\u0026gt;|z| [ 95% C.I. ] ## ============================================================================= ## Conventional -8.201 2.348 -3.493 0.000 [-12.803 , -3.600] ## Robust - - -2.032 0.042 [-13.618 , -0.246] ## ============================================================================= Here the coefficients change slightly:\n  Bandwidth Effect size    9.969 (ideal) 8.578  19.938 (twice) 9.151  4.984 (half) 8.201    You can also adjust the kernel. By default rd_robust uses a triangular kernel (more distant observations have less weight linearly), but you can switch it to Epanechnikov (more distant observations have less weight following a curve) or uniform (more distant observations have the same weight as closer observations; this is unweighted).\n# Non-parametric RD with different kernels rdrobust(y = tutoring$exit_exam, x = tutoring$entrance_exam, c = 70, kernel = \u0026quot;triangular\u0026quot;) %\u0026gt;% summary() # Default ## [1] \u0026quot;Mass points detected in the running variable.\u0026quot; ## Call: rdrobust ## ## Number of Obs. 1000 ## BW type mserd ## Kernel Triangular ## VCE method NN ## ## Number of Obs. 237 763 ## Eff. Number of Obs. 144 256 ## Order est. (p) 1 1 ## Order bias (q) 2 2 ## BW est. (h) 9.969 9.969 ## BW bias (b) 14.661 14.661 ## rho (h/b) 0.680 0.680 ## Unique Obs. 155 262 ## ## ============================================================================= ## Method Coef. Std. Err. z P\u0026gt;|z| [ 95% C.I. ] ## ============================================================================= ## Conventional -8.578 1.601 -5.359 0.000 [-11.715 , -5.441] ## Robust - - -4.352 0.000 [-12.101 , -4.587] ## ============================================================================= rdrobust(y = tutoring$exit_exam, x = tutoring$entrance_exam, c = 70, kernel = \u0026quot;epanechnikov\u0026quot;) %\u0026gt;% summary() ## [1] \u0026quot;Mass points detected in the running variable.\u0026quot; ## Call: rdrobust ## ## Number of Obs. 1000 ## BW type mserd ## Kernel Epanechnikov ## VCE method NN ## ## Number of Obs. 237 763 ## Eff. Number of Obs. 130 204 ## Order est. (p) 1 1 ## Order bias (q) 2 2 ## BW est. (h) 8.201 8.201 ## BW bias (b) 12.807 12.807 ## rho (h/b) 0.640 0.640 ## Unique Obs. 155 262 ## ## ============================================================================= ## Method Coef. Std. Err. z P\u0026gt;|z| [ 95% C.I. ] ## ============================================================================= ## Conventional -8.389 1.677 -5.001 0.000 [-11.677 , -5.101] ## Robust - - -4.085 0.000 [-12.116 , -4.260] ## ============================================================================= rdrobust(y = tutoring$exit_exam, x = tutoring$entrance_exam, c = 70, kernel = \u0026quot;uniform\u0026quot;) %\u0026gt;% summary() ## [1] \u0026quot;Mass points detected in the running variable.\u0026quot; ## Call: rdrobust ## ## Number of Obs. 1000 ## BW type mserd ## Kernel Uniform ## VCE method NN ## ## Number of Obs. 237 763 ## Eff. Number of Obs. 119 176 ## Order est. (p) 1 1 ## Order bias (q) 2 2 ## BW est. (h) 7.346 7.346 ## BW bias (b) 12.561 12.561 ## rho (h/b) 0.585 0.585 ## Unique Obs. 155 262 ## ## ============================================================================= ## Method Coef. Std. Err. z P\u0026gt;|z| [ 95% C.I. ] ## ============================================================================= ## Conventional -8.175 1.651 -4.952 0.000 [-11.411 , -4.939] ## Robust - - -4.085 0.000 [-11.704 , -4.115] ## ============================================================================= Again, the coefficients change slightly:\n  Kernel Effect size    Triangular 8.578  Epanechnikov 8.389  Uniform 8.175    Which one is best? ¯\\_(ツ)_/¯. All that really matters is that the size and direction of the effect doesn’t change. It’s still positive and it’s still in the 8–9ish point range.\n  Step 6: Compare all the effects We just estimated a ton of effect sizes. In real life you’d generally just report one of these as your final effect, but you’d run the different parametric and nonparametric models to check how reliable and robust your findings are. Here’s everything we just found:\n  Method Bandwidth Kernel Estimate    Parametric Full data Unweighted 10.8  Parametric 10 Unweighted 9.273  Parametric 5 Unweighted 9.122  Nonparametric 9.969 Triangular 8.578  Nonparametric 19.938 Triangular 9.151  Nonparametric 4.984 Triangular 8.201  Nonparametric 8.201 Epanechnikov 8.389  Nonparametric 7.346 Uniform (unweighted) 8.175    In real life, I’d likely report the simplest one (row 4: nonparametric, automatic bandwidth, triangular kernel), but knowing how much the effect varies across model specifications is helpful.\n ","date":1603152000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"550b0237dcf0243fe0a031660ba56c81","permalink":"https://evalf20.classes.andrewheiss.com/example/rdd/","publishdate":"2020-10-20T00:00:00Z","relpermalink":"/example/rdd/","section":"example","summary":"Video walk-through Program background Load and clean data Step 1: Determine if process of assigning treatment is rule-based Step 2: Determine if the design is fuzzy or sharp Step 3: Check for discontinuity in running variable around cutpoint Step 4: Check for discontinuity in outcome across running variable Step 5: Measure the size of the effect  Parametric estimation Nonparametric estimation  Step 6: Compare all the effects   Video walk-through If you want to follow along with this example, you can download the data below:","tags":null,"title":"Regression discontinuity","type":"docs"},{"authors":null,"categories":null,"content":"   Readings   Readings This session is a continuation of session 8.\n ","date":1603065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"35bbbb8e03581e65c2b2e10271df2fce","permalink":"https://evalf20.classes.andrewheiss.com/content/09-content/","publishdate":"2020-10-19T00:00:00Z","relpermalink":"/content/09-content/","section":"content","summary":"   Readings   Readings This session is a continuation of session 8.\n ","tags":null,"title":"Difference-in-differences II","type":"docs"},{"authors":null,"categories":null,"content":"  IMPORTANT: This looks like a lot of work, but again, it’s mostly copying/pasting chunks of code and changing things.\n For this problem set, you’ll practice running difference-in-differences analysis with R, both manually and with regression. This example will be incredibly useful for you:\n Difference-in-differences  You’ll be doing all your R work in R Markdown. You can download a zipped file of a pre-made project here:\n  problem-set-4.zip  And as always, if you’re struggling, please talk to me. Work with classmates too (especially for this assignment!). Don’t suffer in silence!\nInstructions If you’re using R on your own computer, download this file, unzip it, and double click on the file named problem-set-4.Rproj:  problem-set-4.zip\nYou’ll need to make sure you have these packages installed on your computer: tidyverse, haven, and broom. If you try to load one of those packages with library(tidyverse) or library(haven), etc., and R gives an error that the package is missing, use the “Packages” panel in RStudio to install it.\n(Alternatively, you can open the project named “Problem Set 4” on RStudio.cloud and complete the assignment in your browser without needing to install anything. If you don’t have access to the class RStudio.cloud account, please let me know as soon as possible. This link should take you to the project—if it doesn’t, log in and look for the project named “Problem Set 4.”)\n Rename the R Markdown file named your-name_problem-set-4.Rmd to something that matches your name and open it in RStudio.\n Complete the tasks given in the R Markdown file. There are questions marked in bold (e.g. **What is the ATE?**). Your job is to answer those questions. You don’t need to put your answers in bold, and you can remove the question text if you want.\nFill out code in the empty chunks provided (you can definitely copy, paste, and adapt from other code in the document or the example page on diff-in-diff—don’t try to write everything from scratch!).\nYou’ll need to insert your own code chunks. Rather than typing them by hand (that’s tedious!), use the “Insert” button at the top of the editing window, or press ⌥ + ⌘ + I on macOS, or ctrl + alt + I on Windows.\nRemember that you can run an entire chunk by clicking on the green play arrow in the top right corner of the chunk. You can also run lines of code line-by-line if you place your cursor on some R code and press ⌘ + enter (for macOS users) or ctrl + enter (for Windows users).\nMake sure you run each chunk sequentially. If you run a chunk in the middle of the document without running previous ones, it might not work, since previous chunks might do things that later chunks depend on.\n When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex) of your document. Upload that file to iCollege.\n   ","date":1603065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"15e3795b90fc4eecfa15c60a4023dff5","permalink":"https://evalf20.classes.andrewheiss.com/assignment/04-problem-set/","publishdate":"2020-10-19T00:00:00Z","relpermalink":"/assignment/04-problem-set/","section":"assignment","summary":"IMPORTANT: This looks like a lot of work, but again, it’s mostly copying/pasting chunks of code and changing things.\n For this problem set, you’ll practice running difference-in-differences analysis with R, both manually and with regression. This example will be incredibly useful for you:\n Difference-in-differences  You’ll be doing all your R work in R Markdown. You can download a zipped file of a pre-made project here:","tags":null,"title":"Problem set 4","type":"docs"},{"authors":null,"categories":null,"content":"   Readings Slides Videos   Readings   Chapter 7 in Impact Evaluation in Practice Paul J. Gertler et al., Impact Evaluation in Practice, 2nd ed. (Inter-American Development Bank; World Bank, 2016), https://openknowledge.worldbank.org/handle/10986/25030.  Chapter 5 in Mastering ’Metrics Joshua D. Angrist and Jörn-Steffen Pischke, Mastering ’Metrics: The Path from Cause to Effect (Princeton, NJ: Princeton University Press, 2015).  “Differences-in-differences” in Causal Inference: The Mixtape Scott Cunningham, Causal Inference: The Mixtape, 2018, https://www.scunning.com/mixtape.html.   Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nIntroduction  Quasi-experiments  Interactions \u0026amp; regression  Two wrongs make a right  Diff-in-diff assumptions                   Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Quasi-experiments Interactions \u0026amp; regression Two wrongs make a right Diff-in-diff assumptions  You can also watch the playlist (and skip around to different sections) here:\n   ","date":1602460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"d3f7fc26f58a06eab9a68e8656b03674","permalink":"https://evalf20.classes.andrewheiss.com/content/08-content/","publishdate":"2020-10-12T00:00:00Z","relpermalink":"/content/08-content/","section":"content","summary":"Readings Slides Videos   Readings   Chapter 7 in Impact Evaluation in Practice Paul J. Gertler et al., Impact Evaluation in Practice, 2nd ed. (Inter-American Development Bank; World Bank, 2016), https://openknowledge.worldbank.org/handle/10986/25030.  Chapter 5 in Mastering ’Metrics Joshua D. Angrist and Jörn-Steffen Pischke, Mastering ’Metrics: The Path from Cause to Effect (Princeton, NJ: Princeton University Press, 2015).  “Differences-in-differences” in Causal Inference: The Mixtape Scott Cunningham, Causal Inference: The Mixtape, 2018, https://www.","tags":null,"title":"Difference-in-differences I + II","type":"docs"},{"authors":null,"categories":null,"content":"  IMPORTANT: This looks like a lot of work, but again, it’s mostly copying/pasting chunks of code and changing things.\n For this problem set, you’ll practice analyzing RCTs and working with matching and inverse probability weighting. These two examples will be incredibly useful for you:\n RCTs Matching and IPW  You’ll be doing all your R work in R Markdown this time (and from now on). You can download a zipped file of a pre-made project here:\n  problem-set-3.zip  And as always, if you’re struggling, please talk to me. Work with classmates too (especially for this assignment!). Don’t suffer in silence!\nInstructions If you’re using R on your own computer, download this file, unzip it, and double click on the file named problem-set-3.Rproj:  problem-set-3.zip\nYou’ll need to make sure you have these packages installed on your computer: tidyverse, MatchIt, modelsummary, and patchwork. If you try to load one of those packages with library(tidyverse) or library(MatchIt), etc., and R gives an error that the package is missing, use the “Packages” panel in RStudio to install it.\n(Alternatively, you can open the project named “Problem Set 3” on RStudio.cloud and complete the assignment in your browser without needing to install anything. If you don’t have access to the class RStudio.cloud account, please let me know as soon as possible. This link should take you to the project—if it doesn’t, log in and look for the project named “Problem Set 3.”)\n Rename the R Markdown file named your-name_problem-set-3.Rmd to something that matches your name and open it in RStudio.\n Complete the tasks given in the R Markdown file. There are questions marked in bold (e.g. **What is the ATE?**). Your job is to answer those questions. You don’t need to put your answers in bold, and you can remove the question text if you want.\nFill out code in the empty chunks provided (you can definitely copy, paste, and adapt from other code in the document or the example page on RCTs and the example page on matching and IPW—don’t try to write everything from scratch!).\nYou’ll need to insert your own code chunks. Rather than typing them by hand (that’s tedious!), use the “Insert” button at the top of the editing window, or press ⌥ + ⌘ + I on macOS, or ctrl + alt + I on Windows.\nRemember that you can run an entire chunk by clicking on the green play arrow in the top right corner of the chunk. You can also run lines of code line-by-line if you place your cursor on some R code and press ⌘ + enter (for macOS users) or ctrl + enter (for Windows users).\nMake sure you run each chunk sequentially. If you run a chunk in the middle of the document without running previous ones, it might not work, since previous chunks might do things that later chunks depend on.\n When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex) of your document. Upload that file to iCollege.\n   ","date":1602460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"805f2715fc046617ed150d3c8fb64e34","permalink":"https://evalf20.classes.andrewheiss.com/assignment/03-problem-set/","publishdate":"2020-10-12T00:00:00Z","relpermalink":"/assignment/03-problem-set/","section":"assignment","summary":"IMPORTANT: This looks like a lot of work, but again, it’s mostly copying/pasting chunks of code and changing things.\n For this problem set, you’ll practice analyzing RCTs and working with matching and inverse probability weighting. These two examples will be incredibly useful for you:\n RCTs Matching and IPW  You’ll be doing all your R work in R Markdown this time (and from now on). You can download a zipped file of a pre-made project here:","tags":null,"title":"Problem set 3","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  RCTs, matching, and inverse probability weighting  Slides Videos   Readings   Andrew Heiss, “Causal Inference,” Chapter 10 in R for Political Data Science: A Practical Guide (forthcoming) (Ignore the exercises!). Get the PDF here.  Chapter 4 in Impact Evaluation in Practice Paul J. Gertler et al., Impact Evaluation in Practice, 2nd ed. (Inter-American Development Bank; World Bank, 2016), https://openknowledge.worldbank.org/handle/10986/25030.  Chapter 1 in Mastering ’Metrics Joshua D. Angrist and Jörn-Steffen Pischke, Mastering ’Metrics: The Path from Cause to Effect (Princeton, NJ: Princeton University Press, 2015).  Planet Money, “Moving To Opportunity?,” episode 937  Aaron Carroll, “Workplace Wellness Programs Don’t Work Well. Why Some Studies Show Otherwise,” The Upshot, August 6, 2018  RCTs, matching, and inverse probability weighting  The example page on RCTs shows how to use R to analyze and estimate causal effects from RCTs The example page on matching and inverse probability weighting shows how to use R to close backdoors, make adjustments, and find causal effects from observational data using matching and inverse probability weighting    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nIntroduction  The magic of randomization  How to analyze RCTs  The “gold” standard  Adjustment with matching                   Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction The magic of randomization How to analyze RCTs The “gold” standard Adjustment with matching  You can also watch the playlist (and skip around to different sections) here:\n   ","date":1601856000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"79f283e309555def72f172af8fa16106","permalink":"https://evalf20.classes.andrewheiss.com/content/07-content/","publishdate":"2020-10-05T00:00:00Z","relpermalink":"/content/07-content/","section":"content","summary":"Readings  RCTs, matching, and inverse probability weighting  Slides Videos   Readings   Andrew Heiss, “Causal Inference,” Chapter 10 in R for Political Data Science: A Practical Guide (forthcoming) (Ignore the exercises!). Get the PDF here.  Chapter 4 in Impact Evaluation in Practice Paul J. Gertler et al., Impact Evaluation in Practice, 2nd ed. (Inter-American Development Bank; World Bank, 2016), https://openknowledge.worldbank.org/handle/10986/25030.  Chapter 1 in Mastering ’Metrics Joshua D.","tags":null,"title":"Randomization and matching","type":"docs"},{"authors":null,"categories":null,"content":"   Instructions Internal validity scores Assessing validity   Instructions You need to complete the “Assessing validity” section below. Ideally you should type this in R Markdown and knit your document to HTML or Word or PDF, but you can also write in Word if you want (though your final project will need to be in R Markdown, and this would give you practice).\nI’ve created an R Markdown template you can use here:  threats-validity.zip. It’s also available on RStudio.cloud.\nSubmit this assignment as a PDF or Word file on iCollege.\n Internal validity scores One helpful way to assess an evaluation’s internal validity is to systematically go through each possible threat and evaluate if the research design addresses it. For each of the 13 types of internal validity that we discussed in class, assign 1 point if the study addresses it and 0 points if it fails to do so. Add up the total points and assign the study a final internal validity score.\nNot all types of validity will apply to every study. Testing, for example, is only an issue if there is a pre-test and it involves a skill that could feasibly be enhanced by practicing the test. If a threat doesn’t apply, don’t give it a score.\n Omitted variable bias Selection Attrition  Trends Maturation Secular trends Seasonality Testing Regression to the mean  Study calibration Measurement error Time frame of study  Contamination Hawthorne effects John Henry effects Spillovers Intervening events    Assessing validity Your textbook Impact Evaluation in Practice is full of short examples of real-world evaluations, experiments, and studies. For this assignment, you will assess the (1) internal validity, (2) external validity, and (3) construct validity for four of the examples from the book. If the summary in the book isn’t sufficient, you can skim through the original study for more details.\nYou will assess these four studies:\nThe effect of conditional cash transfers on education in Mexico (Box 4.2, p. 70)  Original study: T. Paul Schultz, “School Subsidies for the Poor: Evaluating the Mexican Progresa Poverty Program,” Journal of Development Economics 74, no. 1 (June 2004): 199–250, doi:10.1016/j.jdeveco.2003.12.009.  The impact of Sesame Street on school readiness (Box 5.1, p. 91)  Original study: Melissa S. Kearney and Phillip B. Levine, Early Childhood Education by MOOC: Lessons from Sesame Street, Working Paper Series (National Bureau of Economic Research, June 2015), doi:10.3386/w21229.  The effects of police deployment on crime in Argentina (Box 7.2, p. 135)  Original study: Rafael Di Tella and Ernesto Schargrodsky, “Do Police Reduce Crime? Estimates Using the Allocation of Police Forces After a Terrorist Attack,” American Economic Review 94, no. 1 (March 2004): 115–133, doi:10.1257/000282804322970733.  Early childhood development and migration in Jamaica (Box 9.5, p. 170)  Original study: Paul Gertler et al., “Labor Market Returns to an Early Childhood Stimulation Intervention in Jamaica,” Science 344, no. 6187 (May 14, 2014): 998–1001, doi:10.1126/science.1251178.   For each study, do the following:\nGo through the 13 types of internal validity and describe in 2–3 sentences how the study succeeds/fails to address each concern. Calculate a total internal validity score. Describe any threats to external validity and assess how generalizable the findings are. (≈75 words) Describe any threats to construct validity and assess if the researchers are measuring the thing they intended to measure. (≈75 words)   ","date":1601856000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"d6e7bf6cf8ab1090e150f1563304a262","permalink":"https://evalf20.classes.andrewheiss.com/assignment/04-eval-threats/","publishdate":"2020-10-05T00:00:00Z","relpermalink":"/assignment/04-eval-threats/","section":"assignment","summary":"Instructions Internal validity scores Assessing validity   Instructions You need to complete the “Assessing validity” section below. Ideally you should type this in R Markdown and knit your document to HTML or Word or PDF, but you can also write in Word if you want (though your final project will need to be in R Markdown, and this would give you practice).\nI’ve created an R Markdown template you can use here:  threats-validity.","tags":null,"title":"Threats to validity","type":"docs"},{"authors":null,"categories":null,"content":"    Video walk-through Program background Our goal Load data and libraries DAG and adjustment sets Naive correlation-isn’t-causation estimate Matching  Step 1: Preprocess Step 2: Estimation  Inverse probability weighting  Oversimplified crash course in logistic regression Step 1: Generate propensity scores Step 2: Estimation  Results from all the models   Video walk-through If you want to follow along with this example, you can download the data below:\n  mosquito_nets.csv  There’s a set of videos that walks through each section below. To make it easier for you to jump around the video examples, I cut the long video into smaller pieces and included them all in one YouTube playlist.\n Drawing a DAG Creating an RStudio project Naive (and wrong!) estimate Matching Inverse probability weighting  You can also watch the playlist (and skip around to different sections) here:\n   Program background Let’s revisit the mosquito net program we used in the DAG example. Here, researchers are interested in whether using mosquito nets decreases an individual’s risk of contracting malaria. They have collected data from 1,752 households in an unnamed country and have variables related to environmental factors, individual health, and household characteristics. The data is not experimental—researchers have no control over who uses mosquito nets, and individual households make their own choices over whether to apply for free nets or buy their own nets, as well as whether they use the nets if they have them.\nThe CSV file contains the following columns:\n Malaria risk (malaria_risk): The likelihood that someone in the household will be infected with malaria. Measured on a scale of 0–100, with higher values indicating higher risk. Mosquito net (net and net_num): A binary variable indicating if the household used mosquito nets. Eligible for program (eligible): A binary variable indicating if the household is eligible for the free net program. Income (income): The household’s monthly income, in US dollars. Nighttime temperatures (temperature): The average temperature at night, in Celsius. Health (health): Self-reported healthiness in the household. Measured on a scale of 0–100, with higher values indicating better health. Number in household (household): Number of people living in the household. Insecticide resistance (resistance): Some strains of mosquitoes are more resistant to insecticide and thus pose a higher risk of infecting people with malaria. This is measured on a scale of 0–100, with higher values indicating higher resistance.   Our goal Our goal in this example is to estimate the causal effect of bed net usage on malaria risk using only observational data. This was not an RCT, so it might seem a little sketchy to make claims of causality. But if we can draw a correct DAG and adjust for the correct nodes, we can isolate the net → malaria relationship and talk about causality.\nBecause this is simulated data, we know the true causal effect of the net program because I built it into the data. The true average treatment effect (ATE) is -10. Using a mosquito net causes the risk of malaria to decrease by 10 points, on average.\nLet’s see if we can find that 10 point effect!\n Load data and libraries First, let’s download the dataset (if you haven’t already), put in a folder named data, and load it:\n  mosquito_nets.csv  library(tidyverse) # ggplot(), %\u0026gt;%, mutate(), and friends library(ggdag) # Make DAGs library(dagitty) # Do DAG logic with R library(broom) # Convert models to data frames library(MatchIt) # Match things library(modelsummary) # Make side-by-side regression tables set.seed(1234) # Make all random draws reproducible # Load the data. # It\u0026#39;d be a good idea to click on the \u0026quot;nets\u0026quot; object in the Environment panel in # RStudio to see what the data looks like after you load it nets \u0026lt;- read_csv(\u0026quot;data/mosquito_nets.csv\u0026quot;)  DAG and adjustment sets Before running any models, we need to find what we need to adjust for. Recall from the DAG example that we built this causal model to show the data generating process behind the relationship between mosquito net usage and malaria risk:\nmosquito_dag \u0026lt;- dagify( malaria_risk ~ net + income + health + temperature + resistance, net ~ income + health + temperature + eligible + household, eligible ~ income + household, health ~ income, exposure = \u0026quot;net\u0026quot;, outcome = \u0026quot;malaria_risk\u0026quot;, coords = list(x = c(malaria_risk = 7, net = 3, income = 4, health = 5, temperature = 6, resistance = 8.5, eligible = 2, household = 1), y = c(malaria_risk = 2, net = 2, income = 3, health = 1, temperature = 3, resistance = 2, eligible = 3, household = 2)), labels = c(malaria_risk = \u0026quot;Risk of malaria\u0026quot;, net = \u0026quot;Mosquito net\u0026quot;, income = \u0026quot;Income\u0026quot;, health = \u0026quot;Health\u0026quot;, temperature = \u0026quot;Nighttime temperatures\u0026quot;, resistance = \u0026quot;Insecticide resistance\u0026quot;, eligible = \u0026quot;Eligible for program\u0026quot;, household = \u0026quot;Number in household\u0026quot;) ) ggdag_status(mosquito_dag, use_labels = \u0026quot;label\u0026quot;, text = FALSE) + guides(fill = FALSE, color = FALSE) + # Disable the legend theme_dag() Following the logic of do-calculus, we can find all the nodes that confound the relationship between net usage and malaria risk, since those nodes open up backdoor paths and distort the causal effect we care about. We can either do this graphically by looking for any node that points to both net and malaria risk, or we can use R:\nadjustmentSets(mosquito_dag) ## { health, income, temperature } Based on the relationships between all the nodes in the DAG, adjusting for health, income, and temperature is enough to close all backdoors and identify the relationship between net use and malaria risk.\n Naive correlation-isn’t-causation estimate For fun, we can calculate the difference in average malaria risk for those who did/didn’t use mosquito nets. This is most definitely not the actual causal effect—this is the “correlation is not causation” effect that doesn’t account for any of the backdoors in the DAG.\nWe can do this with a table (but then we have to do manual math to subtract the FALSE average from the TRUE average):\nnets %\u0026gt;% group_by(net) %\u0026gt;% summarize(number = n(), avg = mean(malaria_risk)) ## # A tibble: 2 x 3 ## net number avg ## \u0026lt;lgl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 FALSE 1071 41.9 ## 2 TRUE 681 25.6 Or we can do it with regression:\nmodel_wrong \u0026lt;- lm(malaria_risk ~ net, data = nets) tidy(model_wrong) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 41.9 0.405 104. 0. ## 2 netTRUE -16.3 0.649 -25.1 2.25e-119 According to this estimate, using a mosquito net is associated with a 16.33 point decrease in malaria risk, on average. We can’t legally talk about this as a causal effect though—there are confounding variables to deal with.\n Matching We can use matching techniques to pair up similar observations and make the unconfoundedness assumption—that if we see two observations that are pretty much identical, and one used a net and one didn’t, the choice to use a net was random.\nBecause we know from the DAG that income, nighttime temperatures, and health help cause both net use and malaria risk (and confound that relationship!), we’ll try to find observations with similar values of income, temperatures, and health that both used and didn’t use nets.\nWe can use the matchit() function from the MatchIt R package to match points based on Mahalanobis distance. There are lots of other options available—see the online documentation for details.\nWe can include the replace = TRUE option to make it so that points that have been matched already can be matched again (that is, we’re not forcing a one-to-one matching; we have one-to-many matching instead).\nStep 1: Preprocess matched_data \u0026lt;- matchit(net ~ income + temperature + health, data = nets, method = \u0026quot;nearest\u0026quot;, distance = \u0026quot;mahalanobis\u0026quot;, replace = TRUE) summary(matched_data) ## ## Call: ## matchit(formula = net ~ income + temperature + health, data = nets, ## method = \u0026quot;nearest\u0026quot;, distance = \u0026quot;mahalanobis\u0026quot;, replace = TRUE) ## ## Summary of balance for all data: ## Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean eQQ Max ## income 955 873 173 82.44 88.0 83.2 153.0 ## temperature 23 24 4 -0.71 0.7 0.7 1.3 ## health 55 48 17 6.85 7.0 6.9 11.0 ## ## ## Summary of balance for matched data: ## Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean eQQ Max ## income 955 950 191.2 5.175 39.0 40.97 95.0 ## temperature 23 23 4.1 -0.027 0.2 0.23 0.7 ## health 55 55 18.2 0.379 3.0 3.36 7.0 ## ## Percent Balance Improvement: ## Mean Diff. eQQ Med eQQ Mean eQQ Max ## income 94 56 51 38 ## temperature 96 71 67 46 ## health 94 57 51 36 ## ## Sample sizes: ## Control Treated ## All 1071 681 ## Matched 439 681 ## Unmatched 632 0 ## Discarded 0 0 Here we can see that all 681 of the net users were paired with similar-looking non-users (439 of them). 632 people weren’t matched and will get discarded. If you’re curious, you can see which treated rows got matched to which control rows by running matched_data$match.matrix.\nWe can create a new data frame of those matches with match.data():\nmatched_data_for_real \u0026lt;- match.data(matched_data)  Step 2: Estimation Now that the data has been matched, it should work better for modeling. Also, because we used income, temperatures, and health in the matching process, we’ve adjusted for those DAG nodes and have closed those backdoors, so our model can be pretty simple here:\nmodel_matched \u0026lt;- lm(malaria_risk ~ net, data = matched_data_for_real) tidy(model_matched) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 38.5 0.600 64.2 0. ## 2 netTRUE -12.9 0.769 -16.8 2.31e-56 The 12.88 point decrease here is better than the naive estimate, but it’s not the true 10 point causal effect (that I built in to the data). Perhaps that’s because the matches aren’t great, or maybe we threw away too much data. There are a host of diagnostics you can look at to see how well things are matched (check the documentation for MatchIt for examples.)\nActually, the most likely culprit for the incorrect estimate is that there’s some imbalance in the data. Because we set replace = TRUE, we did not do 1:1 matching—untreated observations were paired with more than one treated observation. As a result, the multiply-matched observations are getting overcounted and have too much importance in the model. Fortunately, matchit() provides us with a column called weights that allows us to scale down the overmatched observations when running the model. Importantly, these weights have nothing to do with causal inference or backdoors or inverse probability weighting—their only purpose is to help scale down the imbalance arising from overmatching. If you use replace = FALSE and enforce 1:1 matching, the whole weights column will just be 1.\nWe can incorporate those weights into the model and get a more accurate estimate:\nmodel_matched_wts \u0026lt;- lm(malaria_risk ~ net, data = matched_data_for_real, weights = weights) tidy(model_matched_wts) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 36.1 0.595 60.6 0. ## 2 netTRUE -10.5 0.763 -13.7 7.98e-40 After weighting to account for under- and over-matching, we find a -10.49 point causal effect. That’s much better than any of the other estimates we’ve tried so far! The reason it’s accurate is because we’ve closed the confounding backdoors and isolated the arrow between net use and malaria risk.\n  Inverse probability weighting One potential downside to matching is that you generally have to throw away a sizable chunk of your data—anything that’s unmatched doesn’t get included in the final matched data.\nAn alternative approach to matching is to assign every observation some probability of receiving treatment, and then weight each observation by its inverse probability—observations that are predicted to get treatment and then don’t, or observations that are predicted to not get treatment and then do will receive more weight than the observations that get/don’t get treatment as predicted.\nGenerating these inverse probability weights requires a two step process: (1) we first generate propensity scores, or the probability of receiving treatment, and then (2) we use a special formula to convert those propensity scores into weights. Once we have inverse probability weights weights, we can incorporate them into our regression model.\nOversimplified crash course in logistic regression There are many ways to generate propensity scores (like logistic regression, probit regression, and even machine learning techniques like random forests and neural networks), but logistic regression is probably the most common method.\nThe complete technical details of logistic regression are beyond the scope of this class, but if you’re curious you should check out this highly accessible tutorial.\nAll you really need to know is that the outcome variable in logistic regression models must be binary, and the explanatory variables you include in the model help explain the variation in the likelihood of your binary outcome. The Y (or outcome) in logistic regression is a logged ratio of probabilities, which forces the model’s output to be in a 0-1 range:\n\\[ \\log \\frac{p_y}{p_{1-y}} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n + \\epsilon \\]\nHere’s what it looks like visually. Because net usage is a binary outcome, there are lines of observations at 0 and 1 along the y axis. The blue curvy line here shows the output of a logistic regression model—people with low income have a low likelihood of using a net, while those with high income are far more likely to do so.\nI also included a red line showing the results from a regular old lm() OLS model. It follows the blue line fairly well for a while, but predicts negative probabilities if you use lower values of income, like less than 250. For strange historical and mathy reasons, many economists like using OLS on binary outcomes (they even have a fancy name for it: linear probability models (LPMs)), but I’m partial to logistic regression since it doesn’t generate probabilities greater than 100% or less than 0%. (BUT DON’T EVER COMPLAIN ABOUT LPMs ONLINE. You’ll start battles between economists and other social scientists. 🤣)\nggplot(nets, aes(x = income, y = net_num)) + geom_point(alpha = 0.1) + geom_smooth(method = \u0026quot;lm\u0026quot;, color = \u0026quot;red\u0026quot;, size = 0.5) + geom_smooth(method = \u0026quot;glm\u0026quot;, method.args = list(family = binomial(link = \u0026quot;logit\u0026quot;))) + labs(x = \u0026quot;Income\u0026quot;, y = \u0026quot;Probability of using a net\u0026quot;) The coefficients from a logistic regression model are interpreted differently than you’re used to (and their interpretations can be controversial!). Here’s an example for the model in the graph above:\n# Notice how we use glm() instead of lm(). The \u0026quot;g\u0026quot; stands for \u0026quot;generalized\u0026quot; # linear model. We have to specify a family in any glm() model. You can # technically run a regular OLS model (like you do with lm()) if you use # glm(y ~ x1 + x2, family = gaussian(link = \u0026quot;identity\u0026quot;)), but people rarely do that. # # To use logistic regression, you have to specify a binomial/logit family like so: # family = binomial(link = \u0026quot;logit\u0026quot;) model_logit \u0026lt;- glm(net ~ income + temperature + health, data = nets, family = binomial(link = \u0026quot;logit\u0026quot;)) tidy(model_logit) ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) -1.32 0.376 -3.50 0.000464 ## 2 income 0.00209 0.000421 4.95 0.000000727 ## 3 temperature -0.0589 0.0125 -4.70 0.00000264 ## 4 health 0.00688 0.00430 1.60 0.109 The coefficients here aren’t normal numbers—they’re called “log odds” and represent the change in the logged odds as you move explanatory variables up. For instance, here the logged odds of using a net increase by 0.00688 for every one point increase in your health score. But what do logged odds even mean?! Nobody knows.\nYou can make these coefficients slightly more interpretable by unlogging them and creating something called an “odds ratio.” These coefficients were logged with a natural log, so you unlog them by raising \\(e\\) to the power of the coefficient. The odds ratio for temperature is \\(e^{-0.0589}\\), or 0.94. Odds ratios get interpreted a little differently than regular model coefficients. Odds ratios are all centered around 1—values above 1 mean that there’s an increase in the likelihood of the outcome, while values below 1 mean that there’s a decrease in the likelihood of the outcome.\nOur nighttime temperature odds ratio here is 0.94, which is 0.06 below 1, which means we can say that for every one point increase in nighttime temperatures, a person is 6% less likely to use a net. If the coefficient was something like 1.34, we could say that they’d be 34% more likely to use a net; if it was something like 5.02 we could say that they’d be 5 times more likely to use a net; if it was something like 0.1, we could say that they’re 90% less likely to use a net.\nYou can make R exponentiate the coefficients automatically by including exponentiate = TRUE in tidy():\ntidy(model_logit, exponentiate = TRUE) ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 0.268 0.376 -3.50 0.000464 ## 2 income 1.00 0.000421 4.95 0.000000727 ## 3 temperature 0.943 0.0125 -4.70 0.00000264 ## 4 health 1.01 0.00430 1.60 0.109 BUT AGAIN this goes beyond the scope of this class! Just know that when you build a logistic regression model, you’re using explanatory variables to predict the probability of an outcome.\nJust one last little explanation for why we have to use weird log odds things. When working with binary outcomes, we’re dealing with probabilities, and we can create something called “odds” with probabilities. If there’s a 70% chance of something happening, there’s a 30% chance of it not happening. The ratio of those two probabilities is called “odds”: \\(\\frac{p}{1 - p} = \\frac{0.7}{1 - 0.7} = \\frac{0.7}{0.3} = 2.333\\). Odds typically follow a curvy relationship—as you move up to higher levels of your explanatory variable, the odds get bigger faster. If you log these odds, though (\\(\\log \\frac{p}{1 - p}\\)), the relationship becomes linear, which means we can use regular old linear regression on probabilities. Magic!\nYou can see the relationship between log odds and odds ratios in the first two panels here (this is fake data where X ranges between -5 and 5, and Y is either 0 or 1; you can see the data points in the final panel as dots):\nThe coefficients from logistic regression are log odds because they come from model that creates that nice straight line in the first panel. Log odds are impossible to interpret, so we can unlog them (\\(e^\\beta\\)) to turn them into odds ratios.\nThe bottom panel shows predicted probabilities. You can do one more mathematical transformation with the odds (\\(\\frac{p}{1 - p}\\)) to generate a probability instead of odds: \\(\\frac{\\text{odds}}{1 + \\text{odds}}\\). That is what a propensity score is.\nBUT AGAIN I cannot stress enough how much you don’t need to worry about the inner mechanics of logistic regression for this class! If that went over your head, don’t worry! All we’re doing for IPW is using logistic regression to create propensity scores, and the code below shows how to do that. Behind the scenes you’re moving from log odds (they’re linear!) to odds (they’re interpretable-ish) to probabilities (they’re super interpretable!), but you don’t need to worry about that.\nIf you’re confused, watch this TikTok for consolation.\n Step 1: Generate propensity scores PHEW. With that little tangent into logistic regression done, we can now build a model to generate propensity scores (or predicted probabilities).\nWhen we include variables in the model that generates propensity scores, we’re making adjustments and closing backdoors in the DAG, just like we did with matching. But unlike matching, we’re not throwing any data away! We’re just making some observations more important and others less important.\nFirst we build a model that predicts net usage based on income, nighttime temperatures, and health (since those nodes are our confounders from the DAG):\nmodel_net \u0026lt;- glm(net ~ income + temperature + health, data = nets, family = binomial(link = \u0026quot;logit\u0026quot;)) # We could look at these results if we wanted, but we don\u0026#39;t need to for this class # tidy(model_net, exponentiate = TRUE) We can then plug in the income, temperatures, and health for every row in our dataset and generate a predicted probability using this model:\n# augment_columns() handles the plugging in of values. You need to feed it the # name of the model and the name of the dataset you want to add the predictions # to. The type.predict = \u0026quot;response\u0026quot; argument makes it so the predictions are in # the 0-1 scale. If you don\u0026#39;t include that, you\u0026#39;ll get predictions in an # uninterpretable log odds scale. net_probabilities \u0026lt;- augment_columns(model_net, nets, type.predict = \u0026quot;response\u0026quot;) %\u0026gt;% # The predictions are in a column named \u0026quot;.fitted\u0026quot;, so we rename it here rename(propensity = .fitted) # Look at the first few rows of a few columns net_probabilities %\u0026gt;% select(id, net, income, temperature, health, propensity) %\u0026gt;% head() ## # A tibble: 6 x 6 ## id net income temperature health propensity ## \u0026lt;dbl\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 TRUE 781 21.1 56 0.367 ## 2 2 FALSE 974 26.5 57 0.389 ## 3 3 FALSE 502 25.6 15 0.158 ## 4 4 TRUE 671 21.3 20 0.263 ## 5 5 FALSE 728 19.2 17 0.308 ## 6 6 FALSE 1050 25.3 48 0.429 The propensity scores are in the propensity column. Some people, like person 3, are unlikely to use nets (only a 15.8% chance) given their levels of income, temperature, and health. Others like person 6 have a higher probability (42.9%) since their income and health are higher. Neat.\nNext we need to convert those propensity scores into inverse probability weights, which makes weird observations more important (i.e. people who had a high probability of using a net but didn’t, and vice versa). To do this, we follow this equation:\n\\[ \\frac{\\text{Treatment}}{\\text{Propensity}} - \\frac{1 - \\text{Treatment}}{1 - \\text{Propensity}} \\]\nThis equation will create weights that provide the average treatment effect (ATE), but there are other versions that let you find the average treatment effect on the treated (ATT), average treatment effect on the controls (ATC), and a bunch of others. You can find those equations here.\nWe’ll use mutate() to create a column for the inverse probability weight:\nnet_ipw \u0026lt;- net_probabilities %\u0026gt;% mutate(ipw = (net_num / propensity) + ((1 - net_num) / (1 - propensity))) # Look at the first few rows of a few columns net_ipw %\u0026gt;% select(id, net, income, temperature, health, propensity, ipw) %\u0026gt;% head() ## # A tibble: 6 x 7 ## id net income temperature health propensity ipw ## \u0026lt;dbl\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 TRUE 781 21.1 56 0.367 2.72 ## 2 2 FALSE 974 26.5 57 0.389 1.64 ## 3 3 FALSE 502 25.6 15 0.158 1.19 ## 4 4 TRUE 671 21.3 20 0.263 3.81 ## 5 5 FALSE 728 19.2 17 0.308 1.44 ## 6 6 FALSE 1050 25.3 48 0.429 1.75 These first few rows have fairly low weights—those with low probabilities of using nets didn’t, while those with high probabilities did. But look at person 4! They only had a 26% chance of using a net and they did! That’s weird! They therefore have a higher inverse probability weight (3.81).\n Step 2: Estimation Now that we’ve generated inverse probability weights based on our confounders, we can run a model to find the causal effect of mosquito net usage on malaria risk. Again, we don’t need to include income, temperatures, or health in the model since we already used them when we created the propensity scores and weights:\nmodel_ipw \u0026lt;- lm(malaria_risk ~ net, data = net_ipw, weights = ipw) tidy(model_ipw) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 39.7 0.468 84.7 0. ## 2 netTRUE -10.1 0.658 -15.4 3.21e-50 Cool! After using the inverse probability weights, we find a -10.13 point causal effect. That’s a tiiiny bit off from the true value of 10, but not bad at all!\nIt’s important to check the values of your inverse probability weights. Sometimes they can get too big, like if someone had an income of 0 and the lowest possible health and lived in a snow field and yet still used a net. Having really really high IPW values can throw off estimation. To fix this, we can truncate weights at some lower level. There’s no universal rule of thumb for a good maximum weight—I’ve often seen 10 used. In our mosquito net data, no IPWs are higher than 10 (the max is exactly 10 with person 877), so we don’t need to worry about truncation.\nIf you did want to truncate, you’d do something like this (here we’re truncating at 8 instead of 10 so truncation actually does something):\nnet_ipw \u0026lt;- net_ipw %\u0026gt;% # If the IPW is larger than 8, make it 8, otherwise use the current IPW mutate(ipw_truncated = ifelse(ipw \u0026gt; 8, 8, ipw)) model_ipw_truncated \u0026lt;- lm(malaria_risk ~ net, data = net_ipw, weights = ipw_truncated) tidy(model_ipw_truncated) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 39.7 0.467 85.1 0. ## 2 netTRUE -10.2 0.656 -15.5 4.32e-51 Now the causal effect is -10.19, which is slightly lower and probably less accurate since we don’t really have any exceptional cases messing up our original IPW estimate.\n  Results from all the models All done! We just used observational data to estimate causal effects. Causation without RCTs! Do you know how neat that is?!\nLet’s compare all the ATEs that we just calculated:\nmodelsummary(list(\u0026quot;Naive\u0026quot; = model_wrong, \u0026quot;Matched\u0026quot; = model_matched, \u0026quot;Matched + weights\u0026quot; = model_matched_wts, \u0026quot;IPW\u0026quot; = model_ipw, \u0026quot;IPW truncated at 8\u0026quot; = model_ipw_truncated))    Naive  Matched  Matched + weights  IPW  IPW truncated at 8      (Intercept)  41.937***  38.487***  36.094***  39.679***  39.679***     (0.405)  (0.600)  (0.595)  (0.468)  (0.467)    netTRUE  -16.332***  -12.882***  -10.489***  -10.131***  -10.190***     (0.649)  (0.769)  (0.763)  (0.658)  (0.656)    Num.Obs.  1752  1120  1120  1752  1752    R2  0.265  0.201  0.145  0.119  0.121    R2 Adj.  0.265  0.200  0.144  0.119  0.121       * p \u0026lt; 0.1, ** p \u0026lt; 0.05, *** p \u0026lt; 0.01     Which one is right? In this case, because this is fake simulated data where I built in a 10 point effect, we can see which of these models gets the closest: here, the non-truncated IPW model wins. But that won’t always be the case. Both matching and IPW work well for closing backdoors and adjusting for confounders. In real life, you won’t know the true value, so try multiple ways.\n ","date":1601596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"97486aab3e510983487cd8cf1078d06e","permalink":"https://evalf20.classes.andrewheiss.com/example/matching-ipw/","publishdate":"2020-10-02T00:00:00Z","relpermalink":"/example/matching-ipw/","section":"example","summary":"Video walk-through Program background Our goal Load data and libraries DAG and adjustment sets Naive correlation-isn’t-causation estimate Matching  Step 1: Preprocess Step 2: Estimation  Inverse probability weighting  Oversimplified crash course in logistic regression Step 1: Generate propensity scores Step 2: Estimation  Results from all the models   Video walk-through If you want to follow along with this example, you can download the data below:","tags":null,"title":"Matching and inverse probability weighting","type":"docs"},{"authors":null,"categories":null,"content":"   Program details 1. Check balance 2. Estimate difference   There’s no video for this example, since the R code is fairly straightforward, and since I talked about it in the lecture (see the slides).\nIf you want to follow along with this example, you can download the data below:\n  village_randomized.csv  library(tidyverse) # ggplot(), %\u0026gt;%, mutate(), and friends library(ggdag) # Make DAGs library(scales) # Format numbers with functions like comma(), percent(), and dollar() library(broom) # Convert models to data frames library(patchwork) # Combine ggplots into single composite plots set.seed(1234) # Make all random draws reproducible village_randomized \u0026lt;- read_csv(\u0026quot;data/village_randomized.csv\u0026quot;) Program details In this hypothetical situation, an NGO is planning on launching a training program designed to boost incomes. Based on their experiences in running pilot programs in other countries, they’ve found that older, richer men tend to self-select into the training program. The NGO’s evaluation consultant (you!) drew this causal model explaining the effect of the program on participant incomes, given the confounding caused by age, sex, and prior income:\nincome_dag \u0026lt;- dagify(post_income ~ program + age + sex + pre_income, program ~ age + sex + pre_income, exposure = \u0026quot;program\u0026quot;, outcome = \u0026quot;post_income\u0026quot;, labels = c(post_income = \u0026quot;Post income\u0026quot;, program = \u0026quot;Program\u0026quot;, age = \u0026quot;Age\u0026quot;, sex = \u0026quot;Sex\u0026quot;, pre_income = \u0026quot;Pre income\u0026quot;), coords = list(x = c(program = 1, post_income = 5, age = 2, sex = 4, pre_income = 3), y = c(program = 2, post_income = 2, age = 1, sex = 1, pre_income = 3))) ggdag_status(income_dag, use_labels = \u0026quot;label\u0026quot;, text = FALSE, seed = 1234) + guides(color = FALSE) + theme_dag() The NGO just received funding to run a randomized controlled trial (RCT) in a village, and you’re excited because you can finally manipulate access to the program—you can calculate \\(E(\\text{Post-income} | do(\\text{Program})\\). Following the rules of causal diagrams, you get to delete all the arrows going into the program node:\nincome_dag_rct \u0026lt;- dagify(post_income ~ program + age + sex + pre_income, exposure = \u0026quot;program\u0026quot;, outcome = \u0026quot;post_income\u0026quot;, labels = c(post_income = \u0026quot;Post income\u0026quot;, program = \u0026quot;Program\u0026quot;, age = \u0026quot;Age\u0026quot;, sex = \u0026quot;Sex\u0026quot;, pre_income = \u0026quot;Pre income\u0026quot;), coords = list(x = c(program = 1, post_income = 5, age = 2, sex = 4, pre_income = 3), y = c(program = 2, post_income = 2, age = 1, sex = 1, pre_income = 3))) ggdag_status(income_dag_rct, use_labels = \u0026quot;label\u0026quot;, text = FALSE, seed = 1234) + guides(color = FALSE) + theme_dag()  1. Check balance You ran the study on 1,000 participants over the course of 6 months and you just got your data back.\nBefore calculating the effect of the program, you first check to see how well balanced assignment was, and you find that assignment to the program was pretty much split 50/50, which is good:\nvillage_randomized %\u0026gt;% count(program) %\u0026gt;% mutate(prop = n / sum(n)) ## # A tibble: 2 x 3 ## program n prop ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 No program 503 0.503 ## 2 Program 497 0.497 You then check to see how well balanced the treatment and control groups were in participants’ pre-treatment characteristics:\nvillage_randomized %\u0026gt;% group_by(program) %\u0026gt;% summarize(prop_male = mean(sex_num), avg_age = mean(age), avg_pre_income = mean(pre_income)) ## # A tibble: 2 x 4 ## program prop_male avg_age avg_pre_income ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 No program 0.584 34.9 803. ## 2 Program 0.604 34.9 801. These variables appear fairly well balanced. To check that there aren’t any statistically significant differences between the groups, you make some graphs (you could run t-tests too, but graphs are easier for your non-statistical audience to read later).\nThere were more men in both the treatment and control groups, but the proportion is the same in both, and there’s no substantial difference in sex proportion:\n# Here we save each plot as an object so that we can combine the two plots with # + (which comes from the patchwork package). If you want to see what an # individual plot looks like, you can run `plot_diff_sex`, or whatever the plot # object is named. # # stat_summary() here is a little different from the geom_*() layers you\u0026#39;ve seen # in the past. stat_summary() takes a function (here mean_se()) and runs it on # each of the program groups to get the average and standard error. It then # plots those with geom_pointrange. The fun.args part of this lets us pass an # argument to mean_se() so that we can multiply the standard error by 1.96, # giving us the 95% confidence interval. plot_diff_sex \u0026lt;- ggplot(village_randomized, aes(x = program, y = sex_num, color = program)) + stat_summary(geom = \u0026quot;pointrange\u0026quot;, fun.data = \u0026quot;mean_se\u0026quot;, fun.args = list(mult = 1.96)) + guides(color = FALSE) + labs(x = NULL, y = \u0026quot;Proportion male\u0026quot;) # plot_diff_sex # Uncomment this if you want to see this plot by itself plot_prop_sex \u0026lt;- ggplot(village_randomized, aes(x = program, fill = sex)) + # Using position = \u0026quot;fill\u0026quot; makes the bars range from 0-1 and show the proportion geom_bar(position = \u0026quot;fill\u0026quot;) + labs(x = NULL, y = \u0026quot;Proportion\u0026quot;, fill = NULL) + scale_fill_manual(values = c(\u0026quot;darkblue\u0026quot;, \u0026quot;darkred\u0026quot;)) # Show the plots side-by-side plot_diff_sex + plot_prop_sex The distribution of ages looks basically the same in the treatment and control groups, and there’s no substantial difference in the average age across groups:\nplot_diff_age \u0026lt;- ggplot(village_randomized, aes(x = program, y = age, color = program)) + stat_summary(geom = \u0026quot;pointrange\u0026quot;, fun.data = \u0026quot;mean_se\u0026quot;, fun.args = list(mult = 1.96)) + guides(color = FALSE) + labs(x = NULL, y = \u0026quot;Age\u0026quot;) plot_hist_age \u0026lt;- ggplot(village_randomized, aes(x = age, fill = program)) + geom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;) + guides(fill = FALSE) + labs(x = \u0026quot;Age\u0026quot;, y = \u0026quot;Count\u0026quot;) + facet_wrap(vars(program), ncol = 1) plot_diff_age + plot_hist_age Pre-program income is also distributed the same—and has no substantial difference in averages—across treatment and control groups:\nplot_diff_income \u0026lt;- ggplot(village_randomized, aes(x = program, y = pre_income, color = program)) + stat_summary(geom = \u0026quot;pointrange\u0026quot;, fun.data = \u0026quot;mean_se\u0026quot;, fun.args = list(mult = 1.96)) + guides(color = FALSE) + labs(x = NULL, y = \u0026quot;Pre income\u0026quot;) plot_hist_income \u0026lt;- ggplot(village_randomized, aes(x = pre_income, fill = program)) + geom_histogram(binwidth = 20, color = \u0026quot;white\u0026quot;) + guides(fill = FALSE) + labs(x = \u0026quot;Pre income\u0026quot;, y = \u0026quot;Count\u0026quot;) + facet_wrap(vars(program), ncol = 1) plot_diff_income + plot_hist_income All our pre-treatment covariates look good and balanced! You can now estimate the causal effect of the program.\n 2. Estimate difference You are interested in the causal effect of the program, or\n\\[ E[\\text{Post income}\\ |\\ do(\\text{Program})] \\]\nYou can find this causal effect by calculating the average treatment effect:\n\\[ \\text{ATE} = E(\\overline{\\text{Post income }} | \\text{ Program} = 1) - E(\\overline{\\text{Post income }} | \\text{ Program} = 0) \\]\nThis is simply the average outcome for people in the program minus the average outcome for people not in the program. You calculate the group averages:\nvillage_randomized %\u0026gt;% group_by(program) %\u0026gt;% summarize(avg_post = mean(post_income)) ## # A tibble: 2 x 2 ## program avg_post ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 No program 1180. ## 2 Program 1279. That’s 1279 − 1180, or 99, which means that the program caused an increase of $99 in incomes, on average.\nFinding that difference required some manual math, so as a shortcut, you run a regression model with post-program income as the outcome variable and the program indicator variable as the explanatory variable. The coefficient for program is the causal effect (and it includes information about standard errors and significance). You find the same result:\nmodel_rct \u0026lt;- lm(post_income ~ program, data = village_randomized) tidy(model_rct) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 1180. 4.27 276. 0. ## 2 programProgram 99.2 6.06 16.4 1.23e-53 Based on your RCT, you conclude that the program causes an average increase of $99.25 in income.\nggplot(village_randomized, aes(x = program, y = post_income, color = program)) + stat_summary(geom = \u0026quot;pointrange\u0026quot;, fun.data = \u0026quot;mean_se\u0026quot;, fun.args = list(mult = 1.96)) + guides(color = FALSE) + labs(x = NULL, y = \u0026quot;Post income\u0026quot;)  ","date":1601596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"ae80b25ea7f427de547b910a0aec089d","permalink":"https://evalf20.classes.andrewheiss.com/example/rcts/","publishdate":"2020-10-02T00:00:00Z","relpermalink":"/example/rcts/","section":"example","summary":"Program details 1. Check balance 2. Estimate difference   There’s no video for this example, since the R code is fairly straightforward, and since I talked about it in the lecture (see the slides).\nIf you want to follow along with this example, you can download the data below:\n  village_randomized.csv  library(tidyverse) # ggplot(), %\u0026gt;%, mutate(), and friends library(ggdag) # Make DAGs library(scales) # Format numbers with functions like comma(), percent(), and dollar() library(broom) # Convert models to data frames library(patchwork) # Combine ggplots into single composite plots set.","tags":null,"title":"Randomized controlled trials (RCTs)","type":"docs"},{"authors":null,"categories":null,"content":"   Readings Slides Videos   Readings   Randall Munroe, “Significant”  Alexander Coppock, “10 Things to Know About Statistical Power”  Play around with FiveThirtyEight, “Hack Your Way To Scientific Glory”  Chapter 9 in Impact Evaluation in Practice Paul J. Gertler et al., Impact Evaluation in Practice, 2nd ed. (Inter-American Development Bank; World Bank, 2016), https://openknowledge.worldbank.org/handle/10986/25030.   Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nIntroduction  Construct validity  Statistical conclusion validity  Internal validity  External validity                   Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Construct validity Statistical conclusion validity Internal validity External validity  You can also watch the playlist (and skip around to different sections) here:\n   ","date":1601251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"4fb51266c58f677bf1e1454e5cd9c527","permalink":"https://evalf20.classes.andrewheiss.com/content/06-content/","publishdate":"2020-09-28T00:00:00Z","relpermalink":"/content/06-content/","section":"content","summary":"Readings Slides Videos   Readings   Randall Munroe, “Significant”  Alexander Coppock, “10 Things to Know About Statistical Power”  Play around with FiveThirtyEight, “Hack Your Way To Scientific Glory”  Chapter 9 in Impact Evaluation in Practice Paul J. Gertler et al., Impact Evaluation in Practice, 2nd ed. (Inter-American Development Bank; World Bank, 2016), https://openknowledge.worldbank.org/handle/10986/25030.   Slides The slides for today’s lesson are available online as an HTML file.","tags":null,"title":"Threats to validity","type":"docs"},{"authors":null,"categories":null,"content":"   Instructions 1: DAG I 2: DAG II 3: DAG for your program   For your final project, you will conduct an evaluation for a social program of your choosing. In this assignment, you will decide how to model the causal effect of your program on your primary outcome.\nIf you decide to use a different program for your final project, that’s okay! This assignment doesn’t have to be related to your final program, but it would be extraordinarily helpful—a more polished version of this assignment can be included as part of your final project.\nInstructions You need to complete the three sections listed below. Ideally you should type this in R Markdown and knit your document to HTML or Word or PDF, but you can also write in Word if you want (though your final project will need to be in R Markdown, and this would give you practice).\nI’ve created an R Markdown template you can use here:  causal-model.zip. It’s also available on RStudio.cloud.\nSubmit this assignment as a PDF or Word file on iCollege.\n 1: DAG I Find a news article that makes a causal claim and interpret that claim by drawing an appropriate diagram. The article likely won’t explain all the things the researchers controlled for, so you’ll need to create an ideal DAG. What should be included in the causal process to measure the effect of X on Y?\nExport the figure from dagitty and include it in your assignment, or use this code to draw the DAG with R:\nlibrary(tidyverse) library(ggdag) # Remember that you can change the variable names here--they can be basically # anything, but cannot include spaces. The labels can have spaces. Adjust the # variable names (y, x2, etc) and labels (\u0026quot;Outcome\u0026quot;, \u0026quot;Something\u0026quot;, etc.) as # necessary. my_dag \u0026lt;- dagify(y ~ x1 + x2 + z, z ~ x1, x2 ~ x1 + z, labels = c(\u0026quot;y\u0026quot; = \u0026quot;Outcome\u0026quot;, \u0026quot;x1\u0026quot; = \u0026quot;Something\u0026quot;, \u0026quot;x2\u0026quot; = \u0026quot;Something else\u0026quot;, \u0026quot;z\u0026quot; = \u0026quot;Yet another thing\u0026quot;), exposure = \u0026quot;z\u0026quot;, outcome = \u0026quot;y\u0026quot;) # If you set text = TRUE, you\u0026#39;ll see the variable names in the DAG points # The `seed` argument makes it so that the random layout is the same every time ggdag(my_dag, text = FALSE, use_labels = \u0026quot;label\u0026quot;, seed = 1234) + theme_dag() # If you want the treatment and outcomes colored differently, # replace ggdag() with ggdag_status() ggdag_status(my_dag, text = FALSE, use_labels = \u0026quot;label\u0026quot;, seed = 1234) + theme_dag() + theme(legend.position = \u0026quot;bottom\u0026quot;) # Move legend to bottom for fun Summarize the causal claim. Describe what the authors controlled for and what else you included in the DAG. Justify the inclusion of each node (point) and connection (line) in the graph. (≈150 words)\nIdentify all the frontdoor and backdoor paths between your exposure and outcome. What variables need to be controlled for / adjusted to close the backdoors? Did this happen in the study or article? (≈100 words)\n 2: DAG II Find a different news article with a causal claim and do the same thing as above.\nDraw and include a DAG.\nSummarize the causal claim. Describe what the authors controlled for and what else you included in the DAG. Justify the inclusion of each node (point) and connection (line) in the graph. (≈150 words)\nIdentify all the frontdoor and backdoor paths between your exposure and outcome. What variables need to be controlled for / adjusted to close the backdoors? Did this happen in the study or article? (≈100 words)\n 3: DAG for your program Identify the outcome you care most about from your final project program. Draw a DAG that shows the causal effect of your program’s intervention on the outcome.\nSummarize the causal claim. Describe what needs to be controlled for and what else you included in the DAG. Justify the inclusion of each node (point) and connection (line) in the graph. (≈150 words)\nIdentify all the frontdoor and backdoor paths between your exposure and outcome. What variables need to be controlled for / adjusted to close the backdoors? How might you do this with your evaluation? (≈100 words)\n ","date":1601251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"303ebde249c5eb0b38341b5c0fb44f93","permalink":"https://evalf20.classes.andrewheiss.com/assignment/03-eval-dag/","publishdate":"2020-09-28T00:00:00Z","relpermalink":"/assignment/03-eval-dag/","section":"assignment","summary":"Instructions 1: DAG I 2: DAG II 3: DAG for your program   For your final project, you will conduct an evaluation for a social program of your choosing. In this assignment, you will decide how to model the causal effect of your program on your primary outcome.\nIf you decide to use a different program for your final project, that’s okay! This assignment doesn’t have to be related to your final program, but it would be extraordinarily helpful—a more polished version of this assignment can be included as part of your final project.","tags":null,"title":"Causal model","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Potential outcomes, ATEs, and CATEs example page  Slides Videos   Readings   Prologue and at least one of the four acts from This American Life, “Gardens of Branching Paths,” episode #691, January 10, 2020  Chapter 3 in Impact Evaluation in Practice Paul J. Gertler et al., Impact Evaluation in Practice, 2nd ed. (Inter-American Development Bank; World Bank, 2016), https://openknowledge.worldbank.org/handle/10986/25030.  “Potential outcomes causal model” in Causal Inference: The Mixtape Scott Cunningham, Causal Inference: The Mixtape, 2018, https://www.scunning.com/mixtape.html.  Potential outcomes, ATEs, and CATEs example page  The example page on potential outcomes, ATEs, and CATEs shows how to use R to calculate ATEs and CATEs    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nIntroduction  do()ing observational causal inference  Potential outcomes             Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction do()ing observational causal inference Potential outcomes  You can also watch the playlist (and skip around to different sections) here:\n   ","date":1600646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"744a9e81e6eae570052294b144f8d401","permalink":"https://evalf20.classes.andrewheiss.com/content/05-content/","publishdate":"2020-09-21T00:00:00Z","relpermalink":"/content/05-content/","section":"content","summary":"Readings  Potential outcomes, ATEs, and CATEs example page  Slides Videos   Readings   Prologue and at least one of the four acts from This American Life, “Gardens of Branching Paths,” episode #691, January 10, 2020  Chapter 3 in Impact Evaluation in Practice Paul J. Gertler et al., Impact Evaluation in Practice, 2nd ed. (Inter-American Development Bank; World Bank, 2016), https://openknowledge.worldbank.org/handle/10986/25030.  “Potential outcomes causal model” in Causal Inference: The Mixtape Scott Cunningham, Causal Inference: The Mixtape, 2018, https://www.","tags":null,"title":"DAGs and potential outcomes","type":"docs"},{"authors":null,"categories":null,"content":"   Instructions Assignment outline  1: Measurement and abstraction for full-day kindergarten 2: Measurement and abstraction for your program    For your final project, you will conduct an evaluation for a social program of your choosing. In this assignment, you will decide how to best measure two of the program’s outcomes.\nIf you decide to use a different program for your final project, that’s okay! This assignment doesn’t have to be related to your final program, but it would be extraordinarily helpful—a more polished version of this assignment can be included as part of your final project.\nInstructions You need to complete the two sections listed below. Ideally you should type this in R Markdown and knit your document to HTML or Word or PDF, but you can also write in Word if you want (though your final project will need to be in R Markdown, and this would give you practice).1\nI’ve created an R Markdown template you can use here:  measurement.zip. It’s also available on RStudio.cloud.\nSubmit this assignment as a PDF or Word file on iCollege.\n Assignment outline 1: Measurement and abstraction for full-day kindergarten Read this article about half-day vs. full-day kindergarten in Utah. The article is 10 years old, and half-day kindergarten still remains standard practice in most Utah school districts.\nPretend you are the administrator of the Optional Extended Day Kindergarten initiative. Based on the Salt Lake Tribune article (which provides hints throughout, and especially in one of the final paragraphs), and based on your own knowledge of educational outcomes, make a list of two (2) possible outcomes of the full-day kindergarten program.\nThen, for each of those two outcomes, do the following: Using the concept of the “ladder of abstraction” that we discussed in class (e.g. identifying a witch, measuring poverty, etc.), make a list of all the possible attributes of the outcome. Narrow this list down to 3–4 key attributes. Discuss how you decided to narrow the concepts and justify why you think these attributes capture the outcome. (≈100 words)\nThen, for each of those attributes, answer these questions:\n Measurable definition: How would you specifically define this attribute? (i.e. if the attribute is “reduced crime”, define it as “The percent change in crime in a specific neighborhood during a certain time frame” or something similar) Ideal measurement: How would you measure this attribute in an ideal world? Feasible measurement: How would you measure this given reality and given limitations in budget, time, etc.? Measurement of program effect: How would to connect this measure to people in the program? How would you check to see if the program itself had an effect?   2: Measurement and abstraction for your program Make a list of two possible outcomes of your selected program. For each of those outcomes, make a list of all the possible attributes. Narrow this list down to 3–4 key attributes. Discuss how you decided to narrow the concepts and justify why you think these attributes capture the outcome. (≈100 words)\nThen, for each of those attributes, answer these questions:\n Measurable definition: How would you specifically define this attribute? (i.e. if the attribute is “reduced crime”, define it as “The percent change in crime in a specific neighborhood during a certain time frame” or something similar) Ideal measurement: How would you measure this attribute in an ideal world? Feasible measurement: How would you measure this given reality and given limitations in budget, time, etc.? Measurement of program effect: How would to connect this measure to people in the program? How would you check to see if the program itself had an effect?     And if you want to be super brave, try using R Markdown’s citation system!↩︎\n   ","date":1600646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"8da1a27ddb6f955f50fd17413ba3fa4a","permalink":"https://evalf20.classes.andrewheiss.com/assignment/02-eval-measurement/","publishdate":"2020-09-21T00:00:00Z","relpermalink":"/assignment/02-eval-measurement/","section":"assignment","summary":"Instructions Assignment outline  1: Measurement and abstraction for full-day kindergarten 2: Measurement and abstraction for your program    For your final project, you will conduct an evaluation for a social program of your choosing. In this assignment, you will decide how to best measure two of the program’s outcomes.\nIf you decide to use a different program for your final project, that’s okay! This assignment doesn’t have to be related to your final program, but it would be extraordinarily helpful—a more polished version of this assignment can be included as part of your final project.","tags":null,"title":"Measurement","type":"docs"},{"authors":null,"categories":null,"content":"   Theory Effect of private school on earnings, no controls Effect of private school on earnings, with regression and controls   library(tidyverse) # ggplot, dplyr, and friends library(broom) # Convert models to data frames library(scales) # This makes it easy to format numbers with dollar(), comma(), etc. library(ggdag) # Draw causal diagrams with R library(modelsummary) # Fancy regression tables Theory According to the theory explained in chapter 2 of Mastering ’Metrics, attending a private university should be causally linked to earnings and income—that is, going to a private school should make you earn more money. The type of education you receive, however, is not the only factor that influences earnings. People self-select into different types of universities and choose where to apply and which offers to accept. They also choose to not apply to some schools and are also rejected from other schools. Unmeasured characteristics help determine both acceptance to schools and earnings.\nWe can draw a simplified version of this causal story with a directed acyclic graph (DAG) or causal model:\ndag \u0026lt;- dagify(Y ~ P + G, P ~ G, exposure = \u0026quot;P\u0026quot;, outcome = \u0026quot;Y\u0026quot;, labels = c(Y = \u0026quot;Earnings\u0026quot;, P = \u0026quot;Private university\u0026quot;, G = \u0026quot;Student characteristics (group)\u0026quot;), coords = list(x = c(Y = 3, P = 1, G = 2), y = c(Y = 1, P = 1, G = 2))) ggdag_status(dag, use_labels = \u0026quot;label\u0026quot;) + guides(color = FALSE) + # Remove legend theme_dag()  Effect of private school on earnings, no controls Because we don’t have a time machine, we can’t look at individual-level counterfactuals—we can’t send Person 1 to a private school, watch them for 40 years after, and then go back and send Person 1 to a public school and watch them for 40 years. To get around this, we can instead calculated the average treatment effect, or ATE, and find the average of all individual-level causal effects. We can use the information from our theory’s DAG to calculate a (hopefully!) accurate ATE.\nFirst we load the data from this CSV file (typed from the table in Mastering ’Metrics):\n  public_private_earnings.csv  # The data doesn\u0026#39;t come with an indicator variable marking if students went to a # private school, so we create one here. If the string \u0026quot;ivy\u0026quot; is found in the # enrolled column, mark it as true, otherwise mark it as false. # # We also create a variable that indicates if someone is in group A or not that # we\u0026#39;ll use in the regression models. We could technically just include the # group variable in the model, where it would be treated as a dummy variable, # but the reference category would be A, not B, so this is a little trick to # force it to show results for group A. schools \u0026lt;- read_csv(\u0026quot;data/public_private_earnings.csv\u0026quot;) %\u0026gt;% mutate(private = ifelse(str_detect(enrolled, \u0026quot;ivy\u0026quot;), TRUE, FALSE)) %\u0026gt;% mutate(group_A = ifelse(group == \u0026quot;A\u0026quot;, TRUE, FALSE)) # Only look at groups A and B, since C and D don\u0026#39;t have people in both public # and private schools. | means \u0026quot;or\u0026quot; schools_small \u0026lt;- schools %\u0026gt;% filter(group == \u0026quot;A\u0026quot; | group == \u0026quot;B\u0026quot;) It’s really tempting to just find the average income for private school and subtract it from public school, like so:\nschools_small %\u0026gt;% group_by(private) %\u0026gt;% summarize(avg_earnings = mean(earnings)) ## # A tibble: 2 x 2 ## private avg_earnings ## \u0026lt;lgl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 FALSE 70000 ## 2 TRUE 90000 It looks like those who went to private school have $20,000 more in earnings! That’s a ton! It’s also incredibly wrong. Don’t do this! This does not account for any confounding or selection bias. According to our DAG, student characteristics are a huge confounder. We need to account for those.\nFollowing the potential outcomes framework, we can find the ATE by combining the conditional ATEs (or CATEs) for subgroups that are likely to predict outcomes or that bunch up similar observations. Let’s use the Group A and B distinction as our subgroups here.\nFirst we can look at the average income for people in each group, divided by whether they went to a private or public school:\navg_earnings \u0026lt;- schools_small %\u0026gt;% group_by(group, private) %\u0026gt;% summarize(avg_earnings = mean(earnings)) avg_earnings ## # A tibble: 4 x 3 ## # Groups: group [2] ## group private avg_earnings ## \u0026lt;chr\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 A FALSE 110000 ## 2 A TRUE 105000 ## 3 B FALSE 30000 ## 4 B TRUE 60000 Here we see that group A has higher income on average than group B, regardless of whether they went to a private school. Everyone in group A earns roughly $100,000, while those in group B earn a lot less.\nWe can find the exact differences in earnings for public vs. private within each group by pulling the values out of the table and taking their differences\n# Pulling out each of the numbers is tedious and a good example of why we use # regression instead. filter() selects rows that match the specified conditions # (here where group and private both equal something) and pull extracts a column # from the dataset. Because we\u0026#39;re filtering this data in a way that makes it # only have one row, pull() will extract a single number. # Group A income_private_A \u0026lt;- avg_earnings %\u0026gt;% filter(group == \u0026quot;A\u0026quot; \u0026amp; private == TRUE) %\u0026gt;% pull(avg_earnings) income_public_A \u0026lt;- avg_earnings %\u0026gt;% filter(group == \u0026quot;A\u0026quot; \u0026amp; private == FALSE) %\u0026gt;% pull(avg_earnings) cate_a \u0026lt;- income_private_A - income_public_A cate_a ## [1] -5000 # Group B income_private_B \u0026lt;- avg_earnings %\u0026gt;% filter(group == \u0026quot;B\u0026quot; \u0026amp; private == TRUE) %\u0026gt;% pull(avg_earnings) income_public_B \u0026lt;- avg_earnings %\u0026gt;% filter(group == \u0026quot;B\u0026quot; \u0026amp; private == FALSE) %\u0026gt;% pull(avg_earnings) cate_b \u0026lt;- income_private_B - income_public_B cate_b ## [1] 30000 The private-public earnings gap for people in Group A (or \\(\\text{CATE}_\\text{Group A}\\)) is -$5,000, while for Group B (or \\(\\text{CATE}_\\text{Group B}\\)) it is $30,000. It seems that there’s a big effect for Group B, but a small reversed effect for A.\nWe want to account for how many people are in each group, though, since A has more people than B, so we calculate the proportion of each group in the same and multiply the group differences by those proportions.\n\\[ \\text{ATE} = \\pi_\\text{Group A} \\text{CATE}_\\text{Group A} + \\pi_\\text{Group B} \\text{CATE}_\\text{Group B} \\]\n# We need to find the proportion of people in each group prop_in_groups \u0026lt;- schools_small %\u0026gt;% group_by(group) %\u0026gt;% summarize(n = n()) %\u0026gt;% mutate(prop = n / nrow(schools_small)) prop_in_groups ## # A tibble: 2 x 3 ## group n prop ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 A 3 0.6 ## 2 B 2 0.4 prop_A \u0026lt;- prop_in_groups %\u0026gt;% filter(group == \u0026quot;A\u0026quot;) %\u0026gt;% pull(prop) prop_B \u0026lt;- prop_in_groups %\u0026gt;% filter(group == \u0026quot;B\u0026quot;) %\u0026gt;% pull(prop) # With those proportions, we can weight the differences in groups correctly weighted_effect \u0026lt;- (cate_a * prop_A) + (cate_b * prop_B) weighted_effect ## [1] 9000 From this it looks like attending a private university causes a bump in earnings of $9,000. This is the average treatment effect (ATE).\n Effect of private school on earnings, with regression and controls Instead of looking at weighted averages (since that’s tedious with just two groups—imagine doing all that with 3+ groups!), we can use a regression model that accounts for differences between groups A and B, since something about group A made them way wealthier than group B regardless of school type.\nWe do that by predicting earnings based on private/public school attendance while also controlling for group:\nmodel_earnings \u0026lt;- lm(earnings ~ private + group_A, data = schools_small) tidy(model_earnings) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 40000. 11952. 3.35 0.0789 ## 2 privateTRUE 10000. 13093. 0.764 0.525 ## 3 group_ATRUE 60000. 13093. 4.58 0.0445 There are three important numbers here. The intercept (or \\(\\alpha\\) in Mastering ’Metrics, or \\(\\beta_0\\)) is $40,000. This represents the earnings for someone with all the switches and sliders in the model set to 0 or turned off—in this case, someone who went to a public school in group B.\nThe group_A coefficient shows the effect of just being in that group. For whatever reason, Group A earns an average of $60,000 more than Group B—for reasons other than education. This allows us to control for the effect of selection.\nThe coefficient for private is the one we care about the most—this is the causal effect of private schools on earnings (assuming we can justify all the controls and the matching into groups). It is $10,000, which means that attending a private school gives you that much of a bump in income. This is larger than the $9,000 we found earlier, but is probably more accurate since we’re accounting for other weighted differences between groups.\n ","date":1600473600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"d2d2b59697aef26532be1bec6b07307b","permalink":"https://evalf20.classes.andrewheiss.com/example/po-ate-cate/","publishdate":"2020-09-19T00:00:00Z","relpermalink":"/example/po-ate-cate/","section":"example","summary":"Theory Effect of private school on earnings, no controls Effect of private school on earnings, with regression and controls   library(tidyverse) # ggplot, dplyr, and friends library(broom) # Convert models to data frames library(scales) # This makes it easy to format numbers with dollar(), comma(), etc. library(ggdag) # Draw causal diagrams with R library(modelsummary) # Fancy regression tables Theory According to the theory explained in chapter 2 of Mastering ’Metrics, attending a private university should be causally linked to earnings and income—that is, going to a private school should make you earn more money.","tags":null,"title":"Potential outcomes, ATEs, and CATEs","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Measurement DAGs DAG example page  Slides Videos   Readings Measurement   The witch trial scene from Monty Python and the Holy Grail  Chapter 5 in Evaluation: A Systematic Approach Peter H. Rossi, Mark W. Lipsey, and Gary T. Henry, Evaluation: A Systematic Approach, 8th ed. (Los Angeles: Sage, 2019). This is available on iCollege.   DAGs   Julia M. Rohrer, “Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data” Julia M. Rohrer, “Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data,” Advances in Methods and Practices in Psychological Science 1, no. 1 (March 2018): 27–42, doi:10.1177/2515245917745629. This will be posted on iCollege.  Section 2 only (pp. 4–11) from Julian Schuessler and Peter Selb, “Graphical Causal Models for Survey Inference.” Julian Schuessler and Peter Selb, “Graphical Causal Models for Survey Inference,” Working Paper (SocArXiv, December 17, 2019), doi:10.31235/osf.io/hbg3m. The PDF is available at SocArXiv.  “Directed acyclical graphs” in Causal Inference: The Mixtape Scott Cunningham, Causal Inference: The Mixtape, 2018, https://www.scunning.com/mixtape.html.   DAG example page  The example page on DAGs shows how to draw and analyze DAGs with both dagitty.net and R + ggdag    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nIntroduction  Abstraction, stretching, and validity  Causal models  Paths, doors, and adjustment                Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Abstraction, stretching, and validity Causal models Paths, doors, and adjustment  You can also watch the playlist (and skip around to different sections) here:\n   ","date":1600041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"570f902af3e232a725f0155a46b1910b","permalink":"https://evalf20.classes.andrewheiss.com/content/04-content/","publishdate":"2020-09-14T00:00:00Z","relpermalink":"/content/04-content/","section":"content","summary":"Readings  Measurement DAGs DAG example page  Slides Videos   Readings Measurement   The witch trial scene from Monty Python and the Holy Grail  Chapter 5 in Evaluation: A Systematic Approach Peter H. Rossi, Mark W. Lipsey, and Gary T. Henry, Evaluation: A Systematic Approach, 8th ed. (Los Angeles: Sage, 2019). This is available on iCollege.   DAGs   Julia M. Rohrer, “Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data” Julia M.","tags":null,"title":"Measurement and DAGs","type":"docs"},{"authors":null,"categories":null,"content":"   Instructions Assignment outline  1: Program background and purpose 2: Program theory 3: Logic model 4: Analysis    For your final project, you will conduct an evaluation for a social program of your choosing. In this assignment, you will explore the program’s background, history, purpose, and theory.\nIf you decide to use a different program for your final project, that’s okay! This assignment doesn’t have to be related to your final program, but it would be helpful—a more polished version of this assignment can be included as part of your final project.\nInstructions You need to complete the four sections listed below. Ideally you should type this in R Markdown and knit your document to HTML or Word or PDF, but you can also write in Word if you want (though your final project will need to be in R Markdown, and this would give you practice).1\nI’ve created an R Markdown template you can use here:  background-theory.zip. It’s also available on RStudio.cloud.\nYou can draw your impact theory and logic model charts by hand or with something like Diagrams.net, Lucidchart, or Creately. Export the image as a PNG, place it in the same directory as your R Markdown file, and include the image with Markdown.\nThe syntax for adding an image in Markdown is fairly simple. Importantly, it is not R code, so don’t try putting it in an R chunk. Just type this:\n![Image caption](/path/to/image.png) Submit this assignment as a PDF or Word file on iCollege.\n Assignment outline 1: Program background and purpose (≈350 words)\nProvide in-depth background about the program. Include details about (1) when it was started, (2) why it was started, (3) what it was designed to address in society. If the program hasn’t started yet, explain why it’s under consideration. Make sure you cite your sources appropriately! (In the past, some students have just copied/pasted text from a program’s website; don’t do that! Describe and analyze the program’s background!)\n 2: Program theory (≈400 words)\nExplain and explore the program’s underlying theory. Sometimes programs will explain why they exist in a mission statement, but often they don’t and you have to infer the theory from what the program looks like when implemented. What did the program designers plan on occurring? Was this theory based on existing research? If so, cite it.\nInclude a simple impact theory graph showing the program’s basic activities and outcomes. Recall from class and your reading that this is focused primarily on the theory and mechanisms, not on the implementation of the program.\n 3: Logic model List every possible input, activity, output, and outcome for the program and provide a brief 1–2 sentence description of each.\nInputs  Something Something else   Activities  Something Something else   Outputs  Something Something else   Outcomes  Something Something else   Diagram Use flowchart software to connect the inputs, activities, outputs, and outcomes and create a complete logic model. Remember that inputs will always feed into activities, and that activities always produce outputs (that’s the whole purpose of an activity: convert an input to an output). Include this as a figure.\n  4: Analysis (≈150 words)\nEvaluate how well the logic model relates to the program theory. Do the inputs, activities, and outputs have a logical, well-grounded connection to the intended outcomes? Under ideal conditions, would the components of the program lead to changes or lasting effects?\n   And if you want to be super brave, try using R Markdown’s citation system!↩︎\n   ","date":1600041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"dd4383da750233e0ba5d481cb55dd1d9","permalink":"https://evalf20.classes.andrewheiss.com/assignment/01-eval-background-theory/","publishdate":"2020-09-14T00:00:00Z","relpermalink":"/assignment/01-eval-background-theory/","section":"assignment","summary":"Instructions Assignment outline  1: Program background and purpose 2: Program theory 3: Logic model 4: Analysis    For your final project, you will conduct an evaluation for a social program of your choosing. In this assignment, you will explore the program’s background, history, purpose, and theory.\nIf you decide to use a different program for your final project, that’s okay! This assignment doesn’t have to be related to your final program, but it would be helpful—a more polished version of this assignment can be included as part of your final project.","tags":null,"title":"Background and theory","type":"docs"},{"authors":null,"categories":null,"content":"   DAGs with dagitty.net DAGs with R, ggdag, and dagitty  Live coding example Basic DAGs Layouts and manual coordinates Node names and labels Paths and adjustment sets Plot DAG from dagitty.net with ggdag()  Mosquito net example  Conditional independencies Mosquito net adjustment sets    DAGs with dagitty.net The easiest way to quickly build DAGs and find adjustment sets and testable implications is to use dagitty.net.\nThis video shows how to use it:\n   DAGs with R, ggdag, and dagitty You can use the ggdag and dagitty packages in R to build and work with DAGs too. I typically draw an initial DAG in my browser with dagitty.net and then I rewrite it in code in R so that it’s more official and formal and reproducible.\nLive coding example    Basic DAGs (This is a heavily cleaned up and annotated version of the code from the video.)\n# Load packages library(tidyverse) # For dplyr, ggplot, and friends library(ggdag) # For plotting DAGs library(dagitty) # For working with DAG logic The general process for making and working with DAGs in R is to create a DAG object with dagify() and then plot it with ggdag(). The documentation for ggdag is really good and helpful and full of examples. Check these pages for all sorts of additional details:\n “An introduction to ggdag” “An introduction to directed acyclic graphs” List of all ggdag-related functions  The syntax for dagify() is similar to the formula syntax you’ve been using for building regression models with lm(). The formulas you use in dagify() indicate the relationships between nodes.\nFor instance, in this DAG, y is caused by x, a, and b (y ~ x + a + b), and x is caused by a and b (x ~ a + b). This means that a and b are confounders. Use the exposure and outcome arguments to specify which nodes are the exposure (or treatment/intervention/program) and the outcome.\n# Create super basic DAG simple_dag \u0026lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = \u0026quot;x\u0026quot;, outcome = \u0026quot;y\u0026quot; ) # Adding a theme_dag() layer to the plot makes it have a white background with no axis labels ggdag(simple_dag) + theme_dag() If you want to plot which nodes are the exposure and outcome, use ggdag_status() instead:\nggdag_status(simple_dag) + theme_dag()  Layouts and manual coordinates Notice how the layout is different in both of those graphs. By default, ggdag() positions the nodes randomly every time using a network algorithm. You can change the algorithm by using the layout argument: ggdag(simple_dag, layout = \"nicely\"). You can see a full list of possible algorithms by running ?layout_tbl_graph_igraph in the console.\nAlternatively, you can specify your own coordinates so that the nodes are positioned in the same place every time. Do this with the coords argument in dagify().\nThe best way to figure out what these numbers should be is to draw the DAG on paper or on a whiteboard and add a grid to it and then figure out the coordinates. For instance, in this DAG there are three rows and three columns: x and y go in the middle row (row 2) while a and b go in the middle column (column 2). It can also be helpful to not include theme_dag() at first so you can see the numbers for the underlying grid. Once you have everything positioned correctly, add theme_dag() to clean it up.\nsimple_dag_with_coords \u0026lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = \u0026quot;x\u0026quot;, outcome = \u0026quot;y\u0026quot;, coords = list(x = c(x = 1, a = 2, b = 2, y = 3), y = c(x = 2, a = 1, b = 3, y = 2)) ) ggdag_status(simple_dag_with_coords) + theme_dag()  Node names and labels The variable names you use do not have to be limited to just x, y, and other lowercase letters. You can any names you want, as long as there are no spaces.\ndag_with_var_names \u0026lt;- dagify( outcome ~ treatment + confounder1 + confounder2, treatment ~ confounder1 + confounder2, exposure = \u0026quot;treatment\u0026quot;, outcome = \u0026quot;outcome\u0026quot; ) ggdag_status(dag_with_var_names) + theme_dag() However, unless you use very short names, it is likely that the text will not fit inside the nodes. To get around this, you can add labels to the nodes using the labels argument in dagify(). Plot the labels by setting use_labels = \"label\" in ggdag(). You can turn off the text in the nodes with text = FALSE in ggdag().\nsimple_dag_with_coords_and_labels \u0026lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = \u0026quot;x\u0026quot;, outcome = \u0026quot;y\u0026quot;, labels = c(y = \u0026quot;Outcome\u0026quot;, x = \u0026quot;Treatment\u0026quot;, a = \u0026quot;Confounder 1\u0026quot;, b = \u0026quot;Confounder 2\u0026quot;), coords = list(x = c(x = 1, a = 2, b = 2, y = 3), y = c(x = 2, a = 1, b = 3, y = 2)) ) ggdag_status(simple_dag_with_coords_and_labels, use_labels = \u0026quot;label\u0026quot;, text = FALSE) + guides(fill = FALSE, color = FALSE) + # Disable the legend theme_dag()  Paths and adjustment sets R can also perform analysis on DAG objects. For example, we can find all the testable implications from the DAG using the impliedConditionalIndependencies() function from the dagitty package. For this simple DAG, there is only one: a should be independent of b. If we had a dataset with columns for each of these variables, we could check if this is true by running cor(a, b) to see if the two are related.\nimpliedConditionalIndependencies(simple_dag) ## a _||_ b We can also find all the paths between x and y using the paths() function from the dagitty package. We can see that there are three open paths between x and y:\npaths(simple_dag) ## $paths ## [1] \u0026quot;x -\u0026gt; y\u0026quot; \u0026quot;x \u0026lt;- a -\u0026gt; y\u0026quot; \u0026quot;x \u0026lt;- b -\u0026gt; y\u0026quot; ## ## $open ## [1] TRUE TRUE TRUE The first open path is fine—we want a single d-connected relationship between treatment and outcome—but the other two indicate that there is confounding from a and b. We can see what each of these paths are with the ggdag_paths() function from the ggdag package:\nggdag_paths(simple_dag_with_coords) + theme_dag() Instead of listing out all the possible paths and identifying backdoors by hand, you can use the adjustmentSets() function in the dagitty package to programmatically find all the nodes that need to be adjusted. Here we see that both a and b need to be controlled for to isolate the x -\u0026gt; y relationship.\nadjustmentSets(simple_dag) ## { a, b } You can also visualize the adjustment sets with ggdag_adjustment_set() in the ggdag package. Make sure you set shadow = TRUE to draw the arrows coming out of the adjusted nodes—by default, those are not included.\nggdag_adjustment_set(simple_dag_with_coords, shadow = TRUE) + theme_dag() R will find minimally sufficient adjustment sets, which includes the fewest number of adjustments needed to close all backdoors between x and y. In this example DAG there was only one set of variables (a and b), but in other situations there could be many possible sets, or none if the causal effect is not identifiable.\n Plot DAG from dagitty.net with ggdag() If you use dagitty.net to draw a DAG, you’ll notice that it generates some code for you in the model code section:\nYou can copy that dag{ ... } code and paste it into R to define a DAG object rather than use the dagify() function. To do this, use the dagitty() function from the dagitty library and include the whole generated model code in single quotes (''):\nmodel_from_dagitty \u0026lt;- dagitty(\u0026#39;dag { bb=\u0026quot;0,0,1,1\u0026quot; \u0026quot;A confounder\u0026quot; [pos=\u0026quot;0.809,0.306\u0026quot;] \u0026quot;Another confounder\u0026quot; [pos=\u0026quot;0.810,0.529\u0026quot;] \u0026quot;Some outcome\u0026quot; [outcome,pos=\u0026quot;0.918,0.432\u0026quot;] \u0026quot;Some treatment\u0026quot; [exposure,pos=\u0026quot;0.681,0.426\u0026quot;] \u0026quot;A confounder\u0026quot; -\u0026gt; \u0026quot;Some outcome\u0026quot; \u0026quot;A confounder\u0026quot; -\u0026gt; \u0026quot;Some treatment\u0026quot; \u0026quot;Another confounder\u0026quot; -\u0026gt; \u0026quot;Some outcome\u0026quot; \u0026quot;Another confounder\u0026quot; -\u0026gt; \u0026quot;Some treatment\u0026quot; \u0026quot;Some treatment\u0026quot; -\u0026gt; \u0026quot;Some outcome\u0026quot; } \u0026#39;) ggdag(model_from_dagitty) + theme_dag() By default it’s going to look ugly because (1) the node labels don’t fit, and (2) slight differences in the coordinates make it so the nodes don’t perfectly align with each other. To fix coordinate alignment, you can modify the numbers in the generated DAG code. Here I rounded the numbers so that they’re all 0.3, 0.8, etc.\nTo fix the label issue, you can add the use_labels argument like normally. Only here, you can’t specify use_labels = \"label\". Instead, when you specify a DAG using dagitty’s code like this, the column in the underlying dataset that contains the labels is named name, so you need to use use_labels = \"name\".\nOther ggdag() variations like ggdag_status() will still work fine.\nmodel_from_dagitty_rounded \u0026lt;- dagitty(\u0026#39;dag { bb=\u0026quot;0,0,1,1\u0026quot; \u0026quot;A confounder\u0026quot; [pos=\u0026quot;0.8,0.3\u0026quot;] \u0026quot;Another confounder\u0026quot; [pos=\u0026quot;0.8,0.5\u0026quot;] \u0026quot;Some outcome\u0026quot; [outcome,pos=\u0026quot;0.9,0.4\u0026quot;] \u0026quot;Some treatment\u0026quot; [exposure,pos=\u0026quot;0.7,0.4\u0026quot;] \u0026quot;A confounder\u0026quot; -\u0026gt; \u0026quot;Some outcome\u0026quot; \u0026quot;A confounder\u0026quot; -\u0026gt; \u0026quot;Some treatment\u0026quot; \u0026quot;Another confounder\u0026quot; -\u0026gt; \u0026quot;Some outcome\u0026quot; \u0026quot;Another confounder\u0026quot; -\u0026gt; \u0026quot;Some treatment\u0026quot; \u0026quot;Some treatment\u0026quot; -\u0026gt; \u0026quot;Some outcome\u0026quot; } \u0026#39;) ggdag_status(model_from_dagitty_rounded, text = FALSE, use_labels = \u0026quot;name\u0026quot;) + guides(color = FALSE) + # Turn off legend theme_dag()   Mosquito net example Conditional independencies You can test if your stated relationships are correct by looking at the conditional independencies that are implied by the DAG. In dagitty.net, these appear in the sidebar in the “Testable implications” section. To show how this works, we’ll use a simulated dataset that I generated about a fictional mosquito net program. Download the data here if you want to follow along:\n  mosquito_nets.csv  Researchers are interested in whether using mosquito nets decreases an individual’s risk of contracting malaria. They have collected data from 1,752 households in an unnamed country and have variables related to environmental factors, individual health, and household characteristics. Additionally, this country has a special government program that provides free mosquito nets to households that meet specific requirements: to qualify for the program, there must be more than 4 members of the household, and the household’s monthly income must be lower than $700 a month. Households are not automatically enrolled in the program, and many do not use it. The data is not experimental—researchers have no control over who uses mosquito nets, and individual households make their own choices over whether to apply for free nets or buy their own nets, as well as whether they use the nets if they have them.\nmosquito_dag \u0026lt;- dagify( malaria_risk ~ net + income + health + temperature + resistance, net ~ income + health + temperature + eligible + household, eligible ~ income + household, health ~ income, exposure = \u0026quot;net\u0026quot;, outcome = \u0026quot;malaria_risk\u0026quot;, coords = list(x = c(malaria_risk = 7, net = 3, income = 4, health = 5, temperature = 6, resistance = 8.5, eligible = 2, household = 1), y = c(malaria_risk = 2, net = 2, income = 3, health = 1, temperature = 3, resistance = 2, eligible = 3, household = 2)), labels = c(malaria_risk = \u0026quot;Risk of malaria\u0026quot;, net = \u0026quot;Mosquito net\u0026quot;, income = \u0026quot;Income\u0026quot;, health = \u0026quot;Health\u0026quot;, temperature = \u0026quot;Nighttime temperatures\u0026quot;, resistance = \u0026quot;Insecticide resistance\u0026quot;, eligible = \u0026quot;Eligible for program\u0026quot;, household = \u0026quot;Number in household\u0026quot;) ) ggdag_status(mosquito_dag, use_labels = \u0026quot;label\u0026quot;, text = FALSE) + guides(fill = FALSE, color = FALSE) + # Disable the legend theme_dag() The causal graph above outlines the complete relationship between mosquito net use and risk of malaria. Each node in the DAG is a column in the dataset collected by the researchers, and includes the following:\n Malaria risk (malaria_risk): The likelihood that someone in the household will be infected with malaria. Measured on a scale of 0–100, with higher values indicating higher risk. Mosquito net (net and net_num): A binary variable indicating if the household used mosquito nets. Eligible for program (eligible): A binary variable indicating if the household is eligible for the free net program. Income (income): The household’s monthly income, in US dollars. Nighttime temperatures (temperature): The average temperature at night, in Celsius. Health (health): Self-reported healthiness in the household. Measured on a scale of 0–100, with higher values indicating better health. Number in household (household): Number of people living in the household. Insecticide resistance (resistance): Some strains of mosquitoes are more resistant to insecticide and thus pose a higher risk of infecting people with malaria. This is measured on a scale of 0–100, with higher values indicating higher resistance.  According to the DAG, malaria risk is caused by income, temperatures, health, insecticide resistance, and mosquito net use. People who live in hotter regions, have lower incomes, have worse health, are surrounded by mosquitoes with high resistance to insecticide, and who do not use mosquito nets are at higher risk of contracting malaria than those who do not. Mosquito net use is caused by income, nighttime temperatures, health, the number of people living in the house, and eligibility for the free net program. People who live in areas that are cooler at night, have higher incomes, have better health, have more people in the home, and are eligible for free government nets are more likely to regularly use nets than those who do not. The DAG also shows that eligibility for the free net program is caused by income and household size, since households must meet specific thresholds to qualify.\nFirst, let’s download the dataset, put in a folder named data, and load it:\n  mosquito_nets.csv  # Load the data. # It\u0026#39;d be a good idea to click on the \u0026quot;mosquito_nets.csv\u0026quot; object in the # Environment panel in RStudio to see what the data looks like after you load it mosquito_nets \u0026lt;- read_csv(\u0026quot;data/mosquito_nets.csv\u0026quot;) We can use this data to check if the relationships defined by our DAG reflect reality. Recall that d-separation implies that nodes are statistically independent of each other and do not transfer associational information. If you draw the mosquito net DAG with dagitty.net, or if you run impliedConditionalIndependencies() in R, you can see a list of all the implied conditional independencies.\nimpliedConditionalIndependencies(mosquito_dag) ## elgb _||_ hlth | incm ## elgb _||_ mlr_ | hlth, incm, net, tmpr ## elgb _||_ rsst ## elgb _||_ tmpr ## hlth _||_ hshl ## hlth _||_ rsst ## hlth _||_ tmpr ## hshl _||_ incm ## hshl _||_ mlr_ | hlth, incm, net, tmpr ## hshl _||_ rsst ## hshl _||_ tmpr ## incm _||_ rsst ## incm _||_ tmpr ## net _||_ rsst ## rsst _||_ tmpr The _||_ symbol in the output here is the \\(\\perp\\) symbol, which means “independent of”. The | in the output means “given”.\nIn the interest of space, we will not verify all these implied independencies, but we can test a few of them:\n \\(\\text{Health} \\perp \\text{Household members}\\): (Read as “Health is independent of household member count”.) Health should be independent of the number of people in each household. In the data, the two variables should not be correlated. This is indeed the case:\ncor(mosquito_nets$health, mosquito_nets$household) ## [1] 9.8e-05 \\(\\text{Income} \\perp \\text{Insecticide resistance}\\): (Read as “Income is independent of insecticide resistance”.) Income should be independent of insecticide resistance. This is again true:\ncor(mosquito_nets$income, mosquito_nets$resistance) ## [1] 0.014 \\(\\text{Malaria risk} \\perp \\text{Household members}\\ |\\ \\text{Health, Income, Bed net use, Temperature}\\): (Read as “Malaria risk is independent of house member count given health, income, bed net use, and temperature”.) Malaria risk should be independent of the number of household members given similar levels of health, income, mosquito net use, and nighttime temperatures. We cannot use cor() to test this implication, since there are many variables involved, but we can use a regression model to check if the number of household members is significantly related to malaria risk. It is not significant (\\(t = -0.17\\), \\(p = 0.863\\)), which means the two are independent, as expected.\nlm(malaria_risk ~ household + health + income + net + temperature, data = mosquito_nets) %\u0026gt;% broom::tidy() ## # A tibble: 6 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 76.2 0.966 78.9 0. ## 2 household -0.0155 0.0893 -0.173 8.63e- 1 ## 3 health 0.148 0.0107 13.9 9.75e- 42 ## 4 income -0.0751 0.00104 -72.6 0. ## 5 netTRUE -10.4 0.266 -39.2 2.63e-241 ## 6 temperature 1.01 0.0310 32.5 1.88e-181  We can check all the other conditional independencies to see if the DAG captures the reality of the full system of factors that influence mosquito net use and malaria risk. If there are substantial and significant correlations between nodes that should be independent, there is likely an issue with the specification of the DAG. Return to the theory of how the phenomena are generated and refine the DAG more.\n Mosquito net adjustment sets There is a direct path between mosquito net use and the risk of malaria, but the effect is not causally identified due to several other open paths. We can either list out all the paths and find which open paths have arrows pointing backwards into the mosquito net node (run paths(mosquito_dag) to see these results), or we can let R find the appropriate adjustment sets automatically:\nadjustmentSets(mosquito_dag) ## { health, income, temperature } Based on the relationships between all the nodes in the DAG, adjusting for health, income, and temperature is enough to close all backdoors and identify the relationship between net use and malaria risk. Importantly, we do not need to worry about any of the nodes related to the government program for free nets, since those nodes are not d-connected to malaria risk. We only need to worry about confounding relationships.\nWe can confirm this graphically with ggdag_adjustment_set():\nggdag_adjustment_set(mosquito_dag, shadow = TRUE, use_labels = \u0026quot;label\u0026quot;, text = FALSE) + theme_dag()   ","date":1599782400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"ff7d5c7455b7d64e0e5d0a65565444eb","permalink":"https://evalf20.classes.andrewheiss.com/example/dags/","publishdate":"2020-09-11T00:00:00Z","relpermalink":"/example/dags/","section":"example","summary":"DAGs with dagitty.net DAGs with R, ggdag, and dagitty  Live coding example Basic DAGs Layouts and manual coordinates Node names and labels Paths and adjustment sets Plot DAG from dagitty.net with ggdag()  Mosquito net example  Conditional independencies Mosquito net adjustment sets    DAGs with dagitty.net The easiest way to quickly build DAGs and find adjustment sets and testable implications is to use dagitty.","tags":null,"title":"DAGs","type":"docs"},{"authors":null,"categories":null,"content":"   Readings Slides Videos   Readings   Chapter 2 in Impact Evaluation in Practice Paul J. Gertler et al., Impact Evaluation in Practice, 2nd ed. (Inter-American Development Bank; World Bank, 2016), https://openknowledge.worldbank.org/handle/10986/25030.  Chapter 2 in Evaluation: A Systematic Approach Peter H. Rossi, Mark W. Lipsey, and Gary T. Henry, Evaluation: A Systematic Approach, 8th ed. (Los Angeles: Sage, 2019). This is available on iCollege.  Chapter 3 in Evaluation: A Systematic Approach Rossi, Lipsey, and Henry, Evaluation. This is available on iCollege.   Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nIntroduction  Reproducibility  Program theories  Logic models \u0026amp; results chains                Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Reproducibility Program theories Logic models \u0026amp; results chains  You can also watch the playlist (and skip around to different sections) here:\n   ","date":1599436800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"267a7446c3b4fe31bb41b58ba3b49b11","permalink":"https://evalf20.classes.andrewheiss.com/content/03-content/","publishdate":"2020-09-07T00:00:00Z","relpermalink":"/content/03-content/","section":"content","summary":"Readings Slides Videos   Readings   Chapter 2 in Impact Evaluation in Practice Paul J. Gertler et al., Impact Evaluation in Practice, 2nd ed. (Inter-American Development Bank; World Bank, 2016), https://openknowledge.worldbank.org/handle/10986/25030.  Chapter 2 in Evaluation: A Systematic Approach Peter H. Rossi, Mark W. Lipsey, and Gary T. Henry, Evaluation: A Systematic Approach, 8th ed. (Los Angeles: Sage, 2019). This is available on iCollege.  Chapter 3 in Evaluation: A Systematic Approach Rossi, Lipsey, and Henry, Evaluation.","tags":null,"title":"Theories of change and logic models","type":"docs"},{"authors":null,"categories":null,"content":"   Getting started Instructions Turning everything in   IMPORTANT: This looks like a lot of work, but it’s mostly copying/pasting chunks of code and changing things.\n Getting started For this problem set, you’ll practice running and interpreting regression models using data about penguins in Antarctica and data on food access and mortality in the US.\nYou’ll be doing all your R work in R Markdown this time (and from now on). You should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.\nYou’ll need to download these two CSV files and put them somewhere on your computer or upload them to RStudio.cloud—preferably in a folder named data in your project folder:\n  penguins.csv  food_health_politics.csv  You’ll also need to download this R Markdown file with a template for this problem set. Download that here and include it in your project:\n  problem-set-2.Rmd  In the end, the structure of your project directory should look something like this:\nyour-project-name\\ your-name_problem-set-2.Rmd your-project-name.Rproj data\\ penguins.csv food_health_politics.csv To check that you put everything in the right places, you can download and unzip this file, which contains everything in the correct structure:\n  problem-set-2.zip  You’ll need to make sure you have these packages installed on your computer: tidyverse and modelsummary. If you try to load one of those packages with library(tidyverse) or library(modelsummary), etc., and R gives an error that the package is missing, use the “Packages” panel in RStudio to install it.\n(Alternatively, you can open the project named “Problem Set 2” on RStudio.cloud and complete the assignment in your browser without needing to install anything. If you don’t have access to the class RStudio.cloud account, please let me know as soon as possible. This link should take you to the project—if it doesn’t, log in and look for the project named “Problem Set 2.”)\nRemember that you can run an entire chunk by clicking on the green play arrow in the top right corner of the chunk. You can also run lines of code line-by-line if you place your cursor on some R code and press ⌘ + enter (for macOS users) or ctrl + enter (for Windows users).\nMake sure you run each chunk sequentially. If you run a chunk in the middle of the document without running previous ones, it might not work, since previous chunks might do things that later chunks depend on.\nRemember, if you’re struggling, please talk to me. Work with classmates too. Don’t suffer in silence!\n Instructions For this problem set, we’re less interested in causal relationships and more interested in the mechanics of manipulating data and running regressions in R. We’ll start caring about identification and causal models in the next problem set. Because of this, don’t put too much causal weight into the interpretations of these different models—this is an actual case of correlation not implying causation.\nThe example for week 2 on regression will be incredibly helpful for this exercise. Reference it. Copy and paste from it liberally.\nRename the R Markdown file named your-name_problem-set-2.Rmd to something that matches your name and open it in RStudio.\n Complete the tasks given in the R Markdown file. Fill out code in the empty chunks provided (you can definitely copy, paste, and adapt from other code in the document or from the regression example—don’t try to write everything from scratch!), and replace text in ALL CAPS with your own. (i.e. You’ll see a bunch of TYPE YOUR ANSWER HEREs. Type your answers there.). Again, you don’t need to type your answers in all caps.\n   Turning everything in When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex) of your document. Upload that file to iCollege.\n ","date":1599436800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"0a0df7e129d6be414268bdde82e10600","permalink":"https://evalf20.classes.andrewheiss.com/assignment/02-problem-set/","publishdate":"2020-09-07T00:00:00Z","relpermalink":"/assignment/02-problem-set/","section":"assignment","summary":"Getting started Instructions Turning everything in   IMPORTANT: This looks like a lot of work, but it’s mostly copying/pasting chunks of code and changing things.\n Getting started For this problem set, you’ll practice running and interpreting regression models using data about penguins in Antarctica and data on food access and mortality in the US.\nYou’ll be doing all your R work in R Markdown this time (and from now on).","tags":null,"title":"Problem set 2","type":"docs"},{"authors":null,"categories":null,"content":"   Readings Recommended readings Slides Videos   Readings   Chapter 2 in Mastering ’Metrics Joshua D. Angrist and Jörn-Steffen Pischke, Mastering ’Metrics: The Path from Cause to Effect (Princeton, NJ: Princeton University Press, 2015).  “Properties of regression” in Causal Inference: The Mixtape Scott Cunningham, Causal Inference: The Mixtape, 2018, https://www.scunning.com/mixtape.html.   Recommended readings Look through your notes on regression from your last stats class. Also, you can skim through these resources:\n  6.1–6.4 in ModernDive Chester Ismay and Albert Y. Kim, ModernDive: An Introduction to Statistical and Data Sciences via R, 2018, https://moderndive.com/.  7.1–7.4 in ModernDive Ismay and Kim, ModernDive.  7.1–7.3 in OpenIntro Statistics David M. Diez, Christopher D. Barr, and Mine Çetinkaya-Rundel, OpenIntro Statistics, 3rd ed., 2017, https://www.openintro.org/stat/textbook.php?stat_book=os.  8.1 in OpenIntro Statistics Diez, Barr, and Çetinkaya-Rundel, OpenIntro Statistics.  We’ll review all this regression stuff in the videos, so don’t panic if this all looks terrifying! Also, take advantage of the videos that accompany the OpenIntro chapters. And also, the OpenIntro chapters are heavier on the math—don’t worry if you don’t understand everything.\n Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nIntroduction  Drawing lines  Lines, Greek, and regression  Null worlds and statistical significance                Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Drawing lines Lines, Greek, and regression Null worlds and statistical significance  You can also watch the playlist (and skip around to different sections) here:\n   ","date":1598832000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"f4445f225dbc16ad343f16f0677c881d","permalink":"https://evalf20.classes.andrewheiss.com/content/02-content/","publishdate":"2020-08-31T00:00:00Z","relpermalink":"/content/02-content/","section":"content","summary":"Readings Recommended readings Slides Videos   Readings   Chapter 2 in Mastering ’Metrics Joshua D. Angrist and Jörn-Steffen Pischke, Mastering ’Metrics: The Path from Cause to Effect (Princeton, NJ: Princeton University Press, 2015).  “Properties of regression” in Causal Inference: The Mixtape Scott Cunningham, Causal Inference: The Mixtape, 2018, https://www.scunning.com/mixtape.html.   Recommended readings Look through your notes on regression from your last stats class. Also, you can skim through these resources:","tags":null,"title":"Regression and inference","type":"docs"},{"authors":null,"categories":null,"content":"   Task 1: Introduce yourself to R, RStudio, and the tidyverse Task 2: Make an RStudio Project Task 3: Work with R   Task 1: Introduce yourself to R, RStudio, and the tidyverse Go the the example page for this week, “Welcome to R, RStudio, and the tidyverse”, and work through the different primers and videos in the four parts of the page.\nIt seems like there’s a lot on the page, but they’re short and go fairly quickly (especially as you get the hang of the syntax). Also, I have no way of seeing what you do or what you get wrong or right, and that’s totally fine! If you get stuck and want to skip some (or if it gets too easy), go right ahead and skip them!\n   Task 2: Make an RStudio Project Use either RStudio.cloud or RStudio on your computer (preferably RStudio on your computer! Follow these instructions to get started!) to create a new RStudio Project.\n Create a folder named “data” in the project folder you just made.\n Download this CSV file and place it in that folder:\n  cars.csv  In RStudio, go to “File” \u0026gt; “New File…” \u0026gt; “R Markdown…” and click “OK” in the dialog without changing anything.\n Delete all the placeholder text in that new file and replace it with this:\n--- title: \u0026quot;Problem set 1\u0026quot; author: \u0026quot;Put your name here\u0026quot; output: html_document --- ```{r load-libraries-data, warning=FALSE, message=FALSE} library(tidyverse) cars \u0026lt;- read_csv(\u0026quot;data/cars.csv\u0026quot;) ``` # Learning R Tell me that you worked through the primers and videos and examples at the example page for this week: WRITE SOMETHING HERE LIKE \u0026quot;I did all the primers and had the time of my life!\u0026quot; or whatever. # My first plots Insert a chunk below and use it to create a scatterplot (hint: `geom_point()`) with diplacement (`displ`) on the x-axis, city MPG (`cty`) on the y-axis, and with the points colored by drive (`drv`). PUT CHUNK HERE Insert a chunk below and use it to create a histogram (hint: `geom_histogram()`) with highway MPG (`hwy`) on the x-axis. Do not include anything on the y-axis (`geom_histogram()` will do that automatically for you). Choose an appropriate bin width. If you\u0026#39;re brave, facet by drive (`drv`). PUT CHUNK HERE # My first data manipulation Insert a chunk below and use it to calculate the average city MPG (`cty`) by class of car (`class`). This won\u0026#39;t be a plot---it\u0026#39;ll be a table. Hint: use a combination of `group_by()` and `summarize()`. PUT CHUNK HERE Save the R Markdown file with some sort of name (without any spaces!)\n Your project folder should look something like this:\n   Task 3: Work with R Remove the text that says “PUT CHUNK HERE” and insert a new R code chunk. Either type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS, or use the “Insert Chunk” menu:\n Follow the instructions for the three chunks of code.\n Knit your document as a Word file (or PDF if you’re brave and installed LaTeX). Use the “Knit” menu:\n Upload the knitted document to iCollege.\n 🎉 Party! 🎉\n  You’ll be doing this same process for all your future problem sets. Each problem set will involve an R Markdown file. You can either create a new RStudio Project directory for all your work:\nOr you can create individual projects for each assignment and project:\n  ","date":1598832000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"267d3eeb134ec40c34f76f19395cc74b","permalink":"https://evalf20.classes.andrewheiss.com/assignment/01-problem-set/","publishdate":"2020-08-31T00:00:00Z","relpermalink":"/assignment/01-problem-set/","section":"assignment","summary":"Task 1: Introduce yourself to R, RStudio, and the tidyverse Task 2: Make an RStudio Project Task 3: Work with R   Task 1: Introduce yourself to R, RStudio, and the tidyverse Go the the example page for this week, “Welcome to R, RStudio, and the tidyverse”, and work through the different primers and videos in the four parts of the page.\nIt seems like there’s a lot on the page, but they’re short and go fairly quickly (especially as you get the hang of the syntax).","tags":null,"title":"Problem set 1","type":"docs"},{"authors":null,"categories":null,"content":"    Live coding example Complete code  Introduction Exploratory questions  How well do SAT scores correlate with freshman GPA? How well does high school GPA correlate with freshman GPA? Is the correlation between SAT scores and freshman GPA stronger for men or for women? Is the correlation between high school GPA and freshman GPA stronger for men or for women?  Models  Do SAT scores predict freshman GPAs? Does a certain type of SAT score have a larger effect on freshman GPAs? Do high school GPAs predict freshman GPAs? College GPA ~ SAT + sex College GPA ~ SAT + high school GPA + sex Which model best predicts freshman GPA? How do you know?     If you want to follow along with this example, you can download the data below:\n  sat_gpa.csv  You can also download a complete .zip file with a finished R Markdown file that you can knit and play with on your own:\n  regression-example.zip  Live coding example    Complete code (This is a heavily cleaned up and annotated version of the code from the video.)\nIntroduction SAT scores have long been a major factor in college admissions, under the assumption that students with higher test scores will perform better in college and receive a higher GPA. The SAT’s popularity has dropped in recent years, though, and this summer, the University of Chicago announced that it would stop requiring SAT scores for all prospective undergraduates.\nEducational Testing Service (ETS), the creator of the SAT, collected SAT scores, high school GPAs, and freshman-year-college GPAs for 1,000 students at an unnamed university.1\nYou are a university admissions officer and you are curious if SAT scores really do predict college performance. You’re also interested in other factors that could influence student performance.\nThe data contains 6 variables:\n sex: The sex of the student (male or female; female is the base case) sat_verbal: The student’s percentile score in the verbal section of the SAT sat_math: The student’s percentile score in the math section of the SAT sat_total: sat_verbal + sat_math gpa_hs: The student’s GPA in high school at graduation gpa_fy: The student’s GPA at the end of their freshman year  # First we load the libraries and data library(tidyverse) # This lets you create plots with ggplot, manipulate data, etc. library(broom) # This lets you convert regression models into nice tables library(modelsummary) # This lets you combine multiple regression models into a single table # Load the data. # It\u0026#39;d be a good idea to click on the \u0026quot;sat_gpa\u0026quot; object in the Environment panel # in RStudio to see what the data looks like after you load it. sat_gpa \u0026lt;- read_csv(\u0026quot;data/sat_gpa.csv\u0026quot;)  Exploratory questions How well do SAT scores correlate with freshman GPA? # Note the syntax here with the $. That lets you access columns. The general # pattern is name_of_data_set$name_of_column cor(sat_gpa$gpa_fy, sat_gpa$sat_total) ## [1] 0.46 SAT scores and first-year college GPA are moderately positively correlated (r = 0.46). As one goes up, the other also tends to go up.\nHere’s what that relationship looks like:\nggplot(sat_gpa, aes(x = sat_total, y = gpa_fy)) + geom_point(size = 0.5) + geom_smooth(method = \u0026quot;lm\u0026quot;, se = FALSE) + labs(x = \u0026quot;Total SAT score\u0026quot;, y = \u0026quot;Freshman GPA\u0026quot;)  How well does high school GPA correlate with freshman GPA? cor(sat_gpa$gpa_fy, sat_gpa$gpa_hs) ## [1] 0.54 High school and freshman GPAs are also moderately correlated (r = 0.54), but with a slightly stronger relationship.\nHere’s what that relationship looks like:\nggplot(sat_gpa, aes(x = gpa_hs, y = gpa_fy)) + geom_point(size = 0.5) + geom_smooth(method = \u0026quot;lm\u0026quot;, se = FALSE) + labs(x = \u0026quot;High school GPA\u0026quot;, y = \u0026quot;Freshman GPA\u0026quot;)   Is the correlation between SAT scores and freshman GPA stronger for men or for women? # We can calculate the correlation for subgroups within the data with slightly # different syntax. Notice how this uses the pipe (%\u0026gt;%), which makes it read # like English. We can say \u0026quot;Take the sat_gpa data set, split it into groups # based on sex, and calculate the correlation between sat_total and gpa_fy in # each of the groups sat_gpa %\u0026gt;% group_by(sex) %\u0026gt;% summarize(correlation = cor(sat_total, gpa_fy)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 2 ## sex correlation ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Female 0.493 ## 2 Male 0.481 We can calculate the correlation between SAT scores and freshman GPA for both sexes to see if there are any differences. The correlation is slightly stronger for women, but it’s hardly noticeable (r = 0.49 for females, r = 0.48 for males)\nThis is apparent visually if we include a trendline for each sex. The lines are essentially parallel:\n# The only difference between this graph and the earlier two is that it is # coloring by sex ggplot(sat_gpa, aes(x = gpa_hs, y = gpa_fy, color = sex)) + geom_point(size = 0.5) + geom_smooth(method = \u0026quot;lm\u0026quot;, se = FALSE) + labs(x = \u0026quot;High school GPA\u0026quot;, y = \u0026quot;Freshman GPA\u0026quot;)  Is the correlation between high school GPA and freshman GPA stronger for men or for women? sat_gpa %\u0026gt;% group_by(sex) %\u0026gt;% summarize(correlation = cor(gpa_hs, gpa_fy)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 2 ## sex correlation ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Female 0.597 ## 2 Male 0.483 There is a noticeable difference between men and women in the correlation between high school and college GPA. For men, the two are moderately correlated (r = 0.48), while for women the relationship is much stronger (r = 0.60). High school grades might be a better predictor of college grades for women than for men.\n  Models Do SAT scores predict freshman GPAs? We can build a model that predicts college GPAs (our outcome variable, or dependent variable) using SAT scores (our main explanatory variable):\n\\[ \\text{freshman GPA} = \\beta_0 + \\beta_1 \\text{SAT total} + \\epsilon \\]\nmodel_sat_gpa \u0026lt;- lm(gpa_fy ~ sat_total, data = sat_gpa) # Look at the model results and include confidence intervals for the coefficients tidy(model_sat_gpa, conf.int = TRUE) ## # A tibble: 2 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 0.00193 0.152 0.0127 9.90e- 1 -0.296 0.300 ## 2 sat_total 0.0239 0.00146 16.4 1.39e-53 0.0210 0.0267 Here’s what these coefficients mean:\n The intercept (or \\(\\beta_0\\)) is 0.002, which means that the average freshman GPA will be 0.002 when the total SAT percentile score is 0. This is a pretty nonsensical number (nobody has a score that low), so we can ignore it. The slope of sat_total (or \\(\\beta_1\\)) is 0.024, which means that a 1 percentile increase in SAT score is associated with a 0.024 point increase in GPA, on average.  We can look at the summary table of the regression to check the \\(R^2\\):\nglance(model_sat_gpa) ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 0.212 0.211 0.658 268. 1.39e-53 1 -999. 2005. 2019. 432. 998 1000 The \\(R^2\\) here is 0.212, which means that SAT scores explain 21% of the variation in freshman GPA. It’s not a fantastic model, but it explains some stuff.\n Does a certain type of SAT score have a larger effect on freshman GPAs? The sat_total variable combines both sat_math and sat_verbal. We can disaggregate the total score to see the effect of each portion of the test on freshman GPA, using the following model:\n\\[ \\text{freshman GPA} = \\beta_0 + \\beta_1 \\text{SAT verbal} + \\beta_2 \\text{SAT math} + \\epsilon \\]\nmodel_sat_gpa_types \u0026lt;- lm(gpa_fy ~ sat_verbal + sat_math, data = sat_gpa) tidy(model_sat_gpa_types, conf.int = TRUE) ## # A tibble: 3 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 0.00737 0.152 0.0484 9.61e- 1 -0.291 0.306 ## 2 sat_verbal 0.0254 0.00286 8.88 3.07e-18 0.0198 0.0310 ## 3 sat_math 0.0224 0.00279 8.04 2.58e-15 0.0169 0.0279 Again, the intercept is meaningless since no student has a zero on both the verbal and the math test. The slope for sat_verbal (or \\(\\beta_1\\)) is 0.025, so a one percentile point increase in the verbal SAT is associated with a 0.025 point increase in GPA, on average, controlling for math scores. Meanwhile, a one percentile point increase in the math SAT (\\(\\beta_2\\)) is associated with a 0.022 point increase in GPA, on average, controlling for verbal scores. These are essentially the same, so at first glance, it doesn’t seem like the type of test has substantial bearing on college GPAs.\nThe adjusted \\(R^2\\) (which we need to look at because we’re using more than one explanatory variable) is 0.211, which means that this model explains 21% of the variation in college GPA. Like before, this isn’t great, but it tells us a little bit about the importance of SAT scores.\nglance(model_sat_gpa_types) ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 0.212 0.211 0.658 134. 2.36e-52 2 -999. 2006. 2026. 432. 997 1000  Do high school GPAs predict freshman GPAs? We can also use high school GPA to predict freshman GPA, using the following model:\n\\[ \\text{freshman GPA} = \\beta_0 + \\beta_1 \\text{high school GPA} + \\epsilon \\]\nmodel_sat_gpa_hs \u0026lt;- lm(gpa_fy ~ gpa_hs, data = sat_gpa) tidy(model_sat_gpa_hs) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 0.0913 0.118 0.775 4.39e- 1 ## 2 gpa_hs 0.743 0.0363 20.4 6.93e-78 The intercept here (\\(\\beta_0\\)) is 0.091, which means that a student with a high school GPA of zero would have a predicted freshman GPA of 0.091, on average. This is nonsensical, so we can ignore it. The slope for gpa_hs (\\(\\beta_1\\)), on the other hand, is helpful. For every 1 point increase in GPA (i.e. moving from 2.0 to 3.0, or 3.0 to 4.0), there’s an associated increase in college GPA of 0.743 points, on average.\nThe \\(R^2\\) value is 0.295, which means that nearly 30% of the variation in college GPA can be explained by high school GPA. Neat.\nglance(model_sat_gpa_hs) ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 0.295 0.295 0.622 418. 6.93e-78 1 -943. 1893. 1908. 386. 998 1000  College GPA ~ SAT + sex Next, we can see how both SAT scores and sex affect variation in college GPA with the following model:\n\\[ \\text{freshman GPA} = \\beta_0 + \\beta_1 \\text{SAT total} + \\beta_2 \\text{sex} + \\epsilon \\]\nmodel_sat_sex \u0026lt;- lm(gpa_fy ~ sat_total + sex, data = sat_gpa) tidy(model_sat_sex, conf.int = TRUE) ## # A tibble: 3 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) -0.0269 0.149 -0.181 8.57e- 1 -0.319 0.265 ## 2 sat_total 0.0255 0.00145 17.6 1.14e-60 0.0227 0.0284 ## 3 sexMale -0.274 0.0414 -6.62 6.05e-11 -0.355 -0.193 Here, stuff gets interesting. The intercept (\\(\\beta_0\\)) is once again nonsensical—females with a 0 score on their SAT would have a -0.027 college GPA on average. There’s a positive effect with our \\(\\beta_1\\) (or sat_total), since controlling for sex, a one percentile point increase in SAT scores is associated with a 0.026 point increase in freshman GPA, on average. If we control for SAT scores, males see an average drop of 0.274 points (\\(\\beta_2\\)) in their college GPAs.\nThe combination of these two variables, however, doesn’t boost the model’s explanatory power that much. The adjusted \\(R^2\\) (which we must look at because we’re using more than one explanatory variable) is 0.243, meaning that the model explains a little over 24% of the variation in college GPAs.\nglance(model_sat_sex) ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 0.245 0.243 0.644 162. 1.44e-61 2 -978. 1964. 1983. 414. 997 1000  College GPA ~ SAT + high school GPA + sex Finally we can see what the effect of SAT scores, high school GPA, and sex is on college GPAs all at the same time, using the following model:\n\\[ \\text{freshman GPA} = \\beta_0 + \\beta_1 \\text{SAT total} + \\beta_2 \\text{high school GPA} + \\beta_3 \\text{sex} + \\epsilon \\]\nmodel_sat_hs_sex \u0026lt;- lm(gpa_fy ~ sat_total + gpa_hs + sex, data = sat_gpa) tidy(model_sat_hs_sex, conf.int = TRUE) ## # A tibble: 4 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) -0.836 0.148 -5.63 2.35e- 8 -1.13 -0.544 ## 2 sat_total 0.0158 0.00150 10.5 9.72e-25 0.0129 0.0188 ## 3 gpa_hs 0.545 0.0394 13.8 6.61e-40 0.467 0.622 ## 4 sexMale -0.143 0.0391 -3.66 2.66e- 4 -0.220 -0.0664 We can say the following things about these results:\n Yet again, the intercept (\\(\\beta_0\\)) can be safely ignored. Here it means that a female with a 0.0 high school GPA and an SAT score of 0 would have a college GPA of -0.84, on average. That’s pretty impossible. The \\(\\beta_1\\) coefficient for sat_total indicates that taking into account high school GPA and sex, a one percentile point increase in a student’s SAT score is associated with a 0.016 point increase in their college GPA, on average. Controlling for SAT scores and sex, a one point increase in high school GPA is associated with a 0.545 point (this is \\(\\beta_2\\)) increase in college GPA, on average. This coefficient is lower than the 0.74 point coefficient we found previously. That’s because SAT scores and sex soaked up some of high school GPA’s explanatory power. Taking SAT scores and high school GPAs into account, males have a 0.143 point lower GPA in college, on average (this is \\(\\beta_3\\))  As always, the adjusted \\(R^2\\) shows us how well the model fits overall (again, we have to look at the adjusted \\(R^2\\) because we have more than one explanatory variable). In this case, the model explains 36.5% of the variation in college GPA, which is higher than any of the previous models (but not phenomenal, in the end).\nglance(model_sat_hs_sex) ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 0.367 0.365 0.590 192. 2.67e-98 3 -890. 1790. 1815. 347. 996 1000  Which model best predicts freshman GPA? How do you know? As you’ve learned in previous stats classes, adjusted \\(R^2\\) generally shows the strength of a model’s fit, or how well the model will predict future values of the outcome variable. If we compare the adjusted \\(R^2\\) for each of the models, we see that the “best” model is the last one.\n# The modelsummary() function takes a bunch of different regression models and # puts them in a neat side-by-side table. In a normal report or analysis, you\u0026#39;d # include all of these once instead of going one by one like we did above. modelsummary(list(model_sat_gpa, model_sat_gpa_types, model_sat_gpa_hs, model_sat_sex, model_sat_hs_sex))    Model 1  Model 2  Model 3  Model 4  Model 5      (Intercept)  0.002  0.007  0.091  -0.027  -0.836     (0.152)  (0.152)  (0.118)  (0.149)  (0.148)    sat_total  0.024    0.026  0.016     (0.001)    (0.001)  (0.002)    sat_verbal   0.025         (0.003)       sat_math   0.022         (0.003)       gpa_hs    0.743   0.545       (0.036)   (0.039)    sexMale     -0.274  -0.143        (0.041)  (0.039)    Num.Obs.  1000  1000  1000  1000  1000    R2  0.212  0.212  0.295  0.245  0.367    R2 Adj.  0.211  0.211  0.295  0.243  0.365    AIC  2004.8  2006.4  1893.0  1963.8  1790.2    BIC  2019.5  2026.0  1907.7  1983.4  1814.8    Log.Lik.  -999.382  -999.189  -943.477  -977.904  -890.108    F  268.270  134.244  418.071  161.762  192.141         This is real data about real students, compiled and cleaned by a professor at Dartmouth.↩︎\n   ","date":1598400000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"41e7d78d0728fc876b7f8b7efc85c5a3","permalink":"https://evalf20.classes.andrewheiss.com/example/regression/","publishdate":"2020-08-26T00:00:00Z","relpermalink":"/example/regression/","section":"example","summary":"Live coding example Complete code  Introduction Exploratory questions  How well do SAT scores correlate with freshman GPA? How well does high school GPA correlate with freshman GPA? Is the correlation between SAT scores and freshman GPA stronger for men or for women? Is the correlation between high school GPA and freshman GPA stronger for men or for women?  Models  Do SAT scores predict freshman GPAs?","tags":null,"title":"Regression","type":"docs"},{"authors":null,"categories":null,"content":"   Readings Slides Videos Optional lab   Readings  The syllabus, content, examples, and assignments pages for this class  Chapter 1 in Impact Evaluation in Practice Paul J. Gertler et al., Impact Evaluation in Practice, 2nd ed. (Inter-American Development Bank; World Bank, 2016), https://openknowledge.worldbank.org/handle/10986/25030.  DJ Patil, “What Makes a Radical and Revolutionary Technology?”  (DJ Patil is the former Chief Data Scientist of the United States under President Obama. He gave this forum address at Brigham Young University on February 13, 2018.)   Stephen Goldsmith, “Next Generation of Public Employees Must Understand Data and Policy”  Hadley Wickham, “Data Science: How is it Different To Statistics?”   Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nIntroduction  Data science and public service  Evidence, evaluation, and causation (1)  Evidence, evaluation, and causation (2)  Class details                   Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Data science and public service Evidence, evaluation, and causation (1) Evidence, evaluation, and causation (2) Class details  You can also watch the playlist (and skip around to different sections) here:\n   Optional lab Here are all the materials we used in the lab:\n CSV of the cars dataset RStudio.cloud project Slides Project .zip file (the slides are in here too)   ","date":1598227200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"fbbae951c935dd3d35f82710943d5efd","permalink":"https://evalf20.classes.andrewheiss.com/content/01-content/","publishdate":"2020-08-24T00:00:00Z","relpermalink":"/content/01-content/","section":"content","summary":"Readings Slides Videos Optional lab   Readings  The syllabus, content, examples, and assignments pages for this class  Chapter 1 in Impact Evaluation in Practice Paul J. Gertler et al., Impact Evaluation in Practice, 2nd ed. (Inter-American Development Bank; World Bank, 2016), https://openknowledge.worldbank.org/handle/10986/25030.  DJ Patil, “What Makes a Radical and Revolutionary Technology?”  (DJ Patil is the former Chief Data Scientist of the United States under President Obama.","tags":null,"title":"Evaluation and the causal revolution","type":"docs"},{"authors":null,"categories":null,"content":"   Part 1: The basics of R and dplyr Part 2: Getting familiar with RStudio Part 3: RStudio Projects Part 4: Getting familiar with R Markdown   Part 1: The basics of R and dplyr For this week’s problem set, you need to work through a few of RStudio’s introductory primers. You’ll do these in your browser and type code and see results there.\nYou’ll learn some of the basics of R, as well as some powerful methods for manipulating data with the dplyr package.\nComplete these primers. It seems like there are a lot, but they’re short and go fairly quickly (especially as you get the hang of the syntax). Also, I have no way of seeing what you do or what you get wrong or right, and that’s totally fine! If you get stuck and want to skip some (or if it gets too easy), go right ahead and skip them!\n The Basics  Visualization Basics Programming Basics  Work with Data  Working with Tibbles Isolating Data with dplyr Deriving Information with dplyr  Visualize Data  Exploratory Data Analysis Bar Charts Histograms Boxplots and Counts Scatterplots Line plots Overplotting and Big Data Customize Your Plots  Tidy Your Data  Reshape Data   Recent versions of tidyr have renamed these core functions: gather() is now pivot_longer() and spread() is now pivot_wider(). The syntax for these pivot_*() functions is slightly different from what it was in gather() and spread(), so you can’t just replace the names. Fortunately, both gather() and spread() still work and won’t go away for a while, so you can still use them as you learn about reshaping and tidying data. It would be worth learning how the newer pivot_*() functions work, eventually, though (see here for examples).\n The content from these primers comes from the (free and online!) book R for Data Science by Garrett Grolemund and Hadley Wickham. I highly recommend the book as a reference and for continuing to learn and use R in the future.\n Part 2: Getting familiar with RStudio The RStudio primers you just worked through are a great introduction to writing and running R code, but you typically won’t type code in a browser when you work with R. Instead, you’ll use a nicer programming environment like RStudio, which lets you type and save code in scripts, run code from those scripts, and see the output of that code, all in the same program.\nTo get familiar with RStudio, watch this video (it’s from PMAP 8921, but the content still applies here):\n   Part 3: RStudio Projects One of the most powerful and useful aspects of RStudio is its ability to manage projects.\nWhen you first open R, it is “pointed” at some folder on your computer, and anything you do will be relative to that folder. The technical term for this is a “working directory.”\nWhen you first open RStudio, look in the area right at the top of the Console pane to see your current working directory. Most likely you’ll see something cryptic: ~/\nThat tilde sign (~) is a shortcut that stands for your user directory. On Windows this is C:\\Users\\your_user_name\\; on macOS this is /Users/your_user_name/. With the working directory set to ~/, R is “pointed” at that folder, and anything you save will end up in that folder, and R will expect any data that you load to be there too.\nIt’s always best to point R at some other directory. If you don’t use RStudio, you need to manually set the working directory to where you want it with setwd(), and many R scripts in the wild include something like setwd(\"C:\\\\Users\\\\bill\\\\Desktop\\\\Important research project\") at the beginning to change the directory. THIS IS BAD THOUGH (see here for an explanation). If you ever move that directory somewhere else, or run the script on a different computer, or share the project with someone, the path will be wrong and nothing will run and you will be sad.\nThe best way to deal with working directories with RStudio is to use RStudio Projects. These are special files that RStudio creates for you that end in a .Rproj extension. When you open one of these special files, a new RStudio instance will open up and be pointed at the correct directory automatically. If you move the folder later or open it on a different computer, it will work just fine and you will not be sad.\nRead this super short chapter on RStudio projects.\n Part 4: Getting familiar with R Markdown To ensure that the analysis and graphics you make are reproducible, you’ll do the majority of your work in this class using R Markdown files.\nDo the following things:\nWatch this video:     Skim through the content at these pages:\n Using Markdown Using R Markdown How it Works Code Chunks Inline Code Markdown Basics (The R Markdown Reference Guide is super useful here.) Output Formats  Watch this video (again, it’s from PMAP 8921, but the content works for this class):\n    ","date":1598227200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"d717f207da4037092765d89f99a6dfbc","permalink":"https://evalf20.classes.andrewheiss.com/example/rstudio-tidyverse/","publishdate":"2020-08-24T00:00:00Z","relpermalink":"/example/rstudio-tidyverse/","section":"example","summary":"Part 1: The basics of R and dplyr Part 2: Getting familiar with RStudio Part 3: RStudio Projects Part 4: Getting familiar with R Markdown   Part 1: The basics of R and dplyr For this week’s problem set, you need to work through a few of RStudio’s introductory primers. You’ll do these in your browser and type code and see results there.\nYou’ll learn some of the basics of R, as well as some powerful methods for manipulating data with the dplyr package.","tags":null,"title":"Welcome to R, RStudio, and the tidyverse!","type":"docs"},{"authors":null,"categories":null,"content":"    Video walk-through Background Education, wages, and father’s education (fake data)  Naive model Check instrument validity 2SLS manually 2SLS in one step Compare results  Education, wages, and parent’s education (multiple instruments) (real data)  Naive model Check instrument validity 2SLS manually 2SLS in one step Compare results Check for weak instruments  Education, wages, and distance to college (control variables) (real data)  Check instrument validity 2SLS estimation Compare results Extra diagnostics    Video walk-through If you want to follow along with this example, you can download these three datasets:\n  father_education.csv  wage2.csv  card.csv  There’s a set of videos that walks through each section below. To make it easier for you to jump around the video examples, I cut the long video into smaller pieces and included them all in one YouTube playlist.\n Getting started Checking instrument validity Manual 2SLS One-step 2SLS Checking instrument validity (real data) 2SLS (real data) Validity + 2SLS (real data)  You can also watch the playlist (and skip around to different sections) here:\n   Background For all these examples, we’re interested in the perennial econometrics question of whether an extra year of education causes increased wages. Economists love this stuff.\nWe’ll explore the question with three different datasets: a fake one I made up and two real ones from published research.\n  father_education.csv  wage2.csv  card.csv  Make sure you load all these libraries before getting started:\nlibrary(tidyverse) # ggplot(), %\u0026gt;%, mutate(), and friends library(broom) # Convert models to data frames library(modelsummary) # Create side-by-side regression tables library(kableExtra) # Add fancier formatting to tables library(estimatr) # Run 2SLS models in one step with iv_robust()  Education, wages, and father’s education (fake data) First let’s use some fake data to see if education causes additional wages.\ned_fake \u0026lt;- read_csv(\u0026quot;data/father_education.csv\u0026quot;) The father_education.csv file contains four variables:\n    Variable name Description    wage Weekly wage  educ Years of education  ability Magical column that measures your ability to work and go to school (omitted variable)  fathereduc Years of education for father    Naive model If we could actually measure ability, we could estimate this model, which closes the confounding backdoor posed by ability and isolates just the effect of education on wages:\nmodel_forbidden \u0026lt;- lm(wage ~ educ + ability, data = ed_fake) tidy(model_forbidden) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) -85.6 7.20 -11.9 1.42e- 30 ## 2 educ 7.77 0.456 17.1 2.08e- 57 ## 3 ability 0.344 0.0104 33.2 2.14e-163 However, in real life we don’t have ability, so we’re stuck with a naive model:\nmodel_naive \u0026lt;- lm(wage ~ educ, data = ed_fake) tidy(model_naive) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) -59.4 10.4 -5.72 1.39e- 8 ## 2 educ 13.1 0.618 21.2 6.47e-83 The naive model overestimates the effect of education on wages (12.2 vs. 9.24) because of omitted variable bias. Education suffers from endogeneity—there are things in the model (like ability, hidden in the error term) that are correlated with it. Any estimate we calculate will be wrong and biased because of selection effects or omitted variable bias (all different names for endogeneity).\n Check instrument validity To fix the endogeneity problem, we can use an instrument to remove the endogeneity from education and instead use a special exogeneity-only version of education. Perhaps someone’s father’s education can be an instrument for education (it’s not the greatest instrument, but we’ll go with it).\nFor an instrument to be valid, it must meet three criteria:\nRelevance: Instrument is correlated with policy variable Exclusion: Instrument is correlated with outcome only through the policy variable Exogeneity: Instrument isn’t correlated with anything else in the model (i.e. omitted variables)  Relevance\nWe can first test relevance by making a scatterplot and running a model of policy ~ instrument:\nggplot(ed_fake, aes(x = fathereduc, y = educ)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;) check_relevance \u0026lt;- lm(educ ~ fathereduc, data = ed_fake) tidy(check_relevance) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 2.25 0.172 13.1 3.67e-36 ## 2 fathereduc 0.916 0.0108 84.5 0. glance(check_relevance) ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 0.877 0.877 0.703 7136. 0 1 -1066. 2137. 2152. 493. 998 1000 This looks pretty good! The F-statistic is definitely above 10 (it’s 7,136!), and there’s a significant relationship between the instrument and policy. I’d say that this is relevant.\nExclusion\nTo check for exclusion, we need to see if there’s a relationship between father’s education and wages that occurs only because of education. If we plot it, we’ll see a relationship:\nggplot(ed_fake, aes(x = fathereduc, y = wage)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;) That’s to be expected, since in our model, father’s education causes education which causes wages—they should be correlated. But we have to use a convincing story + theory to justify the idea that a father’s education increases the hourly wage only because it increases one’s education, and there’s no real statistical test for that. Good luck.\nExogeneity\nThere’s not really a test for exogeneity either, since there’s no way to measure other endogenous variables in the model (that’s the whole reason we’re using IVs in the first place!). Because we have the magical ability column in this fake data, we can test it. Father’s education shouldn’t be related to ability:\nggplot(ed_fake, aes(x = ability, y = fathereduc)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;) And it’s not! We can safely say that it meets the exogeneity assumption.\nIn real life, though there’s no statistical test for exogeneity. We just have to tell a theory-based story that the number of years of education one’s father has is not correlated with anything else in the model (including any omitted variables). Good luck with that—it’s probably not a good instrument. This relates to Scott Cunningham’s argument that instruments have to be weird. According to Scott:\n The reason I think this is because an instrument doesn’t belong in the structural error term and the structural error term is all the intuitive things that determine your outcome. So it must be weird, otherwise it’s probably in the error term.\n Let’s just pretend that father’s education is a valid instrument and move on :)\n 2SLS manually Now we can do two-stage least squares (2SLS) regression and use the instrument to filter out the endogenous part of education. The first stage predicts education based on the instrument (we already ran this model earlier when checking for relevance, but we’ll do it again just for fun):\nfirst_stage \u0026lt;- lm(educ ~ fathereduc, data = ed_fake) Now we want to add a column of predicted education to our original dataset. The easiest way to do that is with the augment_columns() function from the broom library, which plugs values from a dataset into a model to generate predictions:\ned_fake_with_prediction \u0026lt;- augment_columns(first_stage, ed_fake) head(ed_fake_with_prediction) ## # A tibble: 6 x 11 ## wage educ ability fathereduc .fitted .se.fit .resid .hat .sigma .cooksd .std.resid ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 180. 18.5 408. 17.2 18.0 0.0270 0.576 0.00147 0.703 0.000496 0.820 ## 2 100. 16.2 310. 15.5 16.4 0.0224 -0.203 0.00102 0.703 0.0000424 -0.289 ## 3 125. 18.2 303. 17.7 18.4 0.0306 -0.259 0.00189 0.703 0.000129 -0.369 ## 4 178. 16.6 342. 15.6 16.5 0.0223 0.0522 0.00101 0.703 0.00000278 0.0743 ## 5 265. 17.3 534. 14.7 15.8 0.0248 1.58 0.00124 0.702 0.00316 2.25 ## 6 187. 17.5 409. 16.0 16.9 0.0224 0.577 0.00101 0.703 0.000342 0.822 Note a couple of these new columns. .fitted is the fitted/predicted value of education, and it’s the version of education with endogeneity arguably removed. .resid shows how far off the prediction is from educ. The other columns don’t matter so much.\nInstead of dealing with weird names like .fitted, I like to rename the fitted variable to something more understandable after I use augment_columns:\ned_fake_with_prediction \u0026lt;- augment_columns(first_stage, ed_fake) %\u0026gt;% rename(educ_hat = .fitted) head(ed_fake_with_prediction) ## # A tibble: 6 x 11 ## wage educ ability fathereduc educ_hat .se.fit .resid .hat .sigma .cooksd .std.resid ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 180. 18.5 408. 17.2 18.0 0.0270 0.576 0.00147 0.703 0.000496 0.820 ## 2 100. 16.2 310. 15.5 16.4 0.0224 -0.203 0.00102 0.703 0.0000424 -0.289 ## 3 125. 18.2 303. 17.7 18.4 0.0306 -0.259 0.00189 0.703 0.000129 -0.369 ## 4 178. 16.6 342. 15.6 16.5 0.0223 0.0522 0.00101 0.703 0.00000278 0.0743 ## 5 265. 17.3 534. 14.7 15.8 0.0248 1.58 0.00124 0.702 0.00316 2.25 ## 6 187. 17.5 409. 16.0 16.9 0.0224 0.577 0.00101 0.703 0.000342 0.822 We can now use the new educ_hat variable in our second stage model:\nsecond_stage \u0026lt;- lm(wage ~ educ_hat, data = ed_fake_with_prediction) tidy(second_stage) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 28.8 12.7 2.27 2.32e- 2 ## 2 educ_hat 7.83 0.755 10.4 5.10e-24 The estimate for educ_hat is arguably more accurate now because we’ve used the instrument to remove the endogenous part of education and should only have the exogenous part.\n 2SLS in one step Doing all that two-stage work is neat and it helps with the intuition of instrumental variables, but it’s tedious. More importntly, the standard errors for educ_hat are wrong and the \\(R^2\\) and other diagnostics for the second stage model are wrong too. You can use fancy math to adjust these things in the second stage, but we’re not going to do that. Instead, we’ll use a function that does both stages of the 2SLS model at the same time!\nThere are several functions from different R packages that let you do 2SLS, and they all work a little differently and have different benefits:\n iv_robust() from estimatr:  Syntax: outcome ~ treatment | instrument Benefits: Handles robust and clustered standard errors  ivreg() from ivreg:  Syntax: outcome ~ treatment | instrument Benefits: Includes a ton of fancy diagnostics  ivreg() from AER:  Syntax: outcome ~ treatment | instrument Benefits: Includes special tests for weak instruments (that are better than the standard “check if F \u0026gt; 10”), like Anderson-Rubin confidence intervals  lfe() from felm:  Syntax: outcome ~ treatment | fixed effects | instrument Benefits: Handles fixed effects really quickly (kind of like feols() that you used in Problem Set 5)  plm() from plm:  Syntax: outcome ~ treatment | instrument Benefits: Handles panel data (country/year, state/year, etc.)   This page here has more detailed examples of the main three: iv_robust(), ivreg(), and lfe()\nI typically like using iv_robust(), so we’ll do that here. Instead of running a first stage, generating predictions, and running a second stage, we can do it all at once like this:\nmodel_2sls \u0026lt;- iv_robust(wage ~ educ | fathereduc, data = ed_fake) tidy(model_2sls) ## term estimate std.error statistic p.value conf.low conf.high df outcome ## 1 (Intercept) 28.8 11.16 2.6 1.0e-02 6.9 50.7 998 wage ## 2 educ 7.8 0.66 11.8 3.3e-30 6.5 9.1 998 wage The coefficient for educ here is the same as educ_hat from the manual 2SLS model, but here we found it in one line of code! Also, the model’s standard errors and diagnostics are all correct now.\n Compare results We can put all the models side-by-side to compare them:\n# gof_omit here will omit goodness-of-fit rows that match any of the text. This # means \u0026#39;contains \u0026quot;IC\u0026quot; OR contains \u0026quot;Low\u0026quot; OR contains \u0026quot;Adj\u0026quot; OR contains \u0026quot;p.value\u0026quot; # OR contains \u0026quot;statistic\u0026quot; OR contains \u0026quot;se_type\u0026quot;\u0026#39;. Basically we\u0026#39;re getting rid of # all the extra diagnostic information at the bottom modelsummary(list(\u0026quot;Forbidden\u0026quot; = model_forbidden, \u0026quot;OLS\u0026quot; = model_naive, \u0026quot;2SLS (by hand)\u0026quot; = second_stage, \u0026quot;2SLS (automatic)\u0026quot; = model_2sls), gof_omit = \u0026#39;IC|Log|Adj|p\\\\.value|statistic|se_type\u0026#39;, stars = TRUE) %\u0026gt;% # Add a background color to rows 3 and 7 row_spec(c(3, 7), background = \u0026quot;#F5ABEA\u0026quot;)    Forbidden  OLS  2SLS (by hand)  2SLS (automatic)      (Intercept)  -85.571***  -59.378***  28.819**  28.819***     (7.198)  (10.376)  (12.672)  (11.165)    educ  7.767***  13.124***   7.835***     (0.456)  (0.618)   (0.664)    ability  0.344***        (0.010)       educ_hat    7.835***        (0.755)     Num.Obs.  1000  1000  1000     R2  0.673  0.311  0.097  0.261    F  1025.794  451.244  107.639     N     1000       * p \u0026lt; 0.1, ** p \u0026lt; 0.05, *** p \u0026lt; 0.01     Note how the coefficients for educ_hat and educ in the 2SLS models are close to the coefficient for educ in the forbidden model that accounts for ability. That’s the magic of instrumental variables!\n  Education, wages, and parent’s education (multiple instruments) (real data) This data comes from the wage2 dataset in the wooldridge R package (and it’s real!). The data was used in this paper:\n M. Blackburn and D. Neumark (1992), “Unobserved Ability, Efficiency Wages, and Interindustry Wage Differentials,” Quarterly Journal of Economics 107, 1421-1436. https://doi.org/10.3386/w3857\n wage2 \u0026lt;- read_csv(\u0026quot;data/wage2.csv\u0026quot;) This dataset includes a bunch of different variables. If you run library(wooldridge) and then run ?wage you can see the documentation for the data. These are the variables we care about for this example:\n  Variable name Description    wage Monthly wage (1980 dollars)  educ Years of education  feduc Years of education for father  meduc Years of education for mother    To make life easier, we’ll rename some of the columns and get rid of rows with missing data:\ned_real \u0026lt;- wage2 %\u0026gt;% rename(education = educ, education_dad = feduc, education_mom = meduc) %\u0026gt;% na.omit() # Get rid of rows with missing values Naive model We want to again estimate the effect of education on wages, but this time we’ll use both one’s father’s education and one’s mother’s education as instruments. Here’s the naive estimate of the relationship, which suffers from endogeneity:\nmodel_naive \u0026lt;- lm(wage ~ education, data = ed_real) tidy(model_naive) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 175. 92.8 1.89 5.96e- 2 ## 2 education 59.5 6.70 8.88 6.45e-18 This is wrong though! Education is endogenous to unmeasured things in the model (like ability, which lives in the error term). We can isolate the exogenous part of education with an instrument.\n Check instrument validity Before doing any 2SLS models, we want to check the validity of the instruments. Remember, for an instrument to be valid, it should meet these criteria:\nRelevance: Instrument is correlated with policy variable Exclusion: Instrument is correlated with outcome only through the policy variable Exogeneity: Instrument isn’t correlated with anything else in the model (i.e. omitted variables)  Relevance\nWe can check for relevance by looking at the relationship between the instruments and education:\n# Combine father\u0026#39;s and mother\u0026#39;s education into one column so we can plot both at the same time ed_real_long \u0026lt;- ed_real %\u0026gt;% pivot_longer(cols = c(education_dad, education_mom), names_to = \u0026quot;instrument\u0026quot;, values_to = \u0026quot;instrument_value\u0026quot;) ggplot(ed_real_long, aes(x = instrument_value, y = education)) + # Make points semi-transparent because of overplotting geom_point(alpha = 0.2) + geom_smooth(method = \u0026quot;lm\u0026quot;) + facet_wrap(vars(instrument)) model_check_instruments \u0026lt;- lm(education ~ education_dad + education_mom, data = ed_real) tidy(model_check_instruments) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 9.91 0.320 31.0 5.29e-131 ## 2 education_dad 0.219 0.0289 7.58 1.19e- 13 ## 3 education_mom 0.140 0.0337 4.17 3.52e- 5 glance(model_check_instruments) ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 0.202 0.199 2.00 83.3 5.55e-33 2 -1398. 2804. 2822. 2632. 660 663 There’s a clear relationship between both of the instruments and education, and the coefficients for each are significant. The F-statistic for the model is 83, which is higher than 10, which might be a good sign of a strong instrument. However, it’s less than 104, which, according to this paper, is a better threshold for F statistics. So maybe it’s not so relevant in the end. Who knows.\nExclusion\nWe can check for exclusion in part by looking at the relationship between the instruments and the outcome, or wages. We should see some relationship:\nggplot(ed_real_long, aes(x = instrument_value, y = wage)) + geom_point(alpha = 0.5) + geom_smooth(method = \u0026quot;lm\u0026quot;) + facet_wrap(~ instrument) And we do! Now we just have to make the case that the only reason there’s a relationship is that parental education only influences wages through education. Good luck with that.\nExogeneity\nThe last step is to prove exogeneity—that parental education is not correlated with education or wages. Good luck with that too.\n 2SLS manually Now that we’ve maybe found some okay-ish instruments perhaps, we can use them in a two-stage least squares model. I’ll show you how to do it by hand, just to help with the intuition, but then we’ll do it automatically with iv_robust().\nAssuming that parental education is a good instrument, we can use it to remove the endogenous part of education using 2SLS. In the first stage, we predict education using our instruments:\nfirst_stage \u0026lt;- lm(education ~ education_dad + education_mom, data = ed_real) We can then extract the predicted education and add it to our main dataset, renaming the .fitted variable to something more useful along the way:\ned_real_with_predicted \u0026lt;- augment_columns(first_stage, ed_real) %\u0026gt;% rename(education_hat = .fitted) Finally, we can use predicted education to estimate the exogenous effect of education on wages:\nsecond_stage \u0026lt;- lm(wage ~ education_hat, data = ed_real_with_predicted) tidy(second_stage) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) -538. 208. -2.58 1.00e- 2 ## 2 education_hat 112. 15.2 7.35 5.82e-13 The coefficient for education_hat here should arguably be our actual effect!\n 2SLS in one step Again, in real life, you won’t want to do all that. It’s tedious and your standard errors are wrong. Here’s how to do it all in one step:\nmodel_2sls \u0026lt;- iv_robust(wage ~ education | education_dad + education_mom, data = ed_real) tidy(model_2sls) ## term estimate std.error statistic p.value conf.low conf.high df outcome ## 1 (Intercept) -538 214 -2.5 1.2e-02 -959 -117 661 wage ## 2 education 112 16 7.0 5.7e-12 80 143 661 wage The coefficient for education is the same that we found in the manual 2SLS process, but now the errors are correct.\n Compare results Let’s compare all the findings and interpret the results!\nmodelsummary(list(\u0026quot;OLS\u0026quot; = model_naive, \u0026quot;2SLS (by hand)\u0026quot; = second_stage, \u0026quot;2SLS (automatic)\u0026quot; = model_2sls), gof_omit = \u0026#39;IC|Log|Adj|p\\\\.value|statistic|se_type\u0026#39;, stars = TRUE) %\u0026gt;% # Add a background color to rows 3 and 5 row_spec(c(3, 5), background = \u0026quot;#F5ABEA\u0026quot;)    OLS  2SLS (by hand)  2SLS (automatic)      (Intercept)  175.160*  -537.712**  -537.712**     (92.839)  (208.164)  (214.431)    education  59.452***   111.561***     (6.698)   (15.901)    education_hat   111.561***       (15.176)     Num.Obs.  663  663     R2  0.106  0.076  0.025    F  78.786  54.041     N    663       * p \u0026lt; 0.1, ** p \u0026lt; 0.05, *** p \u0026lt; 0.01     The 2SLS effect is roughly twice as large and is arguably more accurate, since it has removed the endogeneity from education. An extra year of school leads to an extra $111.56 dollars a month in income (in 1980 dollars).\n Check for weak instruments The F-statistic in the first stage was 83.3, which is bigger than 10, but not huge. Again, this newer paper argues that relying on 10 as a threshold isn’t great. They provide a new, more powerful test called the tF procedure, but nobody’s written an R function to do that yet, so we can’t use it yet.\nWe can, however, do a couple other tests for instrument strength. First, if you include the diagnostics = TRUE argument when running iv_robust(), you can get a few extra diagnostic statistics. (See the “Details” section in the documentation for iv_robust for more details about what these are.)\nLet’s re-run the 2SLS model with iv_robust with diagnostics on. To see diagnostic details, you can’t use tidy() (since that just shows the coefficients), so you have to use summary():\nmodel_2sls \u0026lt;- iv_robust(wage ~ education | education_dad + education_mom, data = ed_real, diagnostics = TRUE) summary(model_2sls) ## ## Call: ## iv_robust(formula = wage ~ education | education_dad + education_mom, ## data = ed_real, diagnostics = TRUE) ## ## Standard error type: HC2 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) CI Lower CI Upper DF ## (Intercept) -538 214.4 -2.51 1.24e-02 -958.8 -117 661 ## education 112 15.9 7.02 5.66e-12 80.3 143 661 ## ## Multiple R-squared: 0.0247 , Adjusted R-squared: 0.0232 ## F-statistic: 49.2 on 1 and 661 DF, p-value: 5.66e-12 ## ## Diagnostics: ## numdf dendf value p.value ## Weak instruments 2 660 96.2 \u0026lt; 2e-16 *** ## Wu-Hausman 1 660 16.5 5.5e-05 *** ## Overidentifying 1 NA 0.4 0.53 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 The main diagnostic we care about here is the first one: “Weak instruments”. This is a slightly fancier version of just looking at the first-stage F statistic. The null hypothesis for this test is that the instruments we have specified are weak, so we’d like to reject that null hypothesis. Here, the p-value is tiny, so we can safely reject the null and say the instruments likely aren’t weak. (In general, you want a statistically significant weak instruments test).\nAnother approach for checking for weak instruments is to calculate something called the Anderson-Rubin confidence set, which is essentially a 95% confidence interval for your coefficient that shows the stability of the coefficient based on how weak or strong the instrument is. This test was invented in like 1949 and it’s arguably more robust than checking F statistics, but for whatever reason, nobody really teaches it or uses it!. It’s not in any of the textbooks for this class, and it’s really kind of rare. Even if you google “anderson rubin test weak instruments”, you’ll only find a bunch of lecture notes from fancy econometrics classes (like p. 10 here, or p. 4 here, or p. 4 here).\nAdditionally, most of the automatic 2SLS R packages don’t provide an easy way to do this test! The only one I’ve found is in the AER package. Basically, create a 2SLS model with AER’s ivreg() and then feed that model to the anderson.rubin.ci() function from the ivpack function. This doesn’t work with models you make with iv_robust() or any of the other packages that do 2SLS—only with AER’s ivreg(). It’s a hassle.\nlibrary(AER) # For ivreg() library(ivpack) # For IV diagnostics like Anderson-Rubin causal effects # You have to include x = TRUE so that this works with diagnostic functions model \u0026lt;- ivreg(wage ~ education | education_dad + education_mom, data = ed_real, x = TRUE) # AR 95% confidence interval anderson.rubin.ci(model) ## $confidence.interval ## [1] \u0026quot;[ 75.9391400848449 , 152.076319769297 ]\u0026quot; Based on this confidence interval, given the strength (or weakness) of the instruments, the IV estimate could be as low as 75.9 and as high as 152, which is a fairly big range around the $112 effect we found. Neat.\nThere’s no magic threshold to look for in these confidence intervals—you’re mostly concerned with how much potential variability there is. If you’re fine with a causal effect that could be between 76 and 152, great. If you want that range to be narrower, find some better instruments.\n  Education, wages, and distance to college (control variables) (real data) For this last example we’ll estimate the effect of education on wages using a different instrument—geographic proximity to colleges. This data comes from David Card’s 1995 study where he did the same thing, and it’s available in the wooldridge library as card. You can find a description of all variables here; we’ll use these:\n  Variable name Description    lwage Annual wage (log form)  educ Years of education  nearc4 Living close to college (=1) or far from college (=0)  smsa Living in metropolitan area (=1) or not (=0)  exper Years of experience  expersq Years of experience (squared term)  black Black (=1), not black (=0)  south Living in the south (=1) or not (=0)    card \u0026lt;- read_csv(\u0026quot;data/card.csv\u0026quot;) Once again, Card wants to estimate the effect of education on wage. But to remove the endogeneity that comes from ability, he uses a different instrumental variable: proximity to college.\nHe also uses control variables to help explain additional variation in wages: smsa66 + exper + expersq + black + south66.\nIMPORTANT NOTE: When you include controls, every control variable needs to go in both stages. The only things from the first stage that don’t carry over to the second stage are the instruments—notice how nearc4 is only in the first stage, since it’s the instrument, but it’s not in the second stage. The other controls are all in both stages.\nHe thus estimates a model where:\nFirst stage:\n\\[ \\widehat{\\text{educ}} = \\beta_0 + \\beta_1\\text{nearc4} + \\beta_{2-6}\\text{Control variables} \\]\nSecond stage:\n\\[ \\text{lwage} = \\beta_0 + \\beta_1 \\widehat{\\text{educ}} + \\beta_{2-6}\\text{Control variables} \\]\nCheck instrument validity Card provides arguments to support each of three main characteristics of a good instrumental variable:\nRelevancy: People who live close to a 4-year college have easier access to education at a lower costs (no commuting costs and time nor accommodation costs), so they have greater incentives to pursue education. Exclusion: Proximity to a college has no effect on your annual income, unless you decide to pursue further education because of the nearby college. Exogeneity: Individual ability does not depend on proximity to a college.  Let’s see if these assumptions hold up:\nRelevancy\nThere should be a strong relationship between the instrument (distance to college) and education:\nfirst_stage \u0026lt;- lm(educ ~ nearc4 + smsa66 + exper + expersq + black + south66, data = card) tidy(first_stage) ## # A tibble: 7 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 16.8 0.173 96.9 0. ## 2 nearc4 0.334 0.0870 3.84 1.27e- 4 ## 3 smsa66 0.255 0.0849 3.00 2.72e- 3 ## 4 exper -0.409 0.0337 -12.1 4.74e-33 ## 5 expersq 0.000461 0.00165 0.279 7.80e- 1 ## 6 black -0.924 0.0934 -9.90 9.41e-23 ## 7 south66 -0.348 0.0832 -4.18 3.00e- 5 glance(first_stage) ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 0.473 0.472 1.95 449. 0 6 -6271. 12558. 12606. 11369. 3003 3010 Based on this first stage model, nearc4 has a significant relationship to educ, and the model’s joint F statistic is 449, which is definitely bigger than both 10 and 104. Good. We’ll call it relevant.\nExclusion\nFor distance to college to work as an instrument and meet the exclusion restriction, we have to prove that distance to college causes wages only through getting more education. Think about other possible pathways between living close to a college and increased wages—there could be other paths that don’t go through education. Good luck.\nExogeneity\nFor distance to college to work as an exogenous instrument, we have to prove that none of the unobserved confounders between education and earnings are connected to distance. Also good luck.\n 2SLS estimation Assuming distance to education is a valid instrument (sure), we can use it in a 2SLS model. Remember that control variables have to go in both stages, so specify them accordingly in the model formula:\nmodel_2sls \u0026lt;- iv_robust(lwage ~ educ + smsa66 + exper + expersq + black + south66 | nearc4 + smsa66 + exper + expersq + black + south66, data = card, diagnostics = TRUE) tidy(model_2sls) ## term estimate std.error statistic p.value conf.low conf.high df outcome ## 1 (Intercept) 3.3572 0.93042 3.6 3.1e-04 1.5329 5.18155 3003 lwage ## 2 educ 0.1572 0.05481 2.9 4.2e-03 0.0498 0.26468 3003 lwage ## 3 smsa66 0.0810 0.02686 3.0 2.6e-03 0.0283 0.13363 3003 lwage ## 4 exper 0.1184 0.02365 5.0 5.9e-07 0.0720 0.16472 3003 lwage ## 5 expersq -0.0024 0.00037 -6.6 5.0e-11 -0.0031 -0.00170 3003 lwage ## 6 black -0.1036 0.05245 -2.0 4.8e-02 -0.2064 -0.00074 3003 lwage ## 7 south66 -0.0637 0.02798 -2.3 2.3e-02 -0.1186 -0.00885 3003 lwage Cool cool. Based on the coefficient for educ, a year of education causes a 15.7% increase in annual wages, on average.\nIs that an improvement over a naive model where we don’t account for any of the endogeneity?\nmodel_naive \u0026lt;- lm(lwage ~ educ + smsa66 + exper + expersq + black + south66, data = card) tidy(model_naive) ## # A tibble: 7 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 4.73 0.0686 68.9 0. ## 2 educ 0.0762 0.00355 21.5 3.68e-95 ## 3 smsa66 0.113 0.0151 7.47 1.06e-13 ## 4 exper 0.0852 0.00674 12.6 1.02e-35 ## 5 expersq -0.00238 0.000322 -7.39 1.92e-13 ## 6 black -0.177 0.0185 -9.58 1.99e-21 ## 7 south66 -0.0960 0.0161 -5.97 2.64e- 9 Yep! Without removing endogeneity from education, an additional year of education is only associated with a 7.6% increase in annual wages, on average.\n Compare results For fun, we can look at the results side-by-side:\nmodelsummary(list(\u0026quot;Naive OLS\u0026quot; = model_naive, \u0026quot;2SLS\u0026quot; = model_2sls), gof_omit = \u0026#39;IC|Log|Adj|p\\\\.value|statistic|se_type\u0026#39;, stars = TRUE) %\u0026gt;% # Add a background color to row 3 row_spec(3, background = \u0026quot;#F5ABEA\u0026quot;)    Naive OLS  2SLS      (Intercept)  4.731***  3.357***     (0.069)  (0.930)    educ  0.076***  0.157***     (0.004)  (0.055)    smsa66  0.113***  0.081***     (0.015)  (0.027)    exper  0.085***  0.118***     (0.007)  (0.024)    expersq  -0.002***  -0.002***     (0.000)  (0.000)    black  -0.177***  -0.104**     (0.018)  (0.052)    south66  -0.096***  -0.064**     (0.016)  (0.028)    Num.Obs.  3010     R2  0.269  0.143    F  184.606     N   3010       * p \u0026lt; 0.1, ** p \u0026lt; 0.05, *** p \u0026lt; 0.01      Extra diagnostics Finally, we can check for weak instruments issues. The F statistic we found in the first stage was pretty big, so that’s a good sign, but we can look at the first stage’s weak instrument statistic, as well as the Anderson-Rubin confidence interval.\nBecause we included diagnostics = TRUE in the model, we can just use summary() to check weak instruments diagnostics:\nsummary(model_2sls) ## ## Call: ## iv_robust(formula = lwage ~ educ + smsa66 + exper + expersq + ## black + south66 | nearc4 + smsa66 + exper + expersq + black + ## south66, data = card, diagnostics = TRUE) ## ## Standard error type: HC2 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) CI Lower CI Upper DF ## (Intercept) 3.35723 0.930415 3.61 3.13e-04 1.53292 5.18155 3003 ## educ 0.15722 0.054807 2.87 4.15e-03 0.04976 0.26468 3003 ## smsa66 0.08097 0.026858 3.01 2.59e-03 0.02831 0.13363 3003 ## exper 0.11835 0.023649 5.00 5.93e-07 0.07198 0.16472 3003 ## expersq -0.00241 0.000366 -6.60 5.00e-11 -0.00313 -0.00170 3003 ## black -0.10358 0.052451 -1.97 4.84e-02 -0.20643 -0.00074 3003 ## south66 -0.06371 0.027979 -2.28 2.28e-02 -0.11857 -0.00885 3003 ## ## Multiple R-squared: 0.143 , Adjusted R-squared: 0.141 ## F-statistic: 96.7 on 6 and 3003 DF, p-value: \u0026lt;2e-16 ## ## Diagnostics: ## numdf dendf value p.value ## Weak instruments 1 3003 15.31 9.3e-05 *** ## Wu-Hausman 1 3002 2.61 0.11 ## Overidentifying 0 NA NA NA ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 The p-value for the “Weak instruments” test is tiny, which means we can safely reject the null hypothesis that the near college instrument is weak. Neat.\nTo calculate Anderson-Rubin confidence intervals, we need to rerun the model with the ivreg() function (ugh) and feed those results to anderson.rubin.ci():\nmodel_again \u0026lt;- ivreg(lwage ~ educ + smsa66 + exper + expersq + black + south66 | nearc4 + smsa66 + exper + expersq + black + south66, data = card, x = TRUE) anderson.rubin.ci(model_again) ## $confidence.interval ## [1] \u0026quot;[ 0.0570787017353594 , 0.314883971035808 ]\u0026quot; Phew. That’s a pretty wide interval, ranging from 5.7% to 31.5%. It’s still positive, but it could sometimes be fairly small.\n  ","date":1586304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"9d3d2e64f6fa8f403b420be4ea6ebda8","permalink":"https://evalf20.classes.andrewheiss.com/example/iv/","publishdate":"2020-04-08T00:00:00Z","relpermalink":"/example/iv/","section":"example","summary":"Video walk-through Background Education, wages, and father’s education (fake data)  Naive model Check instrument validity 2SLS manually 2SLS in one step Compare results  Education, wages, and parent’s education (multiple instruments) (real data)  Naive model Check instrument validity 2SLS manually 2SLS in one step Compare results Check for weak instruments  Education, wages, and distance to college (control variables) (real data)  Check instrument validity 2SLS estimation Compare results Extra diagnostics    Video walk-through If you want to follow along with this example, you can download these three datasets:","tags":null,"title":"Instrumental variables","type":"docs"},{"authors":null,"categories":null,"content":"    Video walk-through Program background Load and clean data Exploratory data analysis Diff-in-diff by hand Diff-in-diff with regression Diff-in-diff with regression + controls Comparison of results   Video walk-through If you want to follow along with this example, you can download the data below:\n  injury.csv  There’s a set of videos that walks through each section below. To make it easier for you to jump around the video examples, I cut the long video into smaller pieces and included them all in one YouTube playlist.\n Getting started Load and clean data Exploratory data analysis Diff-in-diff manually Diff-in-diff with regression  You can also watch the playlist (and skip around to different sections) here:\n   Program background In 1980, Kentucky raised its cap on weekly earnings that were covered by worker’s compensation. We want to know if this new policy caused workers to spend more time unemployed. If benefits are not generous enough, then workers could sue companies for on-the-job injuries, while overly generous benefits could cause moral hazard issues and induce workers to be more reckless on the job, or to claim that off-the-job injuries were incurred while at work.\nThe main outcome variable we care about is log_duration (in the original data as ldurat, but we rename it to be more human readable), or the logged duration (in weeks) of worker’s compensation benefits. We log it because the variable is fairly skewed—most people are unemployed for a few weeks, with some unemployed for a long time. The policy was designed so that the cap increase did not affect low-earnings workers, but did affect high-earnings workers, so we use low-earnings workers as our control group and high-earnings workers as our treatment group.\nThe data is included in the wooldridge R package as the injury dataset, and if you install the package, load it with library(wooldridge), and run ?injury in the console, you can see complete details about what’s in it. To give you more practice with loading data from external files, I exported the injury data as a CSV file (using write_csv(injury, \"injury.csv\")) and included it here.\nThese are the main columns we’ll worry about for now:\n durat (which we’ll rename to duration): Duration of unemployment benefits, measured in weeks ldurat (which we’ll rename to log_duration): Logged version of durat (log(durat)) after_1980 (which we’ll rename to after_1980): Indicator variable marking if the observation happened before (0) or after (1) the policy change in 1980. This is our time (or before/after variable) highearn: Indicator variable marking if the observation is a low (0) or high (1) earner. This is our group (or treatment/control) variable   Load and clean data First, let’s download the dataset (if you haven’t already), put in a folder named data, and load it:\n  injury.csv  library(tidyverse) # ggplot(), %\u0026gt;%, mutate(), and friends library(broom) # Convert models to data frames library(scales) # Format numbers with functions like comma(), percent(), and dollar() library(modelsummary) # Create side-by-side regression tables # Load the data. # It\u0026#39;d be a good idea to click on the \u0026quot;injury_raw\u0026quot; object in the Environment # panel in RStudio to see what the data looks like after you load it injury_raw \u0026lt;- read_csv(\u0026quot;data/injury.csv\u0026quot;) Next we’ll clean up the injury_raw data a little to limit the data to just observations from Kentucky. we’ll also rename some awkwardly-named columns:\ninjury \u0026lt;- injury_raw %\u0026gt;% filter(ky == 1) %\u0026gt;% # The syntax for rename is `new_name = original_name` rename(duration = durat, log_duration = ldurat, after_1980 = afchnge)  Exploratory data analysis First we can look at the distribution of unemployment benefits across high and low earners (our control and treatment groups):\nggplot(data = injury, aes(x = duration)) + # binwidth = 8 makes each column represent 2 months (8 weeks) # boundary = 0 make it so the 0-8 bar starts at 0 and isn\u0026#39;t -4 to 4 geom_histogram(binwidth = 8, color = \u0026quot;white\u0026quot;, boundary = 0) + facet_wrap(vars(highearn)) The distribution is really skewed, with most people in both groups getting between 0-8 weeks of benefits (and a handful with more than 180 weeks! that’s 3.5 years!)\nIf we use the log of duration, we can get a less skewed distribution that works better with regression models:\nggplot(data = injury, mapping = aes(x = log_duration)) + geom_histogram(binwidth = 0.5, color = \u0026quot;white\u0026quot;, boundary = 0) + # Uncomment this line if you want to exponentiate the logged values on the # x-axis. Instead of showing 1, 2, 3, etc., it\u0026#39;ll show e^1, e^2, e^3, etc. and # make the labels more human readable # scale_x_continuous(labels = trans_format(\u0026quot;exp\u0026quot;, format = round)) + facet_wrap(vars(highearn)) We should also check the distribution of unemployment before and after the policy change. Copy/paste one of the histogram chunks and change the faceting:\nggplot(data = injury, mapping = aes(x = log_duration)) + geom_histogram(binwidth = 0.5, color = \u0026quot;white\u0026quot;, boundary = 0) + facet_wrap(vars(after_1980)) The distributions look normal-ish, but we can’t really easily see anything different between the before/after and treatment/control groups. We can plot the averages, though. There are a few different ways we can do this.\nYou can use a stat_summary() layer to have ggplot calculate summary statistics like averages. Here we just calculate the mean:\nggplot(injury, aes(x = factor(highearn), y = log_duration)) + geom_point(size = 0.5, alpha = 0.2) + stat_summary(geom = \u0026quot;point\u0026quot;, fun = \u0026quot;mean\u0026quot;, size = 5, color = \u0026quot;red\u0026quot;) + facet_wrap(vars(after_1980)) But we can also calculate the mean and 95% confidence interval:\nggplot(injury, aes(x = factor(highearn), y = log_duration)) + stat_summary(geom = \u0026quot;pointrange\u0026quot;, size = 1, color = \u0026quot;red\u0026quot;, fun.data = \u0026quot;mean_se\u0026quot;, fun.args = list(mult = 1.96)) + facet_wrap(vars(after_1980)) We can already start to see the classical diff-in-diff plot! It looks like high earners after 1980 had longer unemployment on average.\nWe can also use group_by() and summarize() to figure out group means before sending the data to ggplot. I prefer doing this because it gives me more control over the data that I’m plotting:\nplot_data \u0026lt;- injury %\u0026gt;% # Make these categories instead of 0/1 numbers so they look nicer in the plot mutate(highearn = factor(highearn, labels = c(\u0026quot;Low earner\u0026quot;, \u0026quot;High earner\u0026quot;)), after_1980 = factor(after_1980, labels = c(\u0026quot;Before 1980\u0026quot;, \u0026quot;After 1980\u0026quot;))) %\u0026gt;% group_by(highearn, after_1980) %\u0026gt;% summarize(mean_duration = mean(log_duration), se_duration = sd(log_duration) / sqrt(n()), upper = mean_duration + (1.96 * se_duration), lower = mean_duration + (-1.96 * se_duration)) ggplot(plot_data, aes(x = highearn, y = mean_duration)) + geom_pointrange(aes(ymin = lower, ymax = upper), color = \u0026quot;darkgreen\u0026quot;, size = 1) + facet_wrap(vars(after_1980)) Or, plotted in the more standard diff-in-diff format:\nggplot(plot_data, aes(x = after_1980, y = mean_duration, color = highearn)) + geom_pointrange(aes(ymin = lower, ymax = upper), size = 1) + # The group = highearn here makes it so the lines go across categories geom_line(aes(group = highearn))  Diff-in-diff by hand We can find that exact difference by filling out the 2x2 before/after treatment/control table:\n   Before 1980 After 1980 ∆    Low earners A B B − A  High earners C D D − C  ∆ C − A D − B (D − C) − (B − A)    A combination of group_by() and summarize() makes this really easy:\ndiffs \u0026lt;- injury %\u0026gt;% group_by(after_1980, highearn) %\u0026gt;% summarize(mean_duration = mean(log_duration), # Calculate average with regular duration too, just for fun mean_duration_for_humans = mean(duration)) diffs ## # A tibble: 4 x 4 ## # Groups: after_1980 [2] ## after_1980 highearn mean_duration mean_duration_for_humans ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0 0 1.13 6.27 ## 2 0 1 1.38 11.2 ## 3 1 0 1.13 7.04 ## 4 1 1 1.58 12.9 We can pull each of these numbers out of the table with some filter()s and pull():\nbefore_treatment \u0026lt;- diffs %\u0026gt;% filter(after_1980 == 0, highearn == 1) %\u0026gt;% pull(mean_duration) before_control \u0026lt;- diffs %\u0026gt;% filter(after_1980 == 0, highearn == 0) %\u0026gt;% pull(mean_duration) after_treatment \u0026lt;- diffs %\u0026gt;% filter(after_1980 == 1, highearn == 1) %\u0026gt;% pull(mean_duration) after_control \u0026lt;- diffs %\u0026gt;% filter(after_1980 == 1, highearn == 0) %\u0026gt;% pull(mean_duration) diff_treatment_before_after \u0026lt;- after_treatment - before_treatment diff_treatment_before_after ## [1] 0.2 diff_control_before_after \u0026lt;- after_control - before_control diff_control_before_after ## [1] 0.0077 diff_diff \u0026lt;- diff_treatment_before_after - diff_control_before_after diff_diff ## [1] 0.19 The diff-in-diff estimate is 0.19, which means that the program causes an increase in unemployment duration of 0.19 logged weeks. Logged weeks is nonsensical, though, so we have to interpret it with percentages (here’s a handy guide!; this is Example B, where the dependent/outcome variable is logged). Receiving the treatment (i.e. being a high earner after the change in policy) causes a 19% increase in the length of unemployment.\nggplot(diffs, aes(x = as.factor(after_1980), y = mean_duration, color = as.factor(highearn))) + geom_point() + geom_line(aes(group = as.factor(highearn))) + # If you use these lines you\u0026#39;ll get some extra annotation lines and # labels. The annotate() function lets you put stuff on a ggplot that\u0026#39;s not # part of a dataset. Normally with geom_line, geom_point, etc., you have to # plot data that is in columns. With annotate() you can specify your own x and # y values. annotate(geom = \u0026quot;segment\u0026quot;, x = \u0026quot;0\u0026quot;, xend = \u0026quot;1\u0026quot;, y = before_treatment, yend = after_treatment - diff_diff, linetype = \u0026quot;dashed\u0026quot;, color = \u0026quot;grey50\u0026quot;) + annotate(geom = \u0026quot;segment\u0026quot;, x = \u0026quot;1\u0026quot;, xend = \u0026quot;1\u0026quot;, y = after_treatment, yend = after_treatment - diff_diff, linetype = \u0026quot;dotted\u0026quot;, color = \u0026quot;blue\u0026quot;) + annotate(geom = \u0026quot;label\u0026quot;, x = \u0026quot;1\u0026quot;, y = after_treatment - (diff_diff / 2), label = \u0026quot;Program effect\u0026quot;, size = 3) # Here, all the as.factor changes are directly in the ggplot code. I generally # don\u0026#39;t like doing this and prefer to do that separately so there\u0026#39;s less typing # in the ggplot code, like this: # # diffs \u0026lt;- diffs %\u0026gt;% # mutate(after_1980 = as.factor(after_1980), highearn = as.factor(highearn)) # # ggplot(diffs, aes(x = after_1980, y = avg_durat, color = highearn)) + # geom_line(aes(group = highearn))  Diff-in-diff with regression Calculating all the pieces by hand like that is tedious, so we can use regression to do it instead! Remember that we need to include indicator variables for treatment/control and for before/after, as well as the interaction of the two. Here’s what the math equation looks like:\n\\[ \\begin{aligned} \\log(\\text{duration}) = \u0026amp;\\alpha + \\beta \\ \\text{highearn} + \\gamma \\ \\text{after_1980} + \\\\ \u0026amp; \\delta \\ (\\text{highearn} \\times \\text{after_1980}) + \\epsilon \\end{aligned} \\]\nThe \\(\\delta\\) coefficient is the effect we care about in the end—that’s the diff-in-diff estimator.\nmodel_small \u0026lt;- lm(log_duration ~ highearn + after_1980 + highearn * after_1980, data = injury) tidy(model_small) ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 1.13 0.0307 36.6 1.62e-263 ## 2 highearn 0.256 0.0474 5.41 6.72e- 8 ## 3 after_1980 0.00766 0.0447 0.171 8.64e- 1 ## 4 highearn:after_1980 0.191 0.0685 2.78 5.42e- 3 The coefficient for highearn:after_1980 is the same as what we found by hand, as it should be! It is statistically significant, so we can be fairly confident that it is not 0.\n Diff-in-diff with regression + controls One advantage to using regression for diff-in-diff is that we can include control variables to help isolate the effect. For example, perhaps claims made by construction or manufacturing workers tend to have longer duration than claims made workers in other industries. Or maybe those claiming back injuries tend to have longer claims than those claiming head injuries. We might also want to control for worker demographics such as gender, marital status, and age.\nLet’s estimate an expanded version of the basic regression model with the following additional variables:\n male married age hosp (1 = hospitalized) indust (1 = manuf, 2 = construc, 3 = other) injtype (1-8; categories for different types of injury) lprewage (log of wage prior to filing a claim)  Important: indust and injtype are in the dataset as numbers (1-3 and 1-8), but they’re actually categories. We have to tell R to treat them as categories (or factors), otherwise it’ll assume that you can have an injury type of 3.46 or something impossible.\n# Convert industry and injury type to categories/factors injury_fixed \u0026lt;- injury %\u0026gt;% mutate(indust = as.factor(indust), injtype = as.factor(injtype)) model_big \u0026lt;- lm(log_duration ~ highearn + after_1980 + highearn * after_1980 + male + married + age + hosp + indust + injtype + lprewage, data = injury_fixed) tidy(model_big) ## # A tibble: 18 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) -1.53 0.422 -3.62 2.98e- 4 ## 2 highearn -0.152 0.0891 -1.70 8.86e- 2 ## 3 after_1980 0.0495 0.0413 1.20 2.31e- 1 ## 4 male -0.0843 0.0423 -1.99 4.64e- 2 ## 5 married 0.0567 0.0373 1.52 1.29e- 1 ## 6 age 0.00651 0.00134 4.86 1.19e- 6 ## 7 hosp 1.13 0.0370 30.5 5.20e-189 ## 8 indust2 0.184 0.0541 3.40 6.87e- 4 ## 9 indust3 0.163 0.0379 4.32 1.60e- 5 ## 10 injtype2 0.935 0.144 6.51 8.29e- 11 ## 11 injtype3 0.635 0.0854 7.44 1.19e- 13 ## 12 injtype4 0.555 0.0928 5.97 2.49e- 9 ## 13 injtype5 0.641 0.0854 7.51 7.15e- 14 ## 14 injtype6 0.615 0.0863 7.13 1.17e- 12 ## 15 injtype7 0.991 0.191 5.20 2.03e- 7 ## 16 injtype8 0.434 0.119 3.65 2.64e- 4 ## 17 lprewage 0.284 0.0801 3.55 3.83e- 4 ## 18 highearn:after_1980 0.169 0.0640 2.64 8.38e- 3 # Extract just the diff-in-diff estimate diff_diff_controls \u0026lt;- tidy(model_big) %\u0026gt;% filter(term == \u0026quot;highearn:after_1980\u0026quot;) %\u0026gt;% pull(estimate) After controlling for a host of demographic controls, the diff-in-diff estimate is smaller (0.17), indicating that the policy caused a 17% increase in the duration of weeks unemployed following a workplace injury. It is smaller because the other independent variables now explain some of the variation in log_duration.\n Comparison of results We can put the model coefficients side-by-side to compare the value for highearn:after_1980 as we change the model.\nmodelsummary(list(\u0026quot;Simple\u0026quot; = model_small, \u0026quot;Full\u0026quot; = model_big))    Simple  Full      (Intercept)  1.126***  -1.528***     (0.031)  (0.422)    highearn  0.256***  -0.152*     (0.047)  (0.089)    after_1980  0.008  0.050     (0.045)  (0.041)    highearn × after_1980  0.191***  0.169***     (0.069)  (0.064)    male   -0.084**      (0.042)    married   0.057      (0.037)    age   0.007***      (0.001)    hosp   1.130***      (0.037)    indust2   0.184***      (0.054)    indust3   0.163***      (0.038)    injtype2   0.935***      (0.144)    injtype3   0.635***      (0.085)    injtype4   0.555***      (0.093)    injtype5   0.641***      (0.085)    injtype6   0.615***      (0.086)    injtype7   0.991***      (0.191)    injtype8   0.434***      (0.119)    lprewage   0.284***      (0.080)    Num.Obs.  5626  5347    R2  0.021  0.190    R2 Adj.  0.020  0.187       * p \u0026lt; 0.1, ** p \u0026lt; 0.05, *** p \u0026lt; 0.01      ","date":1583280000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"47fcaf150dc645d91ec1f814672dfc8e","permalink":"https://evalf20.classes.andrewheiss.com/example/diff-in-diff/","publishdate":"2020-03-04T00:00:00Z","relpermalink":"/example/diff-in-diff/","section":"example","summary":"Video walk-through Program background Load and clean data Exploratory data analysis Diff-in-diff by hand Diff-in-diff with regression Diff-in-diff with regression + controls Comparison of results   Video walk-through If you want to follow along with this example, you can download the data below:\n  injury.csv  There’s a set of videos that walks through each section below. To make it easier for you to jump around the video examples, I cut the long video into smaller pieces and included them all in one YouTube playlist.","tags":null,"title":"Difference-in-differences","type":"docs"},{"authors":null,"categories":null,"content":"   Basic Markdown formatting Math Tables Footnotes Front matter Citations Other references   Markdown is a special kind of markup language that lets you format text with simple syntax. You can then use a converter program like pandoc to convert Markdown into whatever format you want: HTML, PDF, Word, PowerPoint, etc. (see the full list of output types here)\nBasic Markdown formatting     Type… …or… …to get    Some text in a paragraph. More text in the next paragraph. Always use empty lines between paragraphs.  Some text in a paragraph.\nMore text in the next paragraph. Always use empty lines between paragraphs.\n  *Italic* _Italic_ Italic  **Bold** __Bold__ Bold  # Heading 1  Heading 1   ## Heading 2  Heading 2   ### Heading 3  Heading 3   (Go up to heading level 6 with ######)    [Link text](http://www.example.com)  Link text  ![Image caption](/path/to/image.png)    `Inline code` with backticks  Inline code with backticks  \u0026gt; Blockquote   Blockquote\n  - Things in - an unordered - list * Things in * an unordered * list  Things in an unordered list   1. Things in 2. an ordered 3. list 1) Things in 2) an ordered 3) list Things in an ordered list   Horizontal line --- Horizontal line *** Horizontal line\n     Math Markdown uses LaTeX to create fancy mathematical equations. There are like a billion little options and features available for math equations—you can find helpful examples of the the most common basic commands here.\nYou can use math in two different ways: inline or in a display block. To use math inline, wrap it in single dollar signs, like $y = mx + b$:\n    Type… …to get    Based on the DAG, the regression model for estimating the effect of education on wages is $\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon$, or $\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon$. Based on the DAG, the regression model for estimating the effect of education on wages is \\(\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon\\), or \\(\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon\\).    To put an equation on its own line in a display block, wrap it in double dollar signs, like this:\nType…\nThe quadratic equation was an important part of high school math: $$ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$ But now we just use computers to solve for $x$. …to get…\n The quadratic equation was an important part of high school math:\n\\[ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} \\]\nBut now we just use computers to solve for \\(x\\).\n Because dollar signs are used to indicate math equations, you can’t just use dollar signs like normal if you’re writing about actual dollars. For instance, if you write This book costs $5.75 and this other costs $40, Markdown will treat everything that comes between the dollar signs as math, like so: “This book costs $5.75 and this other costs $40”.\nTo get around that, put a backslash (\\) in front of the dollar signs, so that This book costs \\$5.75 and this other costs \\$40 becomes “This book costs $5.75 and this other costs $40”.\n Tables There are 4 different ways to hand-create tables in Markdown—I say “hand-create” because it’s normally way easier to use R to generate these things with packages like pander (use pandoc.table()) or knitr (use kable()). The two most common are simple tables and pipe tables. You should look at the full documentation here.\nFor simple tables, type…\n Right Left Center Default ------- ------ ---------- ------- 12 12 12 12 123 123 123 123 1 1 1 1 Table: Caption goes here …to get…\n Caption goes here  Right Left Center Default    12 12 12 12  123 123 123 123  1 1 1 1    For pipe tables, type…\n| Right | Left | Default | Center | |------:|:-----|---------|:------:| | 12 | 12 | 12 | 12 | | 123 | 123 | 123 | 123 | | 1 | 1 | 1 | 1 | Table: Caption goes here …to get…\n Caption goes here  Right Left Default Center    12 12 12 12  123 123 123 123  1 1 1 1     Footnotes There are two different ways to add footnotes (see here for complete documentation): regular and inline.\nRegular notes need (1) an identifier and (2) the actual note. The identifier can be whatever you want. Some people like to use numbers like [^1], but if you ever rearrange paragraphs or add notes before #1, the numbering will be wrong (in your Markdown file, not in the output; everything will be correct in the output). Because of that, I prefer to use some sort of text label:\nType…\nHere is a footnote reference[^1] and here is another [^note-on-dags]. [^1]: This is a note. [^note-on-dags]: DAGs are neat. And here\u0026#39;s more of the document. …to get…\n Here is a footnote reference1 and here is another.2\nAnd here’s more of the document.\n  This is a note.↩︎   DAGs are neat.↩︎     You can also use inline footnotes with ^[Text of the note goes here], which are often easier because you don’t need to worry about identifiers:\nType…\nCausal inference is neat.^[But it can be hard too!] …to get…\n Causal inference is neat.1\n  But it can be hard too!↩︎      Front matter You can include a special section at the top of a Markdown document that contains metadata (or data about your document) like the title, date, author, etc. This section uses a special simple syntax named YAML (or “YAML Ain’t Markup Language”) that follows this basic outline: setting: value for setting. Here’s an example YAML metadata section. Note that it must start and end with three dashes (---).\n--- title: Title of your document date: \u0026quot;January 13, 2020\u0026quot; author: \u0026quot;Your name\u0026quot; --- You can put the values inside quotes (like the date and name in the example above), or you can leave them outside of quotes (like the title in the example above). I typically use quotes just to be safe—if the value you’re using has a colon (:) in it, it’ll confuse Markdown since it’ll be something like title: My cool title: a subtitle, which has two colons. It’s better to do this:\n--- title: \u0026quot;My cool title: a subtitle\u0026quot; --- If you want to use quotes inside one of the values (e.g. your document is An evaluation of \"scare quotes\"), you can use single quotes instead:\n--- title: \u0026#39;An evaluation of \u0026quot;scare quotes\u0026quot;\u0026#39; ---  Citations One of the most powerful features of Markdown + pandoc is the ability to automatically cite things and generate bibliographies. to use citations, you need to create a BibTeX file (ends in .bib) that contains a database of the things you want to cite. You can do this with bibliography managers designed to work with BibTeX directly (like BibDesk on macOS), or you can use Zotero (macOS and Windows) to export a .bib file. You can download an example .bib file of all the readings from this class here.\nComplete details for using citations can be found here. In brief, you need to do three things:\nAdd a bibliography: entry to the YAML metadata:\n--- title: Title of your document date: \u0026quot;January 13, 2020\u0026quot; author: \u0026quot;Your name\u0026quot; bibliography: name_of_file.bib --- Choose a citation style based on a CSL file. The default is Chicago author-date, but you can choose from 2,000+ at this repository. Download the CSL file, put it in your project folder, and add an entry to the YAML metadata (or provide a URL to the online version):\n--- title: Title of your document date: \u0026quot;January 13, 2020\u0026quot; author: \u0026quot;Your name\u0026quot; bibliography: name_of_file.bib csl: \u0026quot;https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl\u0026quot; --- Some of the most common CSLs are:\n Chicago author-date Chicago note-bibliography Chicago full note-bibliography (no shortened notes or ibids) APA 7th edition MLA 8th edition  Cite things in your document. Check the documentation for full details of how to do this. Essentially, you use @citationkey inside square brackets ([]):\n    Type… …to get…    Causal inference is neat [@Rohrer:2018; @AngristPischke:2015]. Causal inference is neat (Rohrer 2018; Angrist and Pischke 2015).  Causal inference is neat [see @Rohrer:2018, p. 34; also @AngristPischke:2015, chapter 1]. Causal inference is neat (see Rohrer 2018, 34; also Angrist and Pischke 2015, chap. 1).  Angrist and Pischke say causal inference is neat [-@AngristPischke:2015; see also @Rohrer:2018]. Angrist and Pischke say causal inference is neat (2015; see also Rohrer 2018).  @AngristPischke:2015 [chapter 1] say causal inference is neat, and @Rohrer:2018 agrees. Angrist and Pischke (2015, chap. 1) say causal inference is neat, and Rohrer (2018) agrees.    After compiling, you should have a perfectly formatted bibliography added to the end of your document too:\n Angrist, Joshua D., and Jörn-Steffen Pischke. 2015. Mastering ’Metrics: The Path from Cause to Effect. Princeton, NJ: Princeton University Press.\nRohrer, Julia M. 2018. “Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1 (1): 27–42. https://doi.org/10.1177/2515245917745629.\n   Other references These websites have additional details and examples and practice tools:\n CommonMark’s Markdown tutorial: A quick interactive Markdown tutorial. Markdown tutorial: Another interactive tutorial to practice using Markdown. Markdown cheatsheet: Useful one-page reminder of Markdown syntax. The Plain Person’s Guide to Plain Text Social Science: A comprehensive explanation and tutorial about why you should write data-based reports in Markdown.   ","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"dcf6a5ae191a1cca4f4c8ff8ac114538","permalink":"https://evalf20.classes.andrewheiss.com/resource/markdown/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/resource/markdown/","section":"resource","summary":"Basic Markdown formatting Math Tables Footnotes Front matter Citations Other references   Markdown is a special kind of markup language that lets you format text with simple syntax. You can then use a converter program like pandoc to convert Markdown into whatever format you want: HTML, PDF, Word, PowerPoint, etc. (see the full list of output types here)\nBasic Markdown formatting     Type… …or… …to get    Some text in a paragraph.","tags":null,"title":"Using Markdown","type":"docs"},{"authors":null,"categories":null,"content":"   Key terms Add chunks Chunk names Chunk options Inline chunks Output formats   R Markdown is regular Markdown with R code and output sprinkled in. You can do everything you can with regular Markdown, but you can incorporate graphs, tables, and other R output directly in your document. You can create HTML, PDF, and Word documents, PowerPoint and HTML presentations, websites, books, and even interactive dashboards with R Markdown. This whole course website is created with R Markdown (and a package named blogdown).\nThe documentation for R Markdown is extremely comprehensive, and their tutorials and cheatsheets are excellent—rely on those.\nHere are the most important things you’ll need to know about R Markdown in this class:\nKey terms  Document: A Markdown file where you type stuff\n Chunk: A piece of R code that is included in your document. It looks like this:\n```{r} # Code goes here ``` There must be an empty line before and after the chunk. The final three backticks must be the only thing on the line—if you add more text, or if you forget to add the backticks, or accidentally delete the backticks, your document will not knit correctly.\n Knit: When you “knit” a document, R runs each of the chunks sequentially and converts the output of each chunk into Markdown. R then runs the knitted document through pandoc to convert it to HTML or PDF or Word (or whatever output you’ve selected).\nYou can knit by clicking on the “Knit” button at the top of the editor window, or by pressing ⌘⇧K on macOS or control + shift + K on Windows.\n   Add chunks There are three ways to insert chunks:\n Press ⌘⌥I on macOS or control + alt + I on Windows\n Click on the “Insert” button at the top of the editor window\n Manually type all the backticks and curly braces (don’t do this)\n   Chunk names You can add names to chunks to make it easier to navigate your document. If you click on the little dropdown menu at the bottom of your editor in RStudio, you can see a table of contents that shows all the headings and chunks. If you name chunks, they’ll appear in the list. If you don’t include a name, the chunk will still show up, but you won’t know what it does.\nTo add a name, include it immediately after the {r in the first line of the chunk. Names cannot contain spaces, but they can contain underscores and dashes. All chunk names in your document must be unique.\n```{r name-of-this-chunk} # Code goes here ```  Chunk options There are a bunch of different options you can set for each chunk. You can see a complete list in the RMarkdown Reference Guide or at knitr’s website.\nOptions go inside the {r} section of the chunk:\n```{r name-of-this-chunk, warning=FALSE, message=FALSE} # Code goes here ``` The most common chunk options are these:\n fig.width=5 and fig.height=3 (or whatever number you want): Set the dimensions for figures echo=FALSE: The code is not shown in the final document, but the results are message=FALSE: Any messages that R generates (like all the notes that appear after you load a package) are omitted warning=FALSE: Any warnings that R generates are omitted include=FALSE: The chunk still runs, but the code and results are not included in the final document  You can also set chunk options by clicking on the little gear icon in the top right corner of any chunk:\n Inline chunks You can also include R output directly in your text, which is really helpful if you want to report numbers from your analysis. To do this, use `r r_code_here`.\nIt’s generally easiest to calculate numbers in a regular chunk beforehand and then use an inline chunk to display the value in your text. For instance, this document…\n```{r find-avg-mpg, echo=FALSE} avg_mpg \u0026lt;- mean(mtcars$mpg) ``` The average fuel efficiency for cars from 1974 was `r round(avg_mpg, 1)` miles per gallon. … would knit into this:\n The average fuel efficiency for cars from 1974 was 20.1 miles per gallon.\n  Output formats You can specify what kind of document you create when you knit in the YAML front matter.\ntitle: \u0026quot;My document\u0026quot; output: html_document: default pdf_document: default word_document: default You can also click on the down arrow on the “Knit” button to choose the output and generate the appropriate YAML. If you click on the gear icon next to the “Knit” button and choose “Output options”, you change settings for each specific output type, like default figure dimensions or whether or not a table of contents is included.\nThe first output type listed under output: will be what is generated when you click on the “Knit” button or press the keyboard shortcut (⌘⇧K on macOS; control + shift + K on Windows). If you choose a different output with the “Knit” button menu, that output will be moved to the top of the output section.\nThe indentation of the YAML section matters, especially when you have settings nested under each output type. Here’s what a typical output section might look like:\n--- title: \u0026quot;My document\u0026quot; author: \u0026quot;My name\u0026quot; date: \u0026quot;January 13, 2020\u0026quot; output: html_document: toc: yes fig_caption: yes fig_height: 8 fig_width: 10 pdf_document: latex_engine: xelatex # More modern PDF typesetting engine toc: yes word_document: toc: yes fig_caption: yes fig_height: 4 fig_width: 5 ---  ","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"00c0b36df90b91640842af65d1311657","permalink":"https://evalf20.classes.andrewheiss.com/resource/rmarkdown/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/resource/rmarkdown/","section":"resource","summary":"Key terms Add chunks Chunk names Chunk options Inline chunks Output formats   R Markdown is regular Markdown with R code and output sprinkled in. You can do everything you can with regular Markdown, but you can incorporate graphs, tables, and other R output directly in your document. You can create HTML, PDF, and Word documents, PowerPoint and HTML presentations, websites, books, and even interactive dashboards with R Markdown.","tags":null,"title":"Using R Markdown","type":"docs"},{"authors":null,"categories":null,"content":"  Here’s your roadmap for the semester!\n Content (): This page contains the readings, slides, and recorded lectures for the topic. Read and watch these first. Example (): This page contains fully annotated R code and other supplementary information that you can use as a reference for your assignments and project. This is only a reference page—you don’t have to necessarily do anything here. Some sections also contain videos of me live coding the examples so you can see what it looks like to work with R in real time. This page will be very helpful as you work on your assignments. Assignment (): This page contains the instructions for each assignment. Assignments are due by 11:59 PM on the day they’re listed.    Evaluation and causation Content Example Assignment  August 24–28\n(Session 1) Evaluation and the causal revolution      August 31 Weekly check-in 1       August 31 Problem set 1       August 31–September 4\n(Session 2) Regression and inference       September 7 Weekly check-in 2       September 7 Problem set 2       September 7–11\n(Session 3) Theories of change and logic models       September 14 Weekly check-in 3       September 14 Evaluation: Background and theory       September 14–18\n(Session 4) Measurement and DAGs      September 21 Weekly check-in 4       September 21 Evaluation: Measurement       September 21–25\n(Session 5) DAGs and potential outcomes      September 28 Weekly check-in 5       September 28 Evaluation: Causal model       September 28–October 2\n(Session 6) Threats to validity       October 5 Weekly check-in 6       October 5 Evaluation: Threats to validity       October 5–10  Exam 1         Tools and methods Content Example Assignment  October 5–9\n(Session 7) Randomization and matching      October 5 Georgia voter registration deadline        October 12 Weekly check-in 7       October 12 Problem set 3       October 12–23\n(Sessions 8–9) Difference-in-differences I + II      October 19 Weekly check-in 8       October 19 Problem set 4       October 26 Weekly check-in 9       October 26 Problem set 5       October 26–30\n(Session 10) Regression discontinuity I      October 30 Deadline to request an absentee ballot (request must be received by this day)        November 2 Weekly check-in 10       November 2 Problem set 6       November 2–6\n(Session 11) Instrumental variables I      November 3 Election Day        November 9 Weekly check-in 11       November 9 Problem set 7       November 9–13\n(Session 12) Instrumental variables II + Regression discontinuity II      November 16 Weekly check-in 12       November 16 Problem set 8       November 16–30  Exam 2         Applied evaluation Content Example Assignment  November 16–20\n(Session 13) Choosing and planning ethical evaluations      November 30 Weekly check-in 13       November 30 Problem set 9       November 30–December 4\n(Session 14) Ethics, stories, and curiosity       December 7 Weekly check-in 14       December 7 Work session         Final Content Example Assignment  December 14  Final project due (submit online by 7:00 PM)         ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"3e223d7ba58b0122b42458e4cf52e04c","permalink":"https://evalf20.classes.andrewheiss.com/schedule/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/schedule/","section":"","summary":"Here’s your roadmap for the semester!\n Content (): This page contains the readings, slides, and recorded lectures for the topic. Read and watch these first. Example (): This page contains fully annotated R code and other supplementary information that you can use as a reference for your assignments and project. This is only a reference page—you don’t have to necessarily do anything here. Some sections also contain videos of me live coding the examples so you can see what it looks like to work with R in real time.","tags":null,"title":"Schedule","type":"page"},{"authors":null,"categories":null,"content":"   Course objectives Course philosophy Important pep talk! Course materials  Books Articles, book chapters, and other materials R and RStudio Online help  Course policies  Student hours Learning during a pandemic Late work Counseling and Psychological Services (CPS) Basic needs security Lauren’s Promise Academic honesty Special needs  Assignments and grades Star Wars   Instructor  Dr. Andrew Heiss  357 Andrew Young School  aheiss@gsu.edu  @andrewheiss  Schedule an appointment   Course details  Any day  August 24–December 15, 2020  Any time  Slack   Contacting me E-mail and Slack are the best ways to get in contact with me. I will try to respond to all course-related e-mails and Slack messages within 24 hours (really), but also remember that life can be busy and chaotic for everyone (including me!), so if I don\u0026rsquo;t respond right away, don\u0026rsquo;t worry!\n  Course objectives By the end of this course, you (1) will be literate in the language of causal inference, (2) will communicate evaluation outcomes clearly, and (3) will understand the ethics and limits of data analysis by designing, critiquing, coding, and running rigorous, valid, and feasible evaluations of public sector programs focused on society’s most pressing problems.\nSpecifically, you’ll be able to:\n Explain the philosophy of causation Identify and diagram program logic models Outline theories of change with directed acyclic graphs (DAGs) Summarize key threats to causal inference, identify these threats in evaluations, and mitigate these threats with research design Develop rigorous and valid statistical measures Run statistical models Explain the theory, research design, methods, and results of evaluations to all types of stakeholders, from highly trained econometricians to the general public Share your analyses and data with the public Identify ethical issues and limits in data science and program evaluation Become curious and confident in consuming and producing evaluations   Course philosophy Classical statistics classes spend substantial time covering probability theory, null hypothesis testing, and other statistical tests first developed hundreds of years ago. Some classes don’t use software or actual real data and instead live in the world of mathematical proofs. They can be math-heavy and full of often unintuitive concepts and equations.\nIn this class, we will take the opposite approach. We begin with data and learn how to tidy, wrangle, manipulate, and visualize it with code. Later in the semester we’ll turn to the powerful toolbox of causal inference approaches, but continue to keep the focus on data as we do so.\nIn other words, there’s way less of this:\n\\[ f(x) = \\dfrac{1}{\\sqrt{2\\pi}} e^{-\\frac12 x^2} \\]\nAnd way more of this:\nsummary_monthly_temp \u0026lt;- weather %\u0026gt;% group_by(month) %\u0026gt;% summarize(mean = mean(temp), std_dev = sd(temp)) Over the last decade there has been a revolution in statistical and scientific computing. Open source languages like R and Python have overtaken older (and expensive!) corporate software packages like SAS and SPSS, and there are now thousands of books and blog posts and other online resources with excellent tutorials about how to analyze pretty much any kind of data.\nThis class will expose you to R—one of the most popular, sought-after, and in-demand statistical programming languages. Armed with the foundation of R skills you’ll learn in this class, you’ll know enough to be able to find how to analyze any sort of data-based question in the future.\n Important pep talk! I promise you can succeed in this class.\nLearning R can be difficult at first—it’s like learning a new language, just like Spanish, French, or Chinese. Hadley Wickham—the chief data scientist at RStudio and the author of some amazing R packages you’ll be using like ggplot2—made this wise observation:\n It’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n Even experienced programmers and evaluators find themselves bashing their heads against seemingly intractable errors. If you’re finding yourself taking way too long hitting your head against a wall and not understanding, take a break, talk to classmates, e-mail me, etc.\n\n Course materials Most of the readings in this class are free.\nBooks We will only use one physical textbook. There are three official textbooks for the class:\n Paul J. Gertler et al., Impact Evaluation in Practice, 2nd ed. (Inter-American Development Bank; World Bank, 2016), https://openknowledge.worldbank.org/handle/10986/25030. (Free!) Scott Cunningham, Causal Inference: The Mixtape, 2018, https://www.scunning.com/mixtape.html. (Also free!) Joshua D. Angrist and Jörn-Steffen Pischke, Mastering ’Metrics: The Path from Cause to Effect (Princeton, NJ: Princeton University Press, 2015). ($25 used or $30 new at Amazon)  The World Bank’s Impact Evaluation in Practice will be our main textbook. It’s written at a general, easy-to-understand level with relatively minimal math. Mastering ’Metrics goes into more depth about the mechanics of different causal inference approaches and has a bit more math, but it’s still fairly accessible. Causal Inference: The Mixtape has even more math, but hopefully not an excessively terrifying amount.\nYou do not need to understand all the equations and notation. If your eyes start to gloss over the Greek letters and subscripts, it’s okay. Try to learn them, but don’t stress out about it too much.\n Articles, book chapters, and other materials There will also occasionally be additional articles and videos to read and watch. When this happens, links to these other resources will be included on the reading page for that week.\n R and RStudio You will do all of your analysis with the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code.\nR is free, but it can sometimes be a pain to install and configure. To make life easier, you can (and should!) use the free RStudio.cloud service, which lets you run a full instance of RStudio in your web browser. This means you won’t have to install anything on your computer to get started with R! We will have a shared class workspace in RStudio.cloud that will let you quickly copy templates for labs and problem sets.\nRStudio.cloud is convenient, but it can be slow and it is not designed to be able to handle larger datasets or more complicated analysis. Over the course of the semester, you’ll probably want to get around to installing R, RStudio, and other R packages on your computer and wean yourself off of RStudio.cloud. This isn’t necessary, but it’s helpful.\nYou can find instructions for installing R, RStudio, and all the tidyverse packages here.\n Online help Data science and statistical programming can be difficult. Computers are stupid and little errors in your code can cause hours of headache (even if you’ve been doing this stuff for years!).\nFortunately there are tons of online resources to help you with this. Two of the most important are StackOverflow (a Q\u0026amp;A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)).\nIf you use Twitter, post R-related questions and content with #rstats. The community there is exceptionally generous and helpful.\nSearching for help with R on Google can sometimes be tricky because the program name is, um, a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”).\nAdditionally, we have a class chatroom at Slack where anyone in the class can ask questions and anyone can answer. I will monitor Slack regularly and will respond quickly. (It’s one of the rare Slack workspaces where I actually have notifications enabled!) Ask questions about the readings, assignments, and project. You’ll likely have similar questions as your peers, and you’ll likely be able to answer other peoples’ questions too.\n  Course policies Be nice. Be honest. Don’t cheat.\nWe will also follow Georgia State’s Code of Conduct.\nThis syllabus reflects a plan for the semester. Deviations may become necessary as the semester progresses.\nStudent hours Please watch this video:\n Student hours are set times dedicated to all of you (most professors call these “office hours”; I don’t1). This means that I will be in my office at home (wistfully) waiting for you to come by talk to me remotely with whatever questions you have. This is the best and easiest way to find me and the best chance for discussing class material and concerns.\nBecause of the pandemic, we cannot meet in person. I can meet you online via Webex. Make an appointment with me here, and then use this link to talk to me during student hours: https://gsumeetings.webex.com/meet/aheiss. You can also find me through e-mail and Slack.\n Learning during a pandemic Life absolutely sucks right now. None of us is really okay. We’re all just pretending.\nYou most likely know people who have lost their jobs, have tested positive for COVID-19, have been hospitalized, or perhaps have even died. You all have increased (or possibly decreased) work responsibilities and increased family care responsibilities—you might be caring for extra people (young and/or old!) right now, and you are likely facing uncertain job prospects (or have been laid off!).\nI’m fully committed to making sure that you learn everything you were hoping to learn from this class! I will make whatever accommodations I can to help you finish your exercises, do well on your projects, and learn and understand the class material. Under ordinary conditions, I am flexible and lenient with grading and course expectations when students face difficult challenges. Under pandemic conditions, that flexibility and leniency is intensified.\nIf you tell me you’re having trouble, I will not judge you or think less of you. I hope you’ll extend me the same grace.\nYou never owe me personal information about your health (mental or physical). You are always welcome to talk to me about things that you’re going through, though. If I can’t help you, I usually know somebody who can.\nIf you need extra help, or if you need more time with something, or if you feel like you’re behind or not understanding everything, do not suffer in silence! Talk to me! I will work with you. I promise.\nPlease sign up for a time to meet with me during student hours at https://calendly.com/andrewheiss/. I’m also available through e-mail and Slack. I’ve enabled notifications on my Slack account, so I’ll see your messages quickly!\nI want you to learn lots of things from this class, but I primarily want you to stay healthy, balanced, and grounded during this crisis.\n Late work You will lose 1 point per day for each day a problem set is late. This is designed to not be a huge penalty (3 days late = 27/30 points on a problem set that gets a ✓), but instead is a commitment device to help you stay on schedule.\nAfter 1 week, I will send an automated reminder e-mail. After 2 weeks, you will receive no points.\n Counseling and Psychological Services (CPS) Life at GSU can be complicated and challenging (especially during a pandemic!). You might feel overwhelmed, experience anxiety or depression, or struggle with relationships or family responsibilities. Counseling and Psychological Services (CPS) provides free, confidential support for students who are struggling with mental health and emotional challenges. The CPS office is staffed by professional psychologists who are attuned to the needs of all types of college and professional students. Please do not hesitate to contact CPS for assistance—getting help is a smart and courageous thing to do.\n Basic needs security If you have difficulty affording groceries or accessing sufficient food to eat every day, or if you lack a safe and stable place to live, and you believe this may affect your performance in this course, please contact the Dean of Students for support. They can provide a host of services including free groceries from the Panther Pantry and assisting with homelessness with the Embark Network. Additionally, please talk to me if you are comfortable in doing so. This will enable me to provide any resources that I might possess.\n Lauren’s Promise I will listen and believe you if someone is threatening you.\nLauren McCluskey, a 21-year-old honors student athlete, was murdered on October 22, 2018 by a man she briefly dated on the University of Utah campus. We must all take action to ensure that this never happens again.\nIf you are in immediate danger, call 911 or GSU police (404-413-3333).\nIf you are experiencing sexual assault, domestic violence, or stalking, please report it to me and I will connect you to resources or call GSU’s Counseling and Psychological Services (404-413-1640).\nAny form of sexual harassment or violence will not be excused or tolerated at Georgia State. GSU has instituted procedures to respond to violations of these laws and standards, programs aimed at the prevention of such conduct, and intervention on behalf of the victims. Georgia State University Police officers will treat victims of sexual assault, domestic violence, and stalking with respect and dignity. Advocates on campus and in the community can help with victims’ physical and emotional health, reporting options, and academic concerns.\n Academic honesty Violation of GSU’s Policy on Academic Honesty will result in an F in the course and possible disciplinary action.2 All violations will be formally reported to the Dean of Students.\n Special needs Students who wish to request accommodation for a disability may do so by registering with the Office of Disability Services. Students may only be accommodated upon issuance by the Office of Disability Services of a signed Accommodation Plan and are responsible for providing a copy of that plan to instructors of all classes in which accommodations are sought.\nStudents with special needs should then make an appointment with me during the first week of class to discuss any accommodations that need to be made.\n  Assignments and grades You can find descriptions for all the assignments on the assignments page.\n   Assignment Points Percent    Weekly check-ins (14 × 10) 140 15.6%  Problem sets (8 × 30) 240 26.7%  Evaluation assignments (4 × 30) 120 13.3%  Exam 1 100 11.1%  Exam 2 100 11.1%  Final project 200 22.2%  Total 900 —        Grade Range Grade Range    A 93–100% C 73–76%  A− 90–92% C− 70–72%  B+ 87–89% D+ 67–69%  B 83–86% D 63–66%  B− 80–82% D− 60–62%  C+ 77–79% F \u0026lt; 60%      Star Wars Once you have read this entire syllabus and the assignments page, please click here and e-mail me a picture of a cute Star Wars character.3 Brownie points if it’s animated.\n   There’s fairly widespread misunderstanding about what office hours actually are! Many students often think that they are the times I shouldn’t be disturbed, which is the exact opposite of what they’re for!↩︎\n So seriously, just don’t cheat or plagiarize!↩︎\n Baby Yoda, Babu Frik, porgs, etc. are all super fair game.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"e4d5a4a79239f08c6ad0d7cbf1be756c","permalink":"https://evalf20.classes.andrewheiss.com/syllabus/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/syllabus/","section":"","summary":"Course objectives Course philosophy Important pep talk! Course materials  Books Articles, book chapters, and other materials R and RStudio Online help  Course policies  Student hours Learning during a pandemic Late work Counseling and Psychological Services (CPS) Basic needs security Lauren’s Promise Academic honesty Special needs  Assignments and grades Star Wars   Instructor  Dr. Andrew Heiss  357 Andrew Young School  aheiss@gsu.","tags":null,"title":"Syllabus","type":"page"},{"authors":null,"categories":null,"content":"  Every week, after you finish working through the content, I want to hear about what you learned and what questions you still have. To facilitate this, and to encourage engagement with the course content, you’ll need to fill out a short response on iCollege. This should be ≈150 words. That’s fairly short: there are ≈250 words on a typical double-spaced page in Microsoft Word (500 when single-spaced).\nYou should answer these two questions each week:\nWhat was the most exciting thing you learned from the session? Why? What was the muddiest thing from the session this week? What are you still wondering about?  I will grade these check-ins using a check system:\n ✔+: (11.5 points (115%) in gradebook) Response shows phenomenal thought and engagement with the course content. I will not assign these often. ✔: (10 points (100%) in gradebook) Response is thoughtful, well-written, and shows engagement with the course content. This is the expected level of performance. ✔−: (5 points (50%) in gradebook) Response is hastily composed, too short, and/or only cursorily engages with the course content. This grade signals that you need to improve next time. I will hopefully not assign these often.  Notice that is essentially a pass/fail or completion-based system. I’m not grading your writing ability, I’m not counting the exact number of words you’re writing, and I’m not looking for encyclopedic citations of every single reading to prove that you did indeed read everything. I’m looking for thoughtful engagement, that’s all. Do good work and you’ll get a ✓.\nYou will submit these responses via iCollege.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606601144,"objectID":"aeaeec0a4be2063279a5200875737f34","permalink":"https://evalf20.classes.andrewheiss.com/assignment/weekly-check-in/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/weekly-check-in/","section":"assignment","summary":"Every week, after you finish working through the content, I want to hear about what you learned and what questions you still have. To facilitate this, and to encourage engagement with the course content, you’ll need to fill out a short response on iCollege. This should be ≈150 words. That’s fairly short: there are ≈250 words on a typical double-spaced page in Microsoft Word (500 when single-spaced).\nYou should answer these two questions each week:","tags":null,"title":"Weekly check-in","type":"docs"}]