[{"authors":["rjbeecham"],"categories":null,"content":"Roger Beecham is a Lecturer in Geography at University of Leeds.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en-uk","lastmod":1606733643,"objectID":"c292ffbb3141300ebc61798f6fdba165","permalink":"/authors/rjbeecham/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/rjbeecham/","section":"authors","summary":"Roger Beecham is a Lecturer in Geography at University of Leeds.","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"  The homework for each session are posted here. Work your way through the links on the left to complete the homework associated with each session.\n","date":1622505600,"expirydate":-62135596800,"kind":"section","lang":"en-uk","lastmod":1622505600,"objectID":"996950346c3c0da2fd661f739a1abdfd","permalink":"/homework/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/homework/","section":"homework","summary":"The homework for each session are posted here. Work your way through the links on the left to complete the homework associated with each session.","tags":null,"title":"Homework for sessions","type":"docs"},{"authors":null,"categories":null,"content":"  The materials for each session are posted here. Work your way through the links on the left and complete the associated homeworks.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en-uk","lastmod":1606644925,"objectID":"108da05078d325a5a1f01a1ff2583053","permalink":"/class/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/class/","section":"class","summary":"The materials for each session are posted here. Work your way through the links on the left and complete the associated homeworks.","tags":null,"title":"Class sessions","type":"docs"},{"authors":null,"categories":null,"content":"  The reading for each session is posted here. Work your way through the links on the left to locate the reading associated with each session.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en-uk","lastmod":1606644925,"objectID":"40fcd2da3bf2dc718a2fe044c31cdc56","permalink":"/reading/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/reading/","section":"reading","summary":"The reading for each session is posted here. Work your way through the links on the left to locate the reading associated with each session.","tags":null,"title":"Reading for sessions","type":"docs"},{"authors":null,"categories":null,"content":"  Road safety analysis : https://docs.ropensci.org/stats19/\n  Matt Kay – OpenVis talk     Uncertainty vis book (not written but structure useful) : https://mjskay.github.io/uncertainty-vis-book/\nhttps://medium.com/multiple-views-visualization-research-explained/uncertainty-visualization-explained-67e7a73f031b\n","date":1624924800,"expirydate":-62135596800,"kind":"page","lang":"en-uk","lastmod":1624924800,"objectID":"a696c93d764e2bfb447300660fcf2be7","permalink":"/class/08-class/","publishdate":"2021-06-29T00:00:00Z","relpermalink":"/class/08-class/","section":"class","summary":"Road safety analysis : https://docs.ropensci.org/stats19/\n  Matt Kay – OpenVis talk     Uncertainty vis book (not written but structure useful) : https://mjskay.github.io/uncertainty-vis-book/\nhttps://medium.com/multiple-views-visualization-research-explained/uncertainty-visualization-explained-67e7a73f031b","tags":null,"title":"Visualization for uncertainty analysis","type":"docs"},{"authors":null,"categories":null,"content":"  Contents  Concepts Techniques   Concepts Reading : https://www.tandfonline.com/doi/full/10.1080/00087041.2019.1633103?src=recsys\nIntroducing this :\nThat in academic visualization there has been a realisation of role of storytelling in visualization. Not a single, optimum solution to any visualization problem that expose the true picture/story – no such formulation exists. At same time: statistical literacy, scientific communication and data journalism –\u0026gt; evidence-based society. data is now embedded in most domains, and particularly in the public sphere. Storytelling not about making up stories – but constructing evidence-based arguments, with authority and uncertainty. Point at tensions – need to reduce complexity (and uncertainty) can lead to false interpretation.  From Data Driven Storytelling\n*We think this movement towards data-driven stories, which is apparent in both the data visualization research community and the professional journalism community, has the potential to form a crucial part of keeping the public informed, a movement sometimes referred to as the democratization of data – the making of data understandable to the general public. This exciting new development in the use of data visualization in media has revealed an emerging professional community in the already complex group of disciplines involved in data visualization. Data visualization has roots in many research fields including perception, computer graphics, design, and human-computer interaction, though only recently has this expanded to include journalism.\nData journalism here has also been a growing consciousness that some of today’s most relevant stories are buried in data. This data can be quite hard to understand in its raw formats but can become much more generally accessible when visualized. Journalists have not only begun to use standard data visualizations such as charts and maps in their stories, but are also creating new ones that are tailored to the particular data type and to the message of the story they are writing. Since journalists are now able to easily share interactive data visualizations on the Web, the democratization of data visualization is accelerating with new compelling data visualizations emerging in the media daily…\nBy carefully structuring the information and integrating explanation to guide the consumer, journalists help lead readers towards a valid interpretation of the underlying data. *\n  Amanda Cox - https://www.youtube.com/embed/ha9LA3rYD9g Robert Kosara - https://www.youtube.com/watch?v=PMtWFjjVM5E\u0026amp;amp;ab_channel=BocoupLLC   Scrollytelling : https://medium.com/nightingale/from-storytelling-to-scrollytelling-a-short-introduction-and-beyond-fbda32066964\nStorytelling models?\nStorytelling narratives – immigration : https://openaccess.city.ac.uk/id/eprint/25322/\nNot sure about this resource : https://datajournalism.com/read/handbook/one/delivering-data/using-visualizations-to-tell-stories\nTalk: John Burn-Murdoch VIS 2020 on Covid-19 comparison work. Themes – theoretical and cognitive optimal ways of displaying data. No canonical solution exists. And the objectives/purposes are different when communicating. Log-scales (focus on comparison).\nAnimated rescaling : https://twitter.com/jwoLondon/status/1347208824495726592\nTwitter stories (for data vis) : anything on this? examples?\nPractical : avoid legends, make labels intrinsic to vis. Sacrifice data density? Build sequentially towards complexity. Design consistency.\n Techniques Covid-19 data – comparing regions? Stats19 data – cycling in London?\n ","date":1624924800,"expirydate":-62135596800,"kind":"page","lang":"en-uk","lastmod":1624924800,"objectID":"6fce0ded8aa03d77081bcdf32570ef18","permalink":"/class/09-class/","publishdate":"2021-06-29T00:00:00Z","relpermalink":"/class/09-class/","section":"class","summary":"Contents  Concepts Techniques   Concepts Reading : https://www.tandfonline.com/doi/full/10.1080/00087041.2019.1633103?src=recsys\nIntroducing this :\nThat in academic visualization there has been a realisation of role of storytelling in visualization. Not a single, optimum solution to any visualization problem that expose the true picture/story – no such formulation exists. At same time: statistical literacy, scientific communication and data journalism –\u0026gt; evidence-based society. data is now embedded in most domains, and particularly in the public sphere.","tags":null,"title":"Visualization for data storytelling","type":"docs"},{"authors":null,"categories":null,"content":"  Multi-level modelling : https://bookdown.org/roback/bookdown-BeyondMLR/\nAlso : http://ljwolf.org/teaching/gds/02-workbook.html\nIntroduce Ocam’s Razor on variable selection?\nPartial pooling : https://solomonkurz.netlify.app/post/stein-s-paradox-and-what-partial-pooling-can-do-for-you/\n","date":1624320000,"expirydate":-62135596800,"kind":"page","lang":"en-uk","lastmod":1624320000,"objectID":"026700f4d4068083b37f98f89edf4682","permalink":"/class/07-class/","publishdate":"2021-06-22T00:00:00Z","relpermalink":"/class/07-class/","section":"class","summary":"Multi-level modelling : https://bookdown.org/roback/bookdown-BeyondMLR/\nAlso : http://ljwolf.org/teaching/gds/02-workbook.html\nIntroduce Ocam’s Razor on variable selection?\nPartial pooling : https://solomonkurz.netlify.app/post/stein-s-paradox-and-what-partial-pooling-can-do-for-you/","tags":null,"title":"Visualization for model building 2: Expose, estimate, evaluate","type":"docs"},{"authors":null,"categories":null,"content":"  Brexit analysis : https://github.com/ropensci/nomisr Trump analysis : https://walker-data.com/tidycensus/\nQuote from Spiegelhalter? About explanation of variation in context of what remains unexplained\n Models as functions Vocabulary: outcome variable (whose variation you are trying to understand), explanatory (other variables that you want to use to explain variation), predicted value (output o the model function – expected outcome conditioing on explanatory variables), residuals (a measure of how far each case is from the predicted value)  Why models and not graphics * Can reveal patterns that not evident in graphics – in regression framework we can condition on explanatory variables and talk about the effect of single variables net of others. * Deal with uncertainty and false discovery – we infer/overinterpet from scatterplots\n https://wilkelab.org/SDS375/ https://mjskay.github.io/ggdist/articles/freq-uncertainty-vis.html  Use tidymodels : https://cfss.uchicago.edu/notes/start-with-models/\n","date":1623715200,"expirydate":-62135596800,"kind":"page","lang":"en-uk","lastmod":1623715200,"objectID":"119022d4a0cb3ad45bc292e062e9e05a","permalink":"/class/06-class/","publishdate":"2021-06-15T00:00:00Z","relpermalink":"/class/06-class/","section":"class","summary":"Brexit analysis : https://github.com/ropensci/nomisr Trump analysis : https://walker-data.com/tidycensus/\nQuote from Spiegelhalter? About explanation of variation in context of what remains unexplained\n Models as functions Vocabulary: outcome variable (whose variation you are trying to understand), explanatory (other variables that you want to use to explain variation), predicted value (output o the model function – expected outcome conditioing on explanatory variables), residuals (a measure of how far each case is from the predicted value)  Why models and not graphics * Can reveal patterns that not evident in graphics – in regression framework we can condition on explanatory variables and talk about the effect of single variables net of others.","tags":null,"title":"Visualization for model building 1: Expose, estimate, evaluate","type":"docs"},{"authors":null,"categories":null,"content":"   Contents  Introduction Concepts Network data: nodes and edges Node-link representations Matrix representations OD maps  Techniques Import Analyse over nodes Analyse over edges  References   By the end of this session you should gain the following knowledge:\n       By the end of this session you should gain the following practical skills:\n       Introduction Networks are a special class of data used to represent things (entities) and how they relate to one another. Network data consist of two types of element: nodes, the entities themselves, and edges, the connections between nodes. Both nodes and edges can have additional information attached to them – counts, categories and directions. Network data are cumbersome to work with in R as they are not represented well by flat data frames. This is being improved by packages like tidygraph, but a common workflow is to split the data across two tables – one representing nodes and one representing edges (Wickham, Navarro, and Pedersen 2020).\nA category of network data used heavily in geospatial analysis is origin-destination (OD) data describing, for example, flows of bikes (Beecham and Wood 2014) and commuters (Beecham and Slingsby 2019) around a city, or water around a country. These data consist of nodes, origin and destination locations, and edges, flows linking those origins and destinations. Whilst statistics common to Network Science can and have been deployed in the analysis of geospatial OD data, visualization techniques provide much assistance in exposing the types of complex structural patterns and relations that result from locating OD flow data within geographic context.\nIn this session we will work with an accessible and widely used origin-destination network dataset: 2011 Census travel-to-work data. This records counts of individuals commuting between locations (census geographies) of the UK by travel mode, occupation, sex and age.\n  Read this short blog post describing some of the challenges of visually analysing OD data, and which introduces some of the techniques used in this session.\nNext read this twitter thread demonstrating how these techniques can be depoyed in a data analysis. \n    Concepts Network data: nodes and edges This class introduces techniques for visually representing networks and evaluates their usefulness through an analysis of 2011 Census OD travel-to-work data. We will focus on travel-to-work data in London – the nodes in this dataset are London’s 33 boroughs and the edges are directed OD pairs between these boroughs.\n  Table 1: Nodes table: London Boroughs.    borough  out  in      Barking and Dagenham  54237  33605    Barnet  117657  72024    Bexley  77263  39232    …  …  …       Table 2: Edges table: OD pairs between London Boroughs.    origin  destination  freq      Barking and Dagenham  Barking and Dagenham  14650    Barking and Dagenham  Barnet  280    Barking and Dagenham  Bexley  155    …  …  …     In Figure 1, frequencies of flows into- and out-of London boroughs (the nodes) are represented. Note that job-rich boroughs in central London – Westminster, City of London, Camden, Tower Hamlets – contain many more workers commuting-in for work than residents commuting out for work.\n Figure 1: Barchart of commutes in- and out- of London boroughs.  A map below (Figure 2) to help read the abbreviated labels for London boroughs and the semi-spatial orderings used in this session.\n Figure 2: Relaxed geospatial layout of London boroughs.   Node-link representations The most common examples of network visualization that we could use to represent the data in Tables 1 and 2 are node-link representations. These depict graphs in two dimensions as a force-directed layout. Nodes are positioned such that those sharing greater connection – edges with greater weights (frequencies) – are closer than those that are less well-connected – that do not share edges with such large weights. Edges are drawn as lines connecting nodes.\nFigure 1 uses a force-directed layout to represent the travel-to-work data as such a network graph. This shows large numbers of flows between job rich boroughs (particular Westminster) and boroughs containing large resident populations: Wandsworth, Lambeth, Southwark (close to central London), Barnet, Brent, Haringey (further from central London). Note that although no geographic positioning is used, the layout suggests geographic dependency in the relations between nodes, and so the manner by which populations commute between boroughs for work. For example, Hillingdon (HDN), Hounslow (HNS) and Ealing (ELG) in the bottom left; Redbridge (RDB) and Walthamstow (WTH) in the bottom middle/right.\n Figure 3: Node-link layout of commutes between London boroughs.    You will have no doubt seen many node-link representations before. It is very easy to produce complex and beautiful-looking graphics using off-the-shelf node-link layouts and they are often evoked as “Data Visualization” branding. As always, it is necessary to think about the tasks that these visualizations support and their usefulness in advancing a data analysis. You might consider this as you browse the collection of network visualizations at visualcomplexity.com.   If the aim is to compare relative frequencies of flows and locate them in geographic context – to understand both the distribution and geography of travel-to-work in London, then we can make better use of position. In Figure 4, nodes (boroughs) are placed in their exact geographic position (geometric centroid of boroughs), and line width and transparency is used to encode flow frequency. In the plot on the bottom row, flow direction is encoded by making flow lines asymmetric (following Wood, Slingsby, and Dykes 2011): the straight ends are origins, the curved ends destinations.\n Figure 4: Straight line showing commutes between London boroughs in spatial position; asymmetric lines encode direction.  The geophraphic positioning of nodes adds context and the encoding of direction provides further detail around, for example, the pattern of commuting into central London boroughs versus more peripheral boroughs (asymmetric flows into Westminster, more symmetric pattern between outer London boroughs like Brent and Enfield). However, there are problems that affect the usefulness of the graphic:\n Flows that start and end at the same location are not shown; Longer flows are more visually dominant than are shorter flows; The graphic is cluttered with a ‘hairball’ effect due to multiple overlapping lines; Aggregating to the somewhat arbitrary geometric centre of boroughs and drawing lines between these locations implies an undue level of spatial precision.   Matrix representations An alternative way to represent these edge frequencies is origin-destination matrices, as in Figure 5. The columns are destinations, London boroughs to which residents commute for work; the rows are origins, London boroughs from which residents commute for work. Edge frequencies are encoded using colour value – the darker the colour, the larger the number of commutes between those boroughs. Boroughs are ordered left-to-right and top-to-bottom according to the total number of jobs accessed in each borough.\nWhilst using colour rather than line width to show flow magnitude is a less effective encoding channel (following Munzner 2014), there are obvious advantages. The salience bias of longer flows is removed. Ordering cells of the matrix by destination size (number of jobs accessed in each borough) helps to emphasise patterns in the job-rich boroughs, but also encourages within and between comparison. For example, the lack of colour outside of the diagonals in the less job-rich boroughs, which also tend to be in outer London, suggests that labour markets there might be more self-contained: that is, people are less likely to commute-in from outside of those boroughs for work. The layout also supports ‘look-up’ type tasks – for example in the top-right plot, where the same OD commutes are removed, we can efficiently identify the boroughs importing the second (WNS) and third (LAM) largest number of workers to Westminster. By applying a local scaling on destination (bottom plot), whereby a separate colour scale is created for each destination borough, we can explore the theme of self-containment more directly. The vertical blue bars for Hammersmith and Kingston suggest that there are comparatively large numbers commuting into- these boroughs for work, especially given their size in terms of available jobs.\n Figure 5: OD matrices showing commutes between London boroughs.   OD maps The OD matrices expose new structure that could not be so easily inferred from the (more flashy) network visualizations. Although their layout is space-efficient, clearly for phenomena such as commuting geographic context is highly relevant. It is possible to make better use of layout and position to support the spatial dimension of analysis. OD Maps were proposed by Wood, Dykes, and Slingsby (2010) as a means of supporting such an analysis. They take a little to get your head around, but the idea is very elegant.\nYou will recall that we previously used the LondondSquared grid layout, also in Figure 2, for arranging London boroughs of regular size and shape. The cells of each column or row of the matrix could also be re-ordered according to this geographic layout, as in Figure 6. So, for example, we may be interested in focussing on destination, or workplace, boroughs. In the first highlighted example, commutes into Westminster are considered (the left-most column). Cells in the highlighted column are coloured according to the number of workers resident in each borough that travel into Westminster for work. These cells are then re-ordered spatially, as in the inset map for Westminster in the figure. The geographic ordering allows us to see that residents access jobs in Westminster in large numbers from many boroughs in London, but especially so from Wandsworth, Lambeth and Southwark immediately to the south of Westminster (see real geography in Figure 2). In the second example, commutes out of Hackney are considered (the middle row). Cells in the highlighted row are coloured according to the number of jobs accessed in each borough by residents travelling out of Hackney for work. Cells are again reordered in the inset map. This demonstrates that patterns of commuting are reasonably localised. The modal destination/workplace borough remains Westminster, but relatively large numbers of jobs are accessed in Camden, Islington, Tower Hamlets and the City of London.\n Figure 6: Highlighted columns (destinations/workplaces) and rows (origins/homeplaces) of the OD matrix with a spatial arrangement.  OD maps extend this idea by displaying all cells of the OD matrix with a geographic arrangement. This is achieved via a “map-within-map” layout. In the destination-focussed example in Figure 7, each larger (reference) cell identifies destinations and the smaller cells are coloured according to origins – the number of residents in each borough commuting into the reference cell for work.\n Figure 7: Destination-focussed OD map of commutes between London boroughs  The first map uses a global colour scaling and the second a local colour scaling. Ordering cells geographically helps better appreciate this distinction. As demonstrated in Figure 1, Westminster contains a large portion of jobs filled by London residents. It makes sense then that, outside of the reference cells (residents living and working in the same borough), the darkest colours corresponding with the largest flow counts are in the map where Westminster is the reference cell. Due to this dominant pattern, however, it is difficult to read too much into the patterns of commuting outside of Westminster. This is where a local scaling is useful. Flow counts are summarised over each reference borough (destination in this case) and each is normalised according to the maximum flow count for that reference borough. Most often this maximum flow count (darkest blue) is the reference cell – residents living and working in the same borough. The City of London, which contains many jobs but few residents, is the obvious exception.\nThe local scaling allows us to characterise the geography of commuting into boroughs in reasonable detail. The two job-rich boroughs – Westminster and City of London – clearly draw workers in large proportions across London boroughs and to a lesser extent this is the case for other central/inner boroughs such as Islington (ISL), Camden (CMD), Kingston (KNS). For many outer London boroughs, commuting patterns are very localised – large numbers of available jobs are filled by workers living in those boroughs or immediately neighbouring boroughs. Notice that for inner London boroughs south of the river – Lambeth (LAM), Wandsworth (WNS), Southwark (SWR) – they tend to draw workers in greater number from neighbouring boroughs that are also south of the river. There is more to unpack here. In the Technical element to the class and Homework you will consider how this geography of travel-to-work varies by travel mode.\n  The analysis above might give the impression that OD maps should be used in preference of spatially arranged node-link diagrams. As always, this depends on dataset and analysis context. In Figure 8 is a map displaying bikeshare flow data for the London Cycle Hire Scheme (LCHS), collected via the bikedata package. The LCHS consists of c.700 docking stations in London – so c. 700^2 grid cells if the grid-within-grid layout of an OD map were to be used. This would be quite challengin, although is possible (see Wood, Slingsby, and Dykes 2011). Additionally, if a synoptic overview of spatial patterns is necessary, the more intuitive node-line representation is more successful than the OD map. In the graphic below, I’ve filtered on trips that take place in the morning peak. The dominant pattern is of flows from London’s main commuter rail terminals – King’s Cross and Waterloo – to central London and City of London respectively. The asymmetric bezier curves efficiently communicate this and the reverse pattern when trips in the evening peak are filtered.\n Figure 8: Flow map of London Cycle Hire Scheme trips in the weakday morning peak. Data by TfL, accessed via bikedata; parks and river outline via OSM.       Techniques The technical element to this session continues in our analysis of 2011 Census travel-to-work data. After importing the dataset, you will organise the flow data into nodes and edges. You will then create graphics that summarise over the nodes (London boroughs in this case) and reveal spatial structure in the edges (OD flows between boroughs). You will focus on how the geography of travel-to-work varies by travel mode.\n Download the  05-template.Rmd file for this session and save it to the reports folder of your vis-for-gds project. Open your vis-for-gds project in RStudio and load the template file by clicking File \u0026gt; Open File ... \u0026gt; reports/05-template.Rmd.  Import The template file lists the required packages: tidyverse, sf and also the pct package for downloading the 2011 Census travel-to-work data.\nUsing pct’s get_od function, a .csv file of travel-to-work data between Middle-Layer Super Output Areas (MSOA) can be dowloaded. The default dataset is WU03UK, location of usual residence and place of work by method of travel to work. When downloading, you will limit to London using the region argument in get_od. This will take a few seconds to execute.\n# Import OD by travel mode from 2011 Census. od_pairs \u0026lt;- get_od( region = \u0026quot;london\u0026quot;, type = \u0026quot;within\u0026quot;, omit_intrazonal = FALSE, base_url = paste0(\u0026quot;https://s3-eu-west-1.amazonaws.com/\u0026quot;, \u0026quot;statistics.digitalresources.jisc.ac.uk\u0026quot;, \u0026quot;/dkan/files/FLOW/\u0026quot;), filename = \u0026quot;wu03ew_v2\u0026quot; ) # Import .geojson file with geometry data for LondonSquared and real layout of London boroughs. london_grid_real \u0026lt;- st_read(\u0026quot;https://www.roger-beecham.com/datasets/london_grid_real.geojson\u0026quot;) The downloaded data have the following structure:\n  Table 3: Raw OD pairs retrieved from get_od().    geo_code1  geo_code2  all  train  bus  …cont  la_1  la_2      EO2000001  EO2000014  3  1  0  …  City of London  City of London    EO2000001  EO2000016  1  0  1  …  City of London  Barking and Dagenham    …  …  …  …  …  …  …  …    origin msoa  dest msoa  count  count  count  …  origin bor  dest bor     In the template file is code for aggregating these MSOA data to borough level to derive tables representing nodes and edges.\nFirst, we ensure that only commutes made by London residents between London boroughs are recorded and so we generate a look-up of London boroughs. When installing and loading the pct package, a table matching Local Authorities with Regions (pct_regions_lookup) was made available. We filter this on region_name to extract a list of all local authorities (lad16nm), London boroughs in this case), and store them as a vector using pull(). This vector of London borough names (london_las) is used to filter() the raw od_pairs data frame.\n# Look-up of London boroughs. london_las \u0026lt;- pct_regions_lookup %\u0026gt;% filter(region_name==\u0026quot;london\u0026quot;) %\u0026gt;% pull(lad16nm) To generate an edges table, od_pairs is grouped by the origin and destination borough (la_1, la_2) and flow counts are summed for each unique borough-borough OD pair. Note that we use dplyr’s across() function to summarise over the multiple travel mode columns. Our analysis focuses on trips made by public_transport, car, and active transport (foot+bike) and so we generate new combined mode counts merging travel modes and also dropping those we do not wish to use, using transmute(). The edges table, an OD dataset summarising commute counts between boroughs, contains 1089 rows (33^2 borough-borough commutes).\n edges \u0026lt;- od_pairs %\u0026gt;% # Filter only *within* London. filter(la_1 %in% london_las, la_2 %in% london_las) %\u0026gt;% group_by(la_1, la_2) %\u0026gt;% summarise( across(c(all:other), sum) ) %\u0026gt;% ungroup %\u0026gt;% transmute( o_bor=la_1, d_bor=la_2, all=all, public_transport=train+bus+light_rail, car=car_driver+car_passenger, active=bicycle+foot )   Table 4: Edges table.    o_bor  d_bor  all  public_transport  car  active      Barking and Dagenham  Barnet  280  143  131  2    Barking and Dagenham  Bexley  155  41  109  2    …  …  …  …  …  …    origin bor  dest bor  count  count  count  count     Nodes in the dataset are the 33 London boroughs. We can express the 2.9m commutes between these nodes in different ways – according to whether nodes are destinations or origins. In the code below, two tables are generated with OD data grouped by destination (nodes_d) and origin (nodes_o) and commutes into- and out of- boroughs counted respectively. These two data sets are then joined with bind_rows() and distinguished via the variable name type.\n# Summarise over destinations: commutes into- boroughs. nodes_d \u0026lt;- od_pairs %\u0026gt;% # Filter only *within* London. filter(la_1 %in% london_las, la_2 %in% london_las) %\u0026gt;% group_by(la_2) %\u0026gt;% summarise( across(c(all:other), sum) ) %\u0026gt;% ungroup %\u0026gt;% rename(bor = la_2) %\u0026gt;% transmute( bor=bor, type=\u0026quot;destination\u0026quot;, all=all, public_transport=train+bus+light_rail, car=car_driver+car_passenger, active=bicycle+foot ) nodes_o \u0026lt;- od_pairs %\u0026gt;% # Filter only *within* London. filter(la_1 %in% london_las, la_2 %in% london_las) %\u0026gt;% group_by(la_1) %\u0026gt;% summarise( across(c(all:other), sum) ) %\u0026gt;% ungroup %\u0026gt;% rename(bor = la_1) %\u0026gt;% transmute( bor=bor, type=\u0026quot;origin\u0026quot;, all=all, public_transport=train+bus+light_rail, car=car_driver+car_passenger, active=bicycle+foot ) nodes \u0026lt;- nodes_o %\u0026gt;% bind_rows(nodes_d)   Table 5: Nodes table.    bor  type  all  public_transport  car  active      Barking and Dagenham  dest  33605  9483  19050  4644    Barnet  dest  72024  24838  36484  9890    …  …  …  …  …  …    bor  origin|dest  count  count  count  count      Analyse over nodes In Figure 9 are a set of plots summarising over the nodes dataset and using colour to support comparison by travel mode. The first plot is a bar chart ordered left-to-right on commute frequency (by destination borough). Nodes are expressed as destinations on the left panel and origins on the right. This demonstrates a pattern similar to that in Figure 1 where there are many commutes into- job rich boroughs in central London. Note that the amount of red, representing commutes by car, increases left-to-right as we move away from job-rich central London boroughs. This is best expressed by the proportional/filled bar chart in the second plot. Hillingdon (HDN), a borough which contains a relatively large number of jobs, looks distinctive from its other job-rich neighbours in this plot (Lambeth LAM, Hammersmith \u0026amp; Fulham HMS) as it contains a reasonably large block of red (commuting by car). This is, however, consistent with other boroughs that are similarly “outer” London (Havering HVG, Bexley BEX, Enfield ENF). The third plot incorporates this geographic by ordering the bars again using the LondonSquared layout. Destination and origin summary bars are juxtaposed next to each other and heights are scaled locally by borough. This enables visual comparison of whether there is a net in- or out- flow of workers. Job-rich central London boroughs contain longer “destination” bars importing large numbers of workers from other London boroughs, outer-London boroughs generally have longer “origin” bars exporting large numbers of residents to work in other London boroughs.\n Figure 9: Destination-focussed OD map of commutes between London boroughs  The code – bar charts:\n# Vector of borough names for ordering. bor_orders \u0026lt;- nodes %\u0026gt;% filter(type==\u0026quot;destination\u0026quot;) %\u0026gt;% arrange(-all) %\u0026gt;% pull(bor) # Plot bars nodes %\u0026gt;% pivot_longer(cols=c(active, public_transport, car), names_to=\u0026quot;mode\u0026quot;, values_to=\u0026quot;count\u0026quot;) %\u0026gt;% mutate(bor=factor(bor, levels=bor_orders)) %\u0026gt;% ggplot() + geom_col(aes(x=bor, y=count, fill=mode)) + # add argument position=\u0026quot;fill\u0026quot; for proportional bars. scale_fill_manual(values=c(\u0026quot;#2171b5\u0026quot;,\u0026quot;#cb181d\u0026quot;, \u0026quot;#238b45\u0026quot;))+ guides(fill=FALSE)+ labs(x=\u0026quot;\u0026quot;, y=\u0026quot;count\u0026quot;) + facet_wrap(~type, ncol=2) The plot specification – bar charts:\nData: Create a vector of borough names (bor_orders) used to order bars according to total commute counts (by destination), casting to a factor variable, as in line 3 of the plot specification. As bars are coloured according to travel mode, the data frame must be made narrower and longer, such that each row (observation) represents a count for a single travel mode by borough over origin or destination. This is achieved with pivot_longer(). Encoding: Bar length varies according to commute counts, so map the ordered factor variable (bor) to the x-axis and commute counts (count) to the y-axis. Bars are filled according to mode. Marks: geom_col() for bars. Scale: scale_fill_manual() for differentiating mode by colour hue. Colours are defined in hex space. Facets: facet_wrap() on type for generating juxtaposed plots with node counts summarised by “origin” and “destination”. Setting: For the proportional bases add position=\"fill\" argument to geom_col().  The code – spatially-arranged bars:\n# Plot bars nodes %\u0026gt;% left_join(london_grid_real %\u0026gt;% st_drop_geometry() %\u0026gt;% filter(type==\u0026quot;grid\u0026quot;) %\u0026gt;% select(authority, BOR,x,y),by=c(\u0026quot;bor\u0026quot;=\u0026quot;authority\u0026quot;)) %\u0026gt;% pivot_longer(cols=c(active, public_transport, car), names_to=\u0026quot;mode\u0026quot;, values_to=\u0026quot;count\u0026quot;) %\u0026gt;% group_by(bor) %\u0026gt;% mutate(bor_total=sum(count)) %\u0026gt;% ggplot() + geom_col(aes(x=type, y=count/bor_total, fill=mode))+ geom_text(data=. %\u0026gt;% filter(mode==\u0026quot;active\u0026quot;, type==\u0026quot;destination\u0026quot;), aes(x=1.5, y=1, label=BOR), vjust=\u0026quot;top\u0026quot;, hjust=\u0026quot;centre\u0026quot;)+ geom_text(data=. %\u0026gt;% filter(mode==\u0026quot;active\u0026quot;, type==\u0026quot;destination\u0026quot;), aes(x=1.4, y=.85, label=\u0026quot;dest\u0026quot;), vjust=\u0026quot;top\u0026quot;, hjust=\u0026quot;right\u0026quot;, size=3)+ geom_text(data=. %\u0026gt;% filter(mode==\u0026quot;active\u0026quot;, type==\u0026quot;origin\u0026quot;), aes(x=1.6, y=.85, label=\u0026quot;origin\u0026quot;), vjust=\u0026quot;top\u0026quot;, hjust=\u0026quot;left\u0026quot;, size=3)+ scale_fill_manual(values=c(\u0026quot;#2171b5\u0026quot;,\u0026quot;#cb181d\u0026quot;, \u0026quot;#238b45\u0026quot;))+ facet_grid(y~x)+ guides(fill=FALSE, alpha=FALSE)+ theme( panel.spacing.y=unit(.2, \u0026quot;lines\u0026quot;), panel.spacing.x=unit(.2, \u0026quot;lines\u0026quot;), panel.background = element_rect(fill=\u0026quot;#ffffff\u0026quot;, colour=\u0026quot;#ffffff\u0026quot;), axis.title.x = element_blank(),axis.title.y = element_blank(), axis.text.x=element_blank(), axis.text.y = element_blank(), strip.text.x=element_blank(), strip.text.y = element_blank(), panel.grid=element_blank() ) The plot specification – spatially-arranged bars:\nData: A few things to note here. First we must left_join() on the dataset that gives us the grid locations of each borough (london_grid_real) so that the bars can be spatially arranged (via facet_grid()). We strip out the geometry from this dataset (it is not required), and select only those variables that we need: the x,y position in the grid, borough names for matching with our nodes dataset (“bor”=“authority”) and abbreviation for labelling (BOR). We need to reshape the dataset in the same way as for the non-spatially-arranged bars – and so the call to pivot_wider(). Also, we want to scale the height of bars for each borough such that the relative number of in- and out- commutes within-borough (commute type) can be compared. To do this, we calculate a bor_total variable, which is the total number of residents commuting out-, and jobs filled by workers commuting in-, for each borough. Note that there is double counting here where workers live and work in the same borough. Encoding: Bar length (y-axis) varies according to commute counts normalised by the bor_total variable. Separate bars are drawn depending on type, summarised over “origin” or “destination”, so type is mapped to the x-axis. Again bars are filled according to mode. Marks: geom_col() for bars. Scale: scale_fill_manual() for differentiating mode by colour hue. Colours are defined in hex space. Facets: facet_grid() for laying out plots with a relaxed geographic (2D) arrangement. This is the [LondondSquared layout from the london_grid_real table. Setting: Various within theme() to remove, for example, unnecessary axis and panel labels, which are effectively grid references for 2D arrangement. The plot is annotated with text labels (geom_text()): borough names using the abbreviated BOR variable and also the type of bar (origin or destination). Labels are positioned within each facet panel by manually passing an x,y location, and supplying a value for the type label and using the BOR variable for the borough label. We use filter() here as otherwise a text label would appear for each row of the dataset. Try removing the call to data= in the geom_text lines to explore this.   Analyse over edges Figure 10 displays OD maps of commuting between boroughs using public transport. In a similar way to Figure 9 maps are created separately with a destination and origin focus. The reference cells for the maps on the left are destinations and the small cells are coloured according to number of commutes into- those destinations for work; the references cells for the maps on the right are origins and small cells are coloured according to number of commutes by residents out-of those boroughs for work. I won’t do any interpretation; that is reserved for this session’s Homework. A consolation is that there is no additional coding required for the homework.\n Figure 10: OD map of commutes between London boroughs  The code:\n# Temporary plot object of data joined to geom_sf geometries. # DO map so geometries join on origin (edit this to switch between D-OD and O-DO matrix). plot_data_temp \u0026lt;- london_grid_real %\u0026gt;% filter(type==\u0026quot;grid\u0026quot;) %\u0026gt;% right_join(edges, by=c(\u0026quot;authority\u0026quot;=\u0026quot;o_bor\u0026quot;)) %\u0026gt;% mutate(o_bor=authority) %\u0026gt;% rename(o_fx=x, o_fy=y) %\u0026gt;% left_join(grid_real_sf %\u0026gt;% filter(type==\u0026quot;grid\u0026quot;) %\u0026gt;% st_drop_geometry() %\u0026gt;% select(authority,x,y), by=c(\u0026quot;d_bor\u0026quot;=\u0026quot;authority\u0026quot;) ) %\u0026gt;% rename(d_fx=x, d_fy=y) %\u0026gt;% # Identify borough in focus (edit this to switch between D-OD and O-DO matrix). mutate(bor_label=if_else(o_bor==d_bor,d_bor,\u0026quot;\u0026quot;), bor_focus=if_else(o_bor==d_bor,1,0)) # Bounding box for positioning text labels. bbox_grid \u0026lt;- st_bbox(london_grid_real %\u0026gt;% filter(type==\u0026quot;grid\u0026quot;)) # Draw plot -- simple. plot_data_temp %\u0026gt;% ggplot()+ geom_sf(aes(fill=public_transport), colour=\u0026quot;#616161\u0026quot;, size=0.15)+ coord_sf(crs=st_crs(plot_data_temp), datum=NA)+ guides(fill=FALSE)+ facet_grid(d_fy~d_fx, shrink=FALSE)+ scale_fill_distiller(palette=\u0026quot;Greens\u0026quot;, direction=1) # Draw plot -- with text annotations. plot_data_temp %\u0026gt;% ggplot()+ geom_sf(aes(fill=public_transport), colour=\u0026quot;#616161\u0026quot;, size=0.15)+ geom_sf(data=. %\u0026gt;% filter(bor_focus==1), fill=\u0026quot;transparent\u0026quot;, colour=\u0026quot;#373737\u0026quot;, size=0.3)+ geom_text(data=. %\u0026gt;% filter(bor_focus==1), aes(x=east, y=north, label=str_sub(BOR,1,1)), colour=\u0026quot;#252525\u0026quot;, alpha=1.0, size=2, show.legend=FALSE, hjust=\u0026quot;centre\u0026quot;, vjust=\u0026quot;middle\u0026quot;)+ geom_text(data=. %\u0026gt;% filter(bor_focus==1), aes(x=bbox_grid$xmax, y=bbox_grid$ymin, label=BOR), colour=\u0026quot;#252525\u0026quot;, alpha=0.6, size=2, show.legend=FALSE, hjust=\u0026quot;right\u0026quot;, vjust=\u0026quot;bottom\u0026quot;)+ coord_sf(crs=st_crs(plot_data_temp), datum=NA)+ guides(fill=FALSE)+ facet_grid(d_fy~d_fx, shrink=FALSE)+ scale_fill_distiller(palette=\u0026quot;Greens\u0026quot;, direction=1)+ theme( panel.spacing=unit(0.1, \u0026quot;lines\u0026quot;), axis.title.x=element_blank(),axis.title.y=element_blank(), strip.text.x = element_blank(), strip.text.y = element_blank(), panel.background = element_rect(fill=\u0026quot;#ffffff\u0026quot;, colour=\u0026quot;#ffffff\u0026quot;) ) The plot specification:\n 1. Data:  We create a temporary staging data frame for this plot (plot_data_temp). This is our edges dataset, joined twice on london_grid_real. The first join. The code above is for generating a D-OD map, where the large cells are destinations and the small cells are origins. In the plot spec you will see, as in the spatially arranged bars, that for D-OD maps we facet on destination using the LondonSquared layout. This means that we want to bring in geometry data for the origin borough in each OD pair and make this explicit in the join – by=c(\"authority\"=\"o_bor\"). We also rename() the joined variables accordingly – o_fx, o_fy refers to the LondonSquared grid cell references of the origins. If we wanted to use the LondonSquared layout only for faceting, and represent origin boroughs using their real geography (as in Beecham and Slingsby 2019, @slingsby_od_2014) we could join on the real and not grid geometries – with filter(type==\"real\"). The second join is on destination (origin in the case of an O-OD map). We drop the geometry data from the joining London boundaries object, but keep the LondonSquared layout grid references and rename accordingly – e.g. rename(d_fx=x, d_fy=y). Finally in the mutate() we generate a new variable identifying the borough in focus (bor_focus), destination in this case, and a text label variable for annotating plots on this (bor_label). An additional derived object contains coordinate pairs describing the geographic extent of our geometries data – see st_bbox().  2. Encoding: Cells are coloured according to commute count by public transport (fill=public_transport). 3. Marks: geom_sf() – these are effectively spatially arranged choropleth maps. In the example we use a regular grid geometries, but we could easily use real geographies by filtering london_grid_real on type==\"real\" in the data staging code. 4. Scale: scale_fill_distiller() for a continuous colour scheme using the ColorBrewer Greens palette. 5. Facets: facet_grid() for laying out plots with a relaxed geographic (2D) arrangement. Again we facet on destination as this is a D-OD map. 6. Setting: Various within theme() to remove, for example, unnecessary axis and panel labels. The plot is annotated with text labels (geom_text()) – borough names using the abbreviated BOR variable. The three letter abbreviations are positioned in x,y to the bottom-right of the plot using the values in the bounding box object (bbox_grid). The single letter abbreviation is positioned at the centroid of the grid geometries east, north. Single letters are derived from the BOR variable using str_sub().    Some of the code around the annotated text labels is a little hacky and not particularly elegant. However, with this removed, as in the block above, the plot specification is really quite simple and more importantly links to how OD maps are constructed – fill cells according to a count variable and juxtapose or facet according to the reference, or focus, for comparison.\n     References Beecham, Roger, and Jo Wood. 2014. “Exploring Gendered Cycling Behaviours Within a Large-Scale Behavioural Data-Set.” Transportation Planning and Technology 37 (1). Taylor \u0026amp; Francis: 83–97.\n Beecham, R., and A. Slingsby. 2019. “Characterising labour market self-containment in London with geographically arranged small multiples.” Environment and Planning A: Economy and Space 51 (6): 1217–24.\n Munzner, Tamara. 2014. Visualization Analysis and Design. AK Peters Visualization Series. Boca Raton, FL: CRC Press.\n Slingsby, A., M. Kelly, and J. Dykes. 2014. “OD maps for showing changes inIrish female migration between 1851 and 1911.” Environment and Planning A: Economy and Space 46 (12): 2795–7.\n Wickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2020. Ggplot2: Elegant Graphics for Data Analysis. Springer.\n Wood, J., J. Dykes, and A. Slingsby. 2010. “Visualisation of Origins, Destinations and Flows with Od Maps.” The Cartographic Journal 47 (2): 117–29.\n Wood, Jo, Aidan Slingsby, and Jason Dykes. 2011. “Visualizing the Dynamics of London’s Bicycle-Hire Scheme.” Cartographica: The International Journal for Geographic Information and Geovisualization, no. 4: 239–51.\n   ","date":1623110400,"expirydate":-62135596800,"kind":"page","lang":"en-uk","lastmod":1606644925,"objectID":"0ddf3e2b3a9f5347c88f83033fb01ff2","permalink":"/class/05-class/","publishdate":"2021-06-08T00:00:00Z","relpermalink":"/class/05-class/","section":"class","summary":"Contents  Introduction Concepts Network data: nodes and edges Node-link representations Matrix representations OD maps  Techniques Import Analyse over nodes Analyse over edges  References   By the end of this session you should gain the following knowledge:\n       By the end of this session you should gain the following practical skills:\n       Introduction Networks are a special class of data used to represent things (entities) and how they relate to one another.","tags":null,"title":"Visualization for exploring spatial networks: Containment and connection","type":"docs"},{"authors":null,"categories":null,"content":"  Contents  Session outcomes Welcome to Visualization for Geographic Data Science Why vis-for-gds? Geographic Data Science Geographic Data Science and Visualization  What vis-for-gds? How vis-for-gds? R for modern data analysis Rmarkdown for reproducible research  Getting started with R and RStudio Install R and RStudio Open the RStudio IDE Compute in the console Install some packages Experiment with R Markdown R Scripts Create an RStudio Project  Conclusions References   Session outcomes By the end of this session you should gain the following knowledge:\n   Appreciate the motivation for this module – why visualization, why R and why ggplot2    By the end of this session you should gain the following practical skills:\n   Navigate the materials on this course website, having familiarised yourself with its structure Open R using the RStudio Integrated Developer Environment (IDE) Install and enable R packages and query package documentation Perform basic calculations via the R Console Render R Markdown files Create R Projects Read-in datasets from external resources as objects (specifically tibbles)     Welcome to Visualization for Geographic Data Science Welcome to Visualization for Geographic Data Science (vis-for-gds). In this first session we’ll cover the background to the module – the why, what and how of vis-for-gds. If you’ve not already you should check out the course outline on the Syllabus page for an overall preview of the module.\nThe main home for this module is this website. However, via Minerva you can access the Module Handbook. You will use submission boxes in Minerva to upload coursework in the usual way.\n Why vis-for-gds? Geographic Data Science It is now taken-for-granted that over the last decade or so new data, new technology and new ways of doing science have transformed how we approach the world’s problems. Evidence for this can be seen in the response to the Covid-19 pandemic. Enter Covid19 github into a search and you’ll be confronted with hundreds of repositories demonstrating how an ever-expanding array of data related to the pandemic can be collected, processed and analysed. Data Science is a term used widely to capture this shift and Geographic Data Science (GDS), probably first discussed coherently by Arribas-Bel and Reades (2018) and Singleton and Arribas-Bel (2019), when observing that many of data science’s applications are – or at least should be – of inherent interest to geographers.\nSince gaining traction in the corporate world, the definition of Data Science has been somewhat stretched, but it has its origins in the work of John Tukey’s The Future of Data Analysis (1962). Drawing on this, and a survey of more recent work, Donoho (2017) neatly identify six key facets that a data science discipline might encompass1 :\ndata gathering, preparation, and exploration; data representation and transformation; computing with data; data visualization and presentation; data modelling; and a more introspective “science about data science”   Geographic Data Science and Visualization Visualization is fundamental to meeting the unprecedented challenges and exploiting the wonderful opportunities of the ever-expanding deluge of data confronting virtually every field.\" \\  -- Prof. Jim Hollan of UC San Diego --  Visual approaches to data analysis are particularly suited to Geographic Data Science applications because where datasets are being repurposed for social and natural sciences research for the first time, contain complex structure and geo-spatial relations that cannot be easily captured by statistical summaries alone and so where the types of questions that can be asked and the techniques deployed to answer them cannot be easily specified in advance.   -- Glancing at this list, visualization could be interpreted as a single facet of Data Science process2 – something that happens after data gathering, preparation, exploration, but before modelling. In this module you’ll learn that visualization is intrinsic to, and should inform, each of these activities, especially so when working with data sets that are spatial – for Geographic Data Science.\nLet’s develop this idea by asking why data visualizations are used in the first place. In her book Visualization Analysis and Design, Tamara (???) considers how humans and computers interface in the decision-making process. She makes the point that data visualization is ultimately about connecting people with data in order to make decisions – or to install humans in a ‘decision-making-loop’. There are occasionally decision-making loops that are entirely computational and where an automated solution exists and is trusted. However, most require some form of human intervention.\nThe canonical example demonstrating how relying on computation alone can be problematic, and so for the use of visualization, is Anscombe’s quartet. Here, Anscombe (1973) presents four datasets, each containing eleven observations and two variables for each observation. The data are synthetic, but let’s say that they are the weight and height of independent samples taken from a population of Postgraduate Students studying Data Science.\nPresented with a new dataset it makes sense to compute some summaries and doing so, we observe that the data appear identical – they contain the same means, variances and strong positive correlation coefficient. This seems appropriate since the data are measuring weight and height. Although there may be some variation, we’d expect taller students to be heavier. Given these statical summaries we can be confident that we are drawing samples from the same population of (Data Science) students.\n Figure 1: Data from Anscombe’s quartet  Laying out the data in a meaningful way, horizontally according to weight (x) and vertically according to the height (y) to form a scatterplot, we quickly see that whilst these data contain the same statistical properties they are very different. Only dataset #1 now looks plausible if it were truly a measure of weights and heights drawn from a population of students.\nAnscombe’s is a deliberately contrived example3, but there are real cases of important structure being missed, leading to poorly specified models and potentially faulty claims.\n Figure 2: Plots of Anscombe’s quartet  This is not to undermine the importance of numerical analysis. Numeric summaries that simplify patterns are extremely useful and Statistics has at its disposal an array of tools for helping to guard against making false claims from datasets – a theme that we will return to in session 6, 7 and 8 when we think critically about the use of visual approaches for data anlysis. There remain, though, certain classes of relation and context that cannot be easily captured through statistics alone.\nGeographic context is undoubtedly challenging to capture numerically; many of the early examples of data visualization have been of spatial phenomena and generated by Geographers (see Friendly 2007). We can also probably make a special case for the use of visual approaches in Geographic Data Science (GDS) applications due to its exploratory nature. Often in GDS datasets are being repurposed for social and natural sciences research for the first time; contain complex structure and geo-spatial relations that cannot be easily captured by statistical summaries alone; and so the types of questions that can be asked and the techniques deployed to answer them cannot be easily specified in advance. In this module we will demonstrate this as we explore (Session 4 and 5), model under uncertainty (Session 6 and 7) and communicate (Session 7 and 8) with various social science datasets.\n  Watch Jo Wood’s talk demonstrating how visual techniques can be used to analyse urban travel behaviours. In the video Jo argues that bikeshare schemes can help democratise cycling, but also for their potential contributions to research – he briefly contrasts new, passively collected data sets with more “traditional” actively collected data for analysing how people move around cities. A compelling case is then made for the use of visualization to support this activity. This work and further discussion is published in Beecham and Wood (2014).     Effective data visualizations should expose structure in data that would be difficult to expose through non-visual means --   What vis-for-gds? This is a very practical module. With the exception of this Introduction, the weekly sessions will blend both theory and practical coding activity. We will cover fundamentals around visual data analysis from Information Visualization and Statistics. As you read the session materials you will be writing data processing and analysis code and so be generating analysis outputs of your own. We will also be working with real datasets – from the Political Science, Urban and Transport Planning and Health domains. So we will hopefully be generating real findings and knowledge.\nTo do this in a genuine way – to generate real knowledge from datasets – we will have to cover a reasonably broad set of data processing and analysis procedures. As well as developing expertise around designing data-rich, visually compelling graphics (of the sort demonstrated in Jo Wood’s TEDx talk), we will need to cover more tedious aspects of data processing and wrangling. Additionally, if we are to learn how to generate and communicate and make claims under uncertainty with our data graphics, then we will need to cover some aspects of estimation and modelling from Statistics. In short, we will cover most of Donoho (2017)’s six key facets of a data science discipline:\ndata gathering, preparation, and exploration (Sessions 2, 3, 5); data representation and transformation (Sessions 2, 3); computing with data (Session 2); data visualization and presentation (All Sessions); data modelling (Sessions 6, 7, 8); and a more introspective “science about data science” (All Sessions, Plus Optional Extra)  There is already a rich and impressive set of open Resources practically introducing how to do modern Data Science, Visualization and Geographic Analysis. We will certainly draw on these at different stages in the module. What makes this module different from these existing resources is that we will be doing applied data science throughout – we will be identifying and diagnosing problems when gathering data, discovering patterns (some maybe even spurious) as we do exploratory analysis, and attempt to make claims under uncertainty as we generate models based on observed patterns. We will work with both new, passively-collected datasets, as well as more traditional, actively collected datasets located within various social science domains: Political Science, Health Science and Urban and Transport Planning.\n How vis-for-gds? R for modern data analysis Through the module we will apply modern approaches to data analysis. All data collection, analysis and reporting activity will be completed using R and the RStudio Integrated Development Environment (IDE). Released as open source software as part of a research project in 1995, for some time R was the preserve of academics. From 2010s onwards, the R community expanded rapidly and along with Python is regarded as the key technology for doing data analysis. R is used increasingly outside of academia, by organisations such as Google [example], Facebook [example], Twitter [example], New York Times [example], BBC [example] and many more.\nThere are many benefits that come from being fully open-source, with a critical mass of users. Firstly, there is an array of online forums, tutorials and code examples from which to learn. Second, with such a large community, there are numerous expert R users who themselves contribute by developing libraries or packages that extend its use. As a result R is employed for a very wide set of use cases – this website was even built in R using amongst other things the blogdown package.\n  The key reason for our use of R is the ecosystem of users and packages that have emerged in recent years. An R package is a bundle of code, data and documentation, usually hosted on the CRAN (Comprehensive R Archive Network).   Of particular importance is the tidyverse package. This is a set of packages for doing Data Science authored by a software development team at RStudio led by Hadley Wickham. tidyverse packages share a principled underlying philosophy, syntax and documentation. Contained within the tidyverse is its data visualization package, ggplot2. This package pre-dates the tidyverse – it started as Hadley Wickham’s PhD thesis and is one of the most widely-used toolkits for generating data graphics. As with other heavily used visualization toolkits (Tableau, vega-lite) it is inspired by Leland Wilkinson’s The Grammar of Graphics, the gg in ggplot stands for Grammar of Graphics. Understanding the design principles behind the Grammar of Graphics (and tidyverse) is necessary for modern data analysis and so we will cover this in Sessions 2 and 3, as well as re-visiting later in the module.\n Rmarkdown for reproducible research  Reproducible research is the idea that data analyses, and more generally, scientific claims, are published with their data and software code so that others may verify the findings and build upon them.\nRoger Peng, Jeff Leek and Brian Caffo\n In recent years there has been much introspection around how science works – around how statistical claims are made from reasoning over evidence. This came on the back of, amongst other things, a high profile paper published in Science, which found that of 100 recent peer-reviewed psychology experiments, the findings of only 39 could be replicated. The upshot is that researchers must now make every possible effort to make their work transparent, such that “all aspects of the answer generated by any given analysis [can] be tested” (Brunsdon and Comber 2020).\nA reproducible research project should be accompanied with:\n code and data that allows tables and figures presented in research outputs to be regenerated code and data that does what it claims (the code works) code and data that can be justified and explained through proper documentation  If these goals are met, then it may be possible for others to use the code on new and different data to study whether the findings reported in one project might be replicated or to use the same data, but update the code to, for example, extend the original analysis (to perform a re-analysis). This model – generate findings, test for replicability in new contexts and re-analysis – is essentially how knowledge development has always worked. However, to achieve this the data and procedures on which findings were generated must be made open and transparent.\nIn this setting, traditional proprietary data analysis software such as SPSS and Esri’s ArcGIS that support point-and-click interaction is problematic. First, whilst these software may rely on the sorts of packages and libraries with bundled code that R and Python uses for implementing statistical procedures, those libraries are closed. It is not possible, and therefore less common, for the researcher to fully interrogate into the underlying processes that are being implemented and the results need to be taken more or less on faith. Second, but probably most significantly (for us), it would be tedious to make notes describing all interactions performed when working with a dataset in SPSS or ArcGIS.\nAs a declarative programming language, it is very easy to provide such a provenance trail for your workflows in R since this necessarily exists in the analysis scripts. But more importantly, the Integrated Development Environments (IDEs) through which R (and Python) are most often accessed provide notebook environments that allow users to curate reproducible computational documents that blend input code, explanatory prose and outputs. In this module we will prepare these sorts of notebooks using R Markdown.\n  Getting started with R and RStudio I mentioned that the weekly sessions will blend both theory and practical coding activity. This Introduction has been dedicated more towards conceptual and procedural matters. For the practical element this time, we want to get you configured and familiar with R and RStudio.\nInstall R and RStudio  Install the latest version of R. Note that there are installations for Windows, macOS and Linux. Run the installation from the file you downloaded (an .exe or .pkg extension). Install the latest version of RStudio Desktop. Note again that there are separate installations depedning on operating system – for Windows an .exe extension, macOS a .dmg extension.   Open the RStudio IDE  Figure 3: The RStudio IDE   Once installed, open the RStudio IDE. Open an R Script by clicking File \u0026gt; New File \u0026gt; R Script .  You should see a set of windows roughly similar to those in the Figure (although I’ve already started on some of the computing exercises in the next section). The top left pane is used either as a Code Editor (the tab named Untitled1) or data viewer. This is where you’ll write, organise and comment R code for execution or inspect datasets as a spreadsheet representation. Below this in the bottom left pane is the R Console, in which you write and execute commands directly. To the top right is a pane with the tabs Environment and History. This displays all objects – data and plot items, calculated functions – stored in-memory during an R session. In the bottom right is a pane for navigating through project directories, displaying plots, details of installed and loaded packages and documentation on their functions.\n Compute in the console You will write and execute almost all code from the code editor pane. To start though let’s use R as a calculator by typing some commands into the Console. You’ll create an object (x) and assign it a value using the assignment operator (\u0026lt;-), then perform some simple statistical calculations using functions that are held within the base package.\n  The base package is core and native to R. Unlike all other packages, it does not need to be installed and called explicitly. One means of checking the package to which a function you are using belongs is to call the help command (?) on that function: e.g. ?mean().    Type the commands contained in the code block below into your R Console. Notice that since you are assigning values to each of these objects they are stored in memory and appear under the Global Environment pane.  # Create variable and assign a value. x \u0026lt;- 4 # Perform some calculations using R as a calculator. x_2 \u0026lt;- x^2 # Perform some calculations using functions that form baseR. x_root \u0026lt;- sqrt(x_2)  Install some packages There are two steps to getting packages down and available in your working environment:\ninstall.packages(\u0026lt;package-name\u0026gt;) downloads the named package from a repository. library(\u0026lt;package-name\u0026gt;) makes the package available in your current session.   Install the tidyverse, the core collection of packages for doing Data Science in R, by running the code below:  install.packages(\u0026quot;tidyverse\u0026quot;) If you have little or no experience in R, it is easy to get confused around downloading and then using packages in a session. For example, let’s say we want to make use of the simple features package (sf), which we will drawn on heavily in the module for performing spatial operations.\n Run the code below:  library(sf) Unless you’ve previously installed sf, you’ll probably get an error message that looks like this:\n\u0026gt; Error in library(sf): there is no package called ‘sf’ So let’s install it.\n Run the code below:  install.packages(\u0026quot;sf\u0026quot;) And now it’s installed, why not bring up some documentation on one of its functions (st_contains()).\n Run the code below:  ?st_contains() Since you’ve downloaded the package but not made it available to your session, you should get the message:\n\u0026gt; No documentation for ‘st_contains’ in specified packages and libraries So let’s try again, by first calling library(sf).\n Run the code below:  library(sf) ## Linking to GEOS 3.7.2, GDAL 2.4.1, PROJ 6.1.0 ?st_contains() Now let’s install some of the remaining core packages on which the module depends.\n Run the block below, which passes a vector of package names to the install.packages() function:  install.packages(c(\u0026quot;devtools\u0026quot;,\u0026quot;usethis\u0026quot;, \u0026quot;rmarkdown\u0026quot;, \u0026quot;knitr\u0026quot;,\u0026quot;fst\u0026quot;, \u0026quot;lubridate\u0026quot;, \u0026quot;tidymodels\u0026quot;, \u0026quot;rmapshaper\u0026quot;))   If you wanted to make use of a package only very occasionally in a single session, you could access it without explicitly loading it via library(\u0026lt;package-name\u0026gt;), using this syntax: \u0026lt;package-name\u0026gt;::\u0026lt;function_name\u0026gt;, e.g. ?sf::st_contains().    Experiment with R Markdown R Markdown documents are suffixed with the extension .Rmd and based partly on Markdown, a lightweight markup language originally used as a means of minimising tedious mark-up tags (\u0026lt;header\u0026gt;\u0026lt;/header\u0026gt;) when preparing HTML documents. The idea is that you trade some flexibility in the formatting of your HTML for ease-of-writing. Working with R Markdown is very similar to Markdown. Sections are denoted hierarchically with hashes (#, ##, ###) and emphasis using * symbols (*emphasis* **added** reads emphasis added ). Different from standard Markdown, however, R Markdown documents can also contain code chunks that can be run when the document is typeset – they are a mechanism for producing elegant reproducible notebooks.\nEach session of the module has an accompanying R Markdown file. in later sessions you will use these to author computational notebooks that blend code, analysis prose and outputs.\n Download the  01-template.Rmd file for this session and open it in RStudio by clicking File \u0026gt; Open File ... \u0026gt; \u0026lt;your-downloads\u0026gt;/01-template.Rmd.  A quick anatomy of an R Markdown files :\n YAML - positioned at the head of the document and contains metadata determining amongst other things the author details and the output format when typesetting. TEXT - incorporated throughout to document and comment on your analysis. CODE chunks - containing discrete that are to be run when the .Rmd file is typeset or knit.   Figure 4: The anatomy of R Markdown  The YAML section of an .Rmd file controls how your file is typeset and consists of key: value pairs enclosed by ---. Notice that you can change the output format – so should you wish you can generate for example .pdf, .docx files for your reports.\n--- author: \u0026quot;Roger Beecham\u0026quot; date: \u0026#39;2021-05-01\u0026#39; title: \u0026quot;Session 01\u0026quot; output:html_document --- R Markdown files are rendered or typeset with the Knit button, annotated in the Figure above. This starts the knitr package and executes all the code chunks and outputs a markdown (.md) file. The markdown file can then be converted to many different output formats via pandoc.\n Knit the  01-template.Rmd file for this session, either by clicking the Knit button or by typing ctrl + ⇧ + K on Windows, ⌘ + ⇧ + K on macOS.  You will notice that R Markdown code chunks can be customised in different ways. This is achieved by populating fields in the curly brackets at the start of the code chunk:\n```{r \u0026lt;chunk-name\u0026gt;, echo=TRUE, eval=FALSE, cache=FALSE} # Some code that is either run or rendered. ``` A quick overview of the parameters.\n \u0026lt;chunk-name\u0026gt; - Chunks can be given distinct names. This is useful for navigating R markdown file. It also supports chaching – chunks with distinct names are only run once, important if certain chunks take some time to execute. echo=\u0026lt;TRUE|FALSE\u0026gt; - Determines whether the code is visible or hidden from the typeset file. If you output file is a data analysis report you may not wish to expose lengthy code chunks as these may disrupt the discursive text that appears outside of the code chunks. eval=\u0026lt;TRUE|FALSE\u0026gt; - Determines whether the code is evaluated (executed). This is useful if you wish to present some code in your document for display purposes. cache=\u0026lt;TRUE|FALSE\u0026gt; - Determines where the results from the code chunk are cached.  As part of the homework from this session you will do some more research on R Markdown. It is worth in advance downloading RStudio’s cheatsheets, which provide comprehensive details on how to configure R Markdown documents:\n Open RStudio and select Help \u0026gt; Cheatsheets \u0026gt; R Markdown Cheat Sheet | R Markdown Reference Guide   R Scripts Whilst there are obvious benefits to working in R Markdown documents when doing data analysis, there may be occasions where working in an script is preferable. Scripts are plain text files with the extension .R. Comments – text that are not executed as code – are denoted with the # symbol.\nI tend to use R Scripts for writing discrete but substantial code blocks that are to be executed. For example, I might generate a set of functions that relate to a particular use case and bundle these together in an R script. These then might be referred to in a data analysis from an .Rmd, which makes various use of these functions in a similar way as one might import a package. Below is an example script that we will encounter later in the module when creating flow visualizations in R very similar to those that appear in Jo Wood’s TEDx talk. This script is saved with the fie name bezier_path.R. If it were stored in a sensible location, like a project’s code folder, it could be called from an R Markdown file with source(./code/bezier_path). R Scripts can be edited in the same way as R Markdown files in RStudio, via the Code Editor pane.\n# bezier_path.R # # Author: Roger Beecham ############################################################################## #\u0026#39; Functions for generating input data for asymmetric bezier curve for OD data, #\u0026#39; such that the origin is straight and destination curve. The retuned tibble #\u0026#39; is passed to geom_bezier().Parametrtisation follows that published in #\u0026#39; Wood et al. 2011. doi: 10.3138/carto.46.4.239. #\u0026#39; @param data A df with origin and destination pairs representing 2D locations #\u0026#39; (o_east, o_north, d_east, d_north) in cartesian (OSGB) space. #\u0026#39; @param degrees For converting to radians. #\u0026#39; @return A tibble of coordinate pairs representing asymmetric curve get_trajectory \u0026lt;- function(data) { o_east=data$o_east o_north=data$o_north d_east=data$d_east d_north=data$d_north od_pair=data$od_pair curve_angle=get_radians(-90) east=(o_east-d_east)/6 north=(o_north-d_north)/6 c_east=d_east + east*cos(curve_angle) - north*sin(curve_angle) c_north=d_north + north*cos(curve_angle) + east*sin(curve_angle) d \u0026lt;- tibble( x=c(o_east,c_east,d_east), y=c(o_north,c_north,d_north), od_pair=od_pair ) } # Convert degrees to radians. get_radians \u0026lt;- function(degrees) { (degrees * pi) / (180) } To an extent R Scripts are more straightforward than R Markdown files in that you don’t have to worry about configuring code chunks. They are really useful for quickly developing bits of code. This can be achieved by highlighting over the code that you wish to execute and clicking the Run icon at the top of the Code Editor pane or by typing ctrl + rtn on Windows, ⌘ + rtn on macOS\n Create an RStudio Project  Figure 5: Creating an RStudio Project  Throughout this module we will use project-oriented workflows. This is where all files pertaining to a data analysis – data, code and outputs – are organised from a single root folder and where file path discipline is used such that all paths are relative to the project’s root folder (see Bryan \u0026amp; Hester 2020). You can imagine that this sort of self-contained project set-up is necessary for achieving reproducibility of your research. They allow anyone to take a project and run it on their own machines without having to make any adjustments.\nYou might have noticed that when you open RStudio it automatically points to a working directory, likely the home folder for your local machine, denoted with ~/ in the Console. RStudio will by default save any outputs to this folder and will also expect any data you use to be saved there. Clearly if you want to incorporate neat, self-contained project workflows then you will want to organise your work from a dedicated project folder rather than the default home folder for your machine. This can be achieved with the setwd(\u0026lt;path-to-your-project\u0026gt;) function. The problem with doing this is that you insert a path which cannot be understood outside of your local machine at the time it was created. This is a real pain. It makes simple things like moving projects around on your machine an arduous task and most importantly it hinders reproducibility if others are to reuse your work.\nRStudio Projects are a really excellent feature of the RStudio IDE that resolve these problems. Whenever you load up an RStudio Project, R starts up and the working directory is automatically set to the project’s root folder. If you were to move the project elsewhere on your machine, or to another machine, a new root is automatically generated – so RStudio projects ensure that relative paths work.\nLet’s create a new Project for this module:\n Select File \u0026gt; New Project \u0026gt; New Directory. Browse to a sensible location and give the project a suitable name. Then click Create Project.  You will notice that the top of the Console window now indicates the root for this new project, in my case ~projects/vis-for-gds.\n In the root of your project, create folders called reports, code, data, figures. Save this session’s  01-template.Rmd file to the reports folder.  Your project’s folder structure should now look like this:\nvis-for-gds\\ vis-for-gds.Rproj code\\ data\\ figures\\ reports\\ 01-template.Rmd   Conclusions Visual data analysis approaches are necessary for exploring complex patterns in data and to make and communicate claims under uncertainty. This is especially true of Geographic Data Science applications, where:\n datasets are being repurposed for social and natural sciences research for the first time; contain complex structure and geo-spatial relations that cannot be easily captured by statistical summaries alone; and, consequently, where the types of questions that can be asked and the techniques deployed to answer them cannot be easily specified in advance.  In this module we will demonstrate this as we explore (Session 4 and 5), model under uncertainty (Session 6 and 7) and communicate (Session 7 and 8) with various social science datasets. We will work with both new, large-scale behavioural datasets, as well as more traditional, administrative datasets located within various social science domains: Political Science, Crime Science, Urban and Transport Planning.\nWe will do so using the statistical programming environment R, which along with Python, is the programming environment for modern data analysis. We will make use of various tools, software libraries that form part of the R ecosystem – the tidyverse for doing modern data science and R Markdown for authoring reproducible research projects.\n References Anscombe, F. 1973. “Graphs in Statistical Analysis.” American Statistician 27 (1): 17–21. doi:10.1080/00031305.1973.10478966.\n Arribas-Bel, Dani, and Jon Reades. 2018. “Geography and Computers: Past, Present, and Future.” Geography Compass 12 (10): e12403. doi:10.1111/gec3.12403.\n Beecham, Roger, and Jo Wood. 2014. “Exploring Gendered Cycling Behaviours Within a Large-Scale Behavioural Data-Set.” Transportation Planning and Technology 37 (1). Taylor \u0026amp; Francis: 83–97.\n Brunsdon, Chris, and Alexis Comber. 2020. “Opening Practice: Supporting Reproducibility and Critical Spatial Data Science.” Journal of Geographical Systems. doi:10.1007/s10109-020-00334-2.\n Donoho, David. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (6): 745–66. doi:10.1080/10618600.2017.1384734.\n Friendly, Michael. 2007. “A Brief History of Data Visualization.” In Handbook of Computational Statistics: Data Visualization, edited by C. Chen, W. Härdle, and A Unwin, III:1–34. Heidelberg: Springer-Verlag. http://datavis.ca/papers/hbook.pdf.\n Matejka, Justin, and George Fitzmaurice. 2017. “Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics Through Simulated Annealing.” In, 1290–4. CHI ’17. New York, NY, USA: Association for Computing Machinery. doi:10.1145/3025453.3025912.\n Singleton, Alex, and Dani Arribas-Bel. 2019. “Geographic Data Science.” Geographical Analysis. doi:10.1111/gean.12194.\n    For an excellent precis and interpretation of this for geographers, see Arribas-Bel and Reades (2018).↩︎\n Although not the case when actually reading Donoho (2017).↩︎\n Checkout Matejka and Fitzmaurice (2017)’s Same Stats, Different Graphs paper for a fun take one this.↩︎\n   ","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en-uk","lastmod":1606644925,"objectID":"ac47977a15b3902ca3402f61e5bf9df2","permalink":"/class/01-class/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/class/01-class/","section":"class","summary":"Contents  Session outcomes Welcome to Visualization for Geographic Data Science Why vis-for-gds? Geographic Data Science Geographic Data Science and Visualization  What vis-for-gds? How vis-for-gds? R for modern data analysis Rmarkdown for reproducible research  Getting started with R and RStudio Install R and RStudio Open the RStudio IDE Compute in the console Install some packages Experiment with R Markdown R Scripts Create an RStudio Project  Conclusions References   Session outcomes By the end of this session you should gain the following knowledge:","tags":null,"title":"Introduction: Vis for Geographic Data Science","type":"docs"},{"authors":null,"categories":null,"content":"  Contents  Session Outcomes Introduction Task 1: Read and complete BasicBasics Task 2: Watch Thomas Mock’s Introduction to Tidy Statistics   Session Outcomes By the end of this homework session you should:\n   Be familiar with RStudio, R Markdown and R Projects. Know how to install and enable R packages and query package documentation. Understand the purpose R Markdown files and their anatomy. Know how to create R Projects and understand their benefits     Introduction For each session I will post some additional exercises that relate to the practical learning you will be doing in-class. You will be populating R Markdown files as you complete the exercises. The exercises will vary in detail and challenge but you should try your best effort to complete them as they will form a portfolio of work that will contribute 25% of the module mark. Doing these exercises will also help you build the skills necessary to complete the project coursework that forms the main contribution to your module mark (75%).\nThere has been a lot to take in this session. There are no formal exercises to complete as part of this session’s homework. Instead we want you to really try to solidify your understanding of the core technology for the module.\n Task 1: Read and complete BasicBasics R-Ladies is an excellent initiative and the R-Ladies Sydney BasicBasics pages are really great for introducing the R and RStudio ecosystem, and a little of the tidyverse.\nRead and work your way through:\n BasicBasics 1 BasicBasics 2 BasicBasics 3  This repeats some of the work that you have done in class this session. If you’re entirely new to R programming, this repetition is probably useful (and necessary) at this stage.\n Task 2: Watch Thomas Mock’s Introduction to Tidy Statistics Thomas Mock’s Gentle Introduction to Tidy Statistics is pitched those coming to R after previously working with point-and-click interfaces (SPSS, STATA, ArcGIS). I strongly recommend that you watch this now as you start to embark on the “pain” of doing data analysis programmatically.\n   ","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en-uk","lastmod":1622505600,"objectID":"7f1f54adb3ac450ad99be6af1a9f99df","permalink":"/homework/01-homework/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/homework/01-homework/","section":"homework","summary":"Contents  Session Outcomes Introduction Task 1: Read and complete BasicBasics Task 2: Watch Thomas Mock’s Introduction to Tidy Statistics   Session Outcomes By the end of this homework session you should:\n   Be familiar with RStudio, R Markdown and R Projects. Know how to install and enable R packages and query package documentation. Understand the purpose R Markdown files and their anatomy. Know how to create R Projects and understand their benefits     Introduction For each session I will post some additional exercises that relate to the practical learning you will be doing in-class.","tags":null,"title":"Introduction","type":"docs"},{"authors":null,"categories":null,"content":"  Defining (Geographic) Data science  Dani Arribas-Bel and Jon Reades, “Geography and Computers: Past, Present, and Future,” Geography Compass 12, no. 10 (2018): e12403, doi:10.1111/gec3.12403.\n Alex Singleton and Dani Arribas-Bel, “Geographic Data Science,” Geographical Analysis (2019), doi:10.1111/gean.12194.\n   Reproducibility agenda  Chris Brunsdon and Alexis Comber, “Opening Practice: Supporting Reproducibility and Critical Spatial Data Science,” Journal of Geographical Systems (2020), doi:10.1007/s10109-020-00334-2.   Introducing R for Data Science  Hadley Wickham and Garrett Grolemund, R for Data Science: Import, Tidy, Transform, Visualize, and Model Data (Sebastopol, California: O’Reilly Media, 2017), http://r4ds.had.co.nz/.\n– Chapters 1.\nFree online.   ","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en-uk","lastmod":1606644925,"objectID":"57c6d6996ee98125a5375a3865ff4c4c","permalink":"/reading/01-reading/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/reading/01-reading/","section":"reading","summary":"Defining (Geographic) Data science  Dani Arribas-Bel and Jon Reades, “Geography and Computers: Past, Present, and Future,” Geography Compass 12, no. 10 (2018): e12403, doi:10.1111/gec3.12403.\n Alex Singleton and Dani Arribas-Bel, “Geographic Data Science,” Geographical Analysis (2019), doi:10.1111/gean.12194.\n   Reproducibility agenda  Chris Brunsdon and Alexis Comber, “Opening Practice: Supporting Reproducibility and Critical Spatial Data Science,” Journal of Geographical Systems (2020), doi:10.1007/s10109-020-00334-2.   Introducing R for Data Science  Hadley Wickham and Garrett Grolemund, R for Data Science: Import, Tidy, Transform, Visualize, and Model Data (Sebastopol, California: O’Reilly Media, 2017), http://r4ds.","tags":null,"title":"Introduction","type":"docs"},{"authors":null,"categories":null,"content":"  Tidy Data  H. Wickham, “Tidy Data,” Journal of Statistical Software 59, no. 10 (2014): 1–23.   R for Data Science  Hadley Wickham and Garrett Grolemund, R for Data Science: Import, Tidy, Transform, Visualize, and Model Data (Sebastopol, California: O’Reilly Media, 2017), http://r4ds.had.co.nz/.\n– Chapters 2-12.\nFree online.   bikedata  Mark Padgham and Richard Ellison, “Bikedata,” The Journal of Open Source Software 2, no. 20 (December 2017), doi:10.21105/joss.00471.   ","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en-uk","lastmod":1622505600,"objectID":"def8daf7a7859ddd197971151353ea19","permalink":"/reading/02-reading/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/reading/02-reading/","section":"reading","summary":"Tidy Data  H. Wickham, “Tidy Data,” Journal of Statistical Software 59, no. 10 (2014): 1–23.   R for Data Science  Hadley Wickham and Garrett Grolemund, R for Data Science: Import, Tidy, Transform, Visualize, and Model Data (Sebastopol, California: O’Reilly Media, 2017), http://r4ds.had.co.nz/.\n– Chapters 2-12.\nFree online.   bikedata  Mark Padgham and Richard Ellison, “Bikedata,” The Journal of Open Source Software 2, no. 20 (December 2017), doi:10.","tags":null,"title":"Data Fundamentals","type":"docs"},{"authors":null,"categories":null,"content":"  Contents  Session Outcomes Introduction Task 1: Describe data Task 2: Diagnose data UK General Election Results 2019 UK General Election Results 2017 and 2019  Task 3: Fix data Task 4: Compute from data   Session Outcomes By the end of this homework session you should be able to:\n   Describe data according to its structure and contents. Calculate descriptive summaries over datasets. Apply high-level functions in dplyr and tidyr for working with data.     Introduction This homework requires you to apply the concepts and skills developed in the class session on data fundamentals. Do complete each of the tasks and be sure to save your work as this homework will be submitted as part of your portfolio for Assignment 1.\n Task 1: Describe data Complete the data description table below identifying the measurement level of each variable in the (fictional) New York bikeshare stations dataset below.\n  Variable name Variable value Measurement level    name “Central Park”   capacity 80   rank_capacity 45   date_opened “2014-05-23”   longitude -74.00149746   latitude 40.74177603      Task 2: Diagnose data Below are two different tables with results from UK General Elections. We will be working with these data in the next session. Identify whether or not each is in tidy format [H. Wickham “Tidy Data,” Journal of Statistical Software 59, no. 10 (2014): 1–23. If they are not, provide a layout for a tidy version. No need to use code here, just edit the markdown table. If you’re struggling to work out how to organise markdown tables, you may wish to use this tables generator.\nUK General Election Results 2019   party percent_vote num_mps    Conservative 43.6 365  Labour 32.2 202  Scottish National Party 3.9 48  Liberal Democrats 11.6 11  Democratic Union Party 0.8 9     UK General Election Results 2017 and 2019   party percent_vote_2017 num_mps_2017 percent_vote_2019 num_mps_2019    Conservative 42.4 317 43.6 365  Labour 40.0 262 32.2 202  Scottish National Party 3.0 35 3.9 48  Liberal Democrats 7.4 12 11.6 11  Democratic Union Party 0.9 10 0.8 8      Task 3: Fix data In the  02-template.Rmd file for this session I have provided links to two derived tables (ny_spread_columns and ny_spread_rows) from the New York bikeshare trip data that are not in a tidy format.\nUsing functions from dplyr and tidyr reorganise these data so that they are conform to the rules of tidy data Wickham, “Tidy Data.”.\nA candidate tidy organisation of the data is below. Each row is an origin-destination pair for a weekday or weekend, and each variable describes:\n o_station: station id of the origin station d_station: station ide of the destination station wkday: identifies whether the OD pair describes weekday or weekend ny_trips count: count of trips recorded for that observation (OD pair and weekday/weekend) dist: total distance (cumulative) in kms of all trips recorded for that observation (OD pair and weekday/weekend) duration: total duration in minutes (cumulative) of all trips recorded for that observation (OD pair and weekday/weekend)  ## # A tibble: 386,762 x 6 ## o_station d_station wkday count dist duration ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 72 116 weekend 1 1.15 18.2 ## 2 72 127 weekend 10 18.0 339. ## 3 72 128 weekend 2 3.18 69.6 ## 4 72 146 weekend 12 27.6 402. ## 5 72 151 weekend 2 2.87 54.9 ## 6 72 161 weekend 2 2.52 64.8 ## 7 72 164 weekend 5 13.3 73.3 ## 8 72 167 weekend 1 2.07 17.2 ## 9 72 168 weekend 2 1.70 42.7 ## 10 72 173 weekend 9 9.59 194. ## # … with 386,752 more rows  Task 4: Compute from data Using dplyr functions, calculate the average distance, duration and speed of trips occuring for each observation. Print out to the Console the top 10 most heavily cycled OD pairs (and their associated summary statistics) separately for weekdays and weekends. You may wish to join on your ny_stations table in order to fetch the station names corresponding to the origin and destination stations.\n ","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en-uk","lastmod":1622505600,"objectID":"6064feb04ebe813d60f6e6738c944b18","permalink":"/homework/02-homework/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/homework/02-homework/","section":"homework","summary":"Contents  Session Outcomes Introduction Task 1: Describe data Task 2: Diagnose data UK General Election Results 2019 UK General Election Results 2017 and 2019  Task 3: Fix data Task 4: Compute from data   Session Outcomes By the end of this homework session you should be able to:\n   Describe data according to its structure and contents. Calculate descriptive summaries over datasets. Apply high-level functions in dplyr and tidyr for working with data.","tags":null,"title":"Data fundamentals","type":"docs"},{"authors":null,"categories":null,"content":"  Visualization concepts  Tamara Munzner, Visualization Analysis and Design, AK Peters Visualization Series (Boca Raton, FL: CRC Press, 2014).\n– Chapters 1,2,5.\n   Data Visualization in ggplot2  Kieran Healy, Data Visualization: A Practical Introduction (Princeton: Princeton University Press, 2018), http://socviz.co/.\n– Chapters 1,3,4,7.\nFree online.\n Hadley Wickham and Garrett Grolemund, R for Data Science: Import, Tidy, Transform, Visualize, and Model Data (Sebastopol, California: O’Reilly Media, 2017), http://r4ds.had.co.nz/.\n– Chapters 3.\nFree online.\n   ","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en-uk","lastmod":1622505600,"objectID":"39100b29a777d527fde18e1fccc18a57","permalink":"/reading/03-reading/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/reading/03-reading/","section":"reading","summary":"Visualization concepts  Tamara Munzner, Visualization Analysis and Design, AK Peters Visualization Series (Boca Raton, FL: CRC Press, 2014).\n– Chapters 1,2,5.\n   Data Visualization in ggplot2  Kieran Healy, Data Visualization: A Practical Introduction (Princeton: Princeton University Press, 2018), http://socviz.co/.\n– Chapters 1,3,4,7.\nFree online.\n Hadley Wickham and Garrett Grolemund, R for Data Science: Import, Tidy, Transform, Visualize, and Model Data (Sebastopol, California: O’Reilly Media, 2017), http://r4ds.","tags":null,"title":"Visualization Fundamentals","type":"docs"},{"authors":null,"categories":null,"content":"  Contents  Session Outcomes Introduction Task 1: Describe and evaluate Task 2: Reproduce Histograms faceted by region Scatterplots with multiple encodings  Task 3: Create a map References   Session Outcomes By the end of this homework session you should be able to:\n   Formally describe data graphics according to their visual encoding. Reproduce graphics by writing ggplot2 specifications. Combine dplyr and conditional statements to prep data for plotting.     Introduction This homework requires you to apply the concepts and skills developed in the class session on visualization fundamentals. Do complete each of the tasks and be sure to save your work as this homework will be submitted as part of your portfolio for Assignment 1.\n Task 1: Describe and evaluate  Figure 1: Map of Butler Con-Lab Swing in 2019 General Election.  Complete the description table below identifying each data item that is encoded along with its measurement level, visual mark and visual channel** and the effectiveness rank, according to Munzner (2014), of this encoding.\n  Data item Measurement level Visual mark Visual channel Rank    location      ... ... ... ... ...  ... ... ... ... ...  ... ... ... ... ...  ... ... ... ... ...  ... ... ... ... ...     Task 2: Reproduce Histograms faceted by region Write some code for reproducing something similar to the graphic below – a set of histograms of the Swing variable, faceted by region. Place your code into the  03-template.Rmd file for this session.\n####################### # Enter your code in the chunk provided. ######################  Figure 2: Histograms of Swing variable, grouped by region.   Scatterplots with multiple encodings Write some code for reproducing something similar to the graphic below – a scatterplot comparing 2017 and 2019 vote shares for Labour. Be sure to include every encoding (shape and alpha) as it appears in the graphic. Hint: you may need to use a conditional statement to generate a variable for emphasising constituencies that flipped parties between 2017 and 2019. Place your code into the  03-template.Rmd file for this session.\n####################### # Enter your code in the chunk provided. ######################  Figure 3: Plots of 2019 versus 2017 vote shares.    Task 3: Create a map Write some code for reproducing something similar to the graphic below – a map of the estimated Leave:Remain vote margin by Parliamentary Constituency. Note that I am using a diverging colour scheme here to distinguish whether the Constituency was majority Leave and Remain – brown or green – and also the size of that majority – the darker the colour, the larger the majority.\nPlace your code into the  03-template.Rmd file for this session.\n####################### # Enter your code in the chunk provided. ######################  Figure 4: Map of 2016 EU Referendum vote, estimated by Parliamentary Constituency in GB.   References Munzner, Tamara. 2014. Visualization Analysis and Design. AK Peters Visualization Series. Boca Raton, FL: CRC Press.\n   ","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en-uk","lastmod":1622505600,"objectID":"be8ff83ad863abfd4cb1d24145260a2a","permalink":"/homework/03-homework/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/homework/03-homework/","section":"homework","summary":"Contents  Session Outcomes Introduction Task 1: Describe and evaluate Task 2: Reproduce Histograms faceted by region Scatterplots with multiple encodings  Task 3: Create a map References   Session Outcomes By the end of this homework session you should be able to:\n   Formally describe data graphics according to their visual encoding. Reproduce graphics by writing ggplot2 specifications. Combine dplyr and conditional statements to prep data for plotting.","tags":null,"title":"Visualization fundamentals","type":"docs"},{"authors":null,"categories":null,"content":"  Contents  Session Outcomes Introduction Visualization design challenge References   Session Outcomes By the end of this homework session you should be able to:\n          Introduction This homework requires you to apply the concepts and skills developed in the class session on exploratory data anlsysis. Do complete the task and be sure to save your work as this homework will be submitted as part of your portfolio for Assignment 1.\n Visualization design challenge  Figure 1: Boxplots of casualty age by vehicle type and class.  This week’s homework is a single visualization design task. Please do not spend too long on this. The aim of is to get you thinking a little about the concepts introduced in the session rather than to overly burden you with additional work.\nIn the session we discussed visual approaches to summarising within-variable variation in a dataset and later the use of layout and colour for supporting comparison across categories. In Figure 1 above, boxplots are used to summarise the age distribution of those involved in pedestrian road crashes, comparing the ages of the injured pedestrians, drivers and how this varies by the vehicle types involved.\nYour task is to generate a single data graphic (I would class Figure 1 as a single graphic) to summarise how casualty age varies by some other interesting combination of categorical variable. For example, you may wish to explore how casualty age varies by injury severity.\nOnce you have generated the graphic, save it as a .png file and insert a link to it in the associated code block in the  04-template.Rmd. Next, complete the table that asks you to describe your graphic according to its visual encoding. You may also wish to note how comparison is being enable – via juxtaposition, superposition and/or direct encoding (Gleicher and Roberts 2011).\n####################### # Enter your code in the chunk provided. ######################   Data item Measurement level Visual mark Visual channel    location     ... ... ... ...  ... ... ... ...  ... ... ... ...  ... ... ... ...  ... ... ... ...     References Gleicher, Albers, M., and J. Roberts. 2011. “Visual Comparison for Information Visualization. Information Visualization.” Information Visualization 10 (4): 289–309.\n   ","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en-uk","lastmod":1622505600,"objectID":"aca8533a03caf8f8cc9f42aab330b7ac","permalink":"/homework/04-homework/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/homework/04-homework/","section":"homework","summary":"Contents  Session Outcomes Introduction Visualization design challenge References   Session Outcomes By the end of this homework session you should be able to:\n          Introduction This homework requires you to apply the concepts and skills developed in the class session on exploratory data anlsysis. Do complete the task and be sure to save your work as this homework will be submitted as part of your portfolio for Assignment 1.","tags":null,"title":"Exploratory data analysis","type":"docs"},{"authors":null,"categories":null,"content":"   Contents  Introduction Concepts Exploratory data analysis and statistical graphics Plots for continuous variables Plots for categorical variables Supporting comparison with layout and colour  Techniques Import Explore  Conclusions References   By the end of this session you should gain the following knowledge:\n   An appreciation of the exploratory data analysis (EDA) worklow. The main chart types (or idioms Munzner 2014) for summarising variation within- and between- variables. The three strategies that can be used for supporting comparison in EDA: juxtaposition, superposition and direct encoding (Gleicher and Roberts 2011)    By the end of this session you should gain the following practical skills:\n   Write ggplot2 specifications that use colour and layout to support comparison.    Introduction Exploratory Data Analysis (EDA) is a data-driven approach to analysis which aims to expose the properties and structure of a dataset, and from here suggest directions for analytic inquiry. In an EDA, relationships are quickly inferred, anomalies labelled, assumptions tested and new hypotheses and ideas are formulated. EDA relies heavily on visual approaches to analysis – it is common to generate many dozens of (often throwaway) data graphics.\nThis session will demonstrate how the concepts and principles introduced previously – of data types and their visual encoding – can be applied to support EDA. It will do so by analysing STATS19, a dataset containing detailed information on every reported road traffic crash in Great Britain that resulted in personal injury. STATS19 is highly detailed, with many categorical variables. This session will start by revisiting commonly used chart idioms (Munzner 2014) for summarising within-variable variation and between-variable co-variation in a dataset. It will then focus more directly on the STATS19 case, and how detailed comparison across many categorical variables can be effected using colour, layout and statistical computation.\n Concepts Exploratory data analysis and statistical graphics  The simple graph has brought more information to the data analyst’s mind than any other device.\nJohn Tukey\n In an Exploratory Data Analysis (EDA), graphical and statistical summaries are variously used to build knowledge and understanding of a dataset. The goal of EDA is to infer relationships, identify anomalies and test new ideas and hypotheses: it is a knowlegde-building activity. Rather than a formal set of techniques, EDA should be considered an approach to analysis. It aims to reveal the underlying properties of variables in a dataset (central tendency and dispersion) and their structure (how variables relate to one another) and from there formulate hypotheses to be investigated.\nWickham and Grolemund (2017) identify two main questions that an EDA should address:\nWhat type of variation occurs within variables of a dataset? What type of covariation occurs between variables of a dataset?    Table 1: Statistics and data graphics that can be used to summarise variation and covariation by variable type.    Measurement  Statistics  Chart idiom     Within-variable variation    Nominal  mode | entropy  bar charts, dot plots …    Ordinal  median | percentile  bar charts, dot plots …    Continuous  mean | variance  histograms, box plots, density plots …   Between-variable variation    Nominal  contingency tables  mosaic/spine plots …    Ordinal  rank correlation  slope/bump charts …    Continuous  correlation  scatterplots, parallel coordinate plots …     You will recall from early statistics classes, usually under the theme Descriptive Statistics, that when summarising within-variable variation, we are interested in a variable’s spread or dispersion and its location within this distribution (central tendency). Different statistics can be applied, but a familiar distinction is between measures of central tendency that are or are not robust to outliers (e.g. mode and median versus mean). Correlation statistics are most obviously applied when studying between-variable covariation, but other statistics, such as odds ratios and chi-square tests are commonly used when studying covariation in contingency tables.\nDecisions around which statistic to use depend on a variable’s measurement-level (e.g. Table 1). As demonstrated by Anscombe’s quartet, statistical summaries can hide important structure, or assume structure that doesn’t exist. None of the measures of central tendency in Table 1 would expose whether a variable for instance is multi-modal and only when studying all measures of central tendency and dispersion together might it be possible to guess at the presence of outliers that could undermine statistical assumptions. It is for this reason that data visualization is seen as intrinsic to EDA (Tukey 1977).\n Plots for continuous variables Within-variable variation: histograms, desity plots, boxplots  Figure 1: Univariate plots of dispersion.  Figure 1 presents statistical graphics that are commonly used to display the distribution of continuous variables measured on a ratio or interval scale – in this instance the age of casualty for a random sample of Stast19 road crashes (casualty_age).\nIn the bottom row is a strip-plot. Every observation is displayed as a dot and mapped to x-position, with transparency and a random vertical perturbation applied to resolve occlusion due to overlapping observations. Although strip-plots scale poorly, the advantage is that all observations are displayed without the need to impose an aggregation. It is possible to visually identify the location of the distribution – denser dots towards 20-25 age range – but also that there is quite a degree of spread across the age values.\nHistograms were used in the previous session when analysing the 2019 UK General Election dataset. Histograms partition continuous variables into equal-range bins and observations in each bin are counted. These counts are encoded on an aligned scale using bar height. Increasing the size of the bins increases the resolution of the graphical summary. If reasonable decisions are made around choice of bin, histograms give distributions a “shape” that is expressive. It is easy to identify the location of a distribution and, in using length on aligned scale to encode frequency, estimate relative densities between different parts of the distribution. Different from the strip-plot, the histogram allows us to intuit that despite the heavy spread, the distribution of casualty_age is right-skewed, and we’d expect this given the location of the mean (36 years) relative to the median (33 years).\nA problem with histograms is the potential for discontinuities and artificial edge-effects around the bins. Density plots overcome this and can be thought of as smoothed histograms. They use kernel density estimation (KDE) to show the probability density function of a variable – the relative amount of probability attached to each value of casualty_age. As with histograms, there are decisions to be made around how data are partitioned into bins (or KDE bandwidths). From glancing at the density plots an overall “shape” to the distribution can be immediately derived. It is also possible to infer statistical properties: the mode of the distribution (the highest density), the mean (by visual averaging) and median (finding the midpoint of the area under the curve). Density plots are better suited to datasets that contain a reasonably large number of observations and due to the smoothing function it is possible to generate a density plot that suggests nonsensical values (e.g. negative ages in this case if I hadn’t censored the plot range).\nFinally, boxplots, or box and whisker plots (McGill and Larsen 1978), encode the statistical properties that we infer from strip-plots, histograms and density plots directly. The box is the interquartile range (\\(IQR\\)) of the casualty_age variable, the vertical line splitting the box is the median, and the whiskers are placed at observations \\(\\leq 1.5*IQR\\). Whilst we lose information around the shape of a distribution, box-plots are space-efficient and useful for comparing many distributions at once.\n  This discussion was a little prosaic – we haven’t made too many observations to advance our knowledge of the dataset. I was surprised at the low average age of road casualties and so quickly explored the distribution of casualty_age conditioning on another variable and differentiating between variable values using colour. Figure 2 displays boxplots of the location and spread in casualty_age by vehicle type (left) and also by casualty_class for all crashes involving pedestrians. A noteworthy pattern is that riders of bicycles and motorcycles tend to be younger than the pedestrians they are contacting with, whereas for buses, taxis, HGVs and cars the reverse is true. Pedestrians involved in crashes with cars are especially skewed towards the younger ages.\n Figure 2: Boxplots of casualty age by vehicle type and class.      Between-variable covariation: scatterplots The previous session included several scatterplots for displaying association between two quantitative variables. Scatterplots are used to check whether the association between variables is linear, as in Anscombe’s quartet, but also to make inferences about the direction and intensity of linear correlation between variables – the extent to which values in one variable depend on the values of another – and also around the nature of variation between variables – the extent to which variation in one variable depends on another (heteroscedasticity). Although other chart idioms (Munzner 2014) for displaying bivariate data exist, empirical studies in Information Visualization have demonstrated that aggregate correlation statistics can be reliably estimated from scatterplots (Rensink and Baldridge 2010; Harrison et al. 2014; Kay and Heer 2016; Correll and Heer 2017a).\nThere are few variables in the STATS19 dataset measured on a continuous scale, but based on the analysis above, we may wish to explore more directly whether an association exists between the age of pedestrians and drivers for pedestrian-vehicle crashes. The scatterplots in Figure 3 show that no obvious linear association exists.\n Figure 3: Scatterplots of pedestrian age by driver age and grouped by vehicle type.    It is common in an EDA to quickly compare associations between many quantitive variables in a dataset using scatterplot matrices or alternatively parallel coordinate plots. We will use both in sessions 6 and 7 when building models that attempt to formally structure and explain between-variable covariation.     Plots for categorical variables Within-variable variation: bars, dotplots, heatmaps With categorical variables we are interested in exploring how relative frequencies distribute across the variable’s categories. Bar charts are most commonly used. As established in the previous session, length is an efficient visual channel for encoding quantities, counts in this case. Often it is useful to flip bar charts on their side so that category labels are arranged horizontally for ease of reading and, unless there is a natural ordering to categories, arrange the bars in descending order based on their frequency, as in Figure 4.\n Figure 4: Bars displaying crash frequencies by vehicle type.  Bar charts are effective at conveying frequencies where the number of categories is reasonably small. For summarising frequencies across many categories, alternatives chart types that minimise non-data-ink, such as Cleveland dot plots and heatmaps may be appropriate. The left plot in Figure 5 displays crash counts for boroughs in London, ordered by crash frequency and grouped by whether boroughs are in inner- or outer- London. To the right is a heatmap with the same ordering and grouping of boroughs applied to the rows and with columns coloured according to crash frequencies by time period and further grouped by day of week. From scanning the graphic we can make several observations: that reported crashes tend to occur in the midday or evening peak (the middle two columns of a day are typically darker); that in relative terms there are more crashes recorded in outer London boroughs than inner London boroughs during the middle of the day on weekends (salient dark strips corresponding with midday Saturday and Sunday).\n Figure 5: Cleveland dot plots and heatmaps summarising crash frequencies by London borough and period of day.    Remembering Munzner (2014)’s ordering of visual channels, Cleveland dot plots, which use position on an aligned scale, are far better at encoding quantities than heatmaps, which use colour value. Stevens’s power estimates for the perception of changes in lightness were (n~0.5). Despite this, I think the heatmap in Figure 5 is a successful data graphic. It displays 924 values in a reasonably compact space with cells arranged to emphasise comparison. This illustrates the point raised in the previous session around the trade-offs that must be negotiated when designing graphics for analysis. In this case we trade encoding effectiveness for data density.    Between-variable covariation: contingency tables, standardised bars, mosaic plots To study the association between two categorical variables, contingency tables are routinely used. Each cell in a contingency table is the joint frequency of two category outcomes occurring together. The contingency table below (Table 2) looks at how crashes by vehicle type co-vary with injury severity. The question being posed is: are crashes involving certain vehicle associated with more/less severe injury than others? The table presents counts by injury severity for pedestrians only; so these are pedestrians being hit by cars, vans, bicycles etc. Injury severity is either slight or serious/fatal (KSI).\n  Table 2: Pedestrian casualties by vehicle involved and injury severity.    Vehicle type  KSI  Slight  Row Total      Car  42305  137924  180229    Van  3786  11422  15208    Taxi  2580  9188  11768    Bus  2951  8425  11376    Motorcycle  2137  7102  9239    HGV  2030  3195  5225    Other  1096  3302  4398    Bicycle  1033  3184  4217    Column Total  57918  183742  NA     To display these quantities graphically we could update our earlier bar chart to create stacked bars that use colour to distinguish injury severity (Figure 6). Injury severity is an ordinal variable and the choice of colour in Figure 6 reflects this order – dark red for KSI, light red for slight.\n Figure 6: Bars (and Mosaic plot) displaying association between vehicle type and injury severity.  Cars are by far the dominant travel mode, causing the largest number of slight and serious injuries to pedestrians. Whether or not cars result in more severe injury rates than other travel modes is not clear from the left-most chart. Length effectively encodes absolute crash counts but relative comparison of injury severity between vehicle types is challenging. The graphic can be reconfigured to emphasise relative comparison – fixing the absolute length of bars and splitting the bars according to proportional injury severity (middle). Now we see that relative injury severity of pedestrians – KSI as a proportion of all crashes – varies slightly between modes (c.22% Taxis - c.26% Buses) except for HGVs where around 40% of crashes result in a serious injury or fatality to the pedestrian. However, we lose a sense of the absolute numbers involved.\nThis can be problematic in an EDA when absolute and relative frequencies over many category combinations are made. For combinations in a contingency table that are rare, for example bicycle-pedestrian casualties, it may only take a small number observations in a particular direction to change the KSI rate. Since proportional summaries are agnostic to sample size, they can induce false discoveries, overemphasising patterns that may be unlikely to replicate. It is sometimes desirable, then, to update standardised bar charts so that they are weighted by frequency – to make more visually salient those categories that occur more often and visually downweight those that occur less often. This is possible using mosaic plots (Friendly 1992). Bar widths and heights are allowed to vary; bar area is proportional to absolute number of observations and bars are further subdivided for relative comparison across category values. Mosaic plots are useful tools for exploratory analysis. That they are space-efficient and regularly sized (squarified) means that that they can be laid out for comparison.\n  The Mosaic plot in Figure 6 was generated using the ggmosaic package, an extension to ggplot2. We won’t cover the details of how to implement Mosaic plots in this session. Should you wish to apply them as part of the data challenge element of the homework for this session, ggmosaic’s documentation pages are an excellent resource.    Deriving effect sizes from contingency tables When generating the contingency table on which these plots are based, there are some implied questions:\n Does casualty severity vary depending on the type of vehicle involved in the crash? For which vehicle types is injury severity the highest or lowest?  An imbalance is clearly to be expected, but we may start by assuming that injury severity rates, the proportion of pedestrian injuries that are KSI, is independent of vehicle type and look to quantify and locate any imbalance in these proportions. This happens automatically when comparing the relative widths of the dark red bars in Figure 6. Annotating with an expectation – e.g. the injury severity rate for all pedestrian casualties (middle plot) – helps to further locate differences from expectation for specific vehicles.\nEffect sizes estimates could also be computed directly using risk ratios (RR) – comparing the observed severity rates by vehicle type against an expectation of independence of severity rates by vehicle type. From this, we could find that the RR for HGVs is 1.64: a crash involving an HGV is 64% more likely to result in a fatality or serious injury to the pedestrian than one not involving an HGV.\nAlternatively, we could calculate signed chi-score residuals (Visalingam 1981), a measure of effect size that is sensitive both to absolute and relative differences from expectation. Expected values are counts calculated for each observation (each cell in our contingency table). Observations (\\(O_i ... O_n\\)) are then compared to expected values (\\(E_i ... E_n\\)) as below:\n\\(\\chi=\\frac{O_i - E_i}{\\sqrt{E_i}}\\)\nThe way in which the differences (residuals) between observed and expected values are standardised in the denominator is important. If the denominator was simply the raw expected value, the residuals would express the proportional difference between each observation and its expected value. The denominator is instead transformed using the square root (\\(\\sqrt{E_i}\\)), which has the effect of inflating smaller expected values and squashing larger expected values, thereby giving greater saliency to differences that are large in both relative and absolute number. The Mosaic plot sort of does this visually.\nExpected values are calculated in the same way as the standard chi-square statistic that tests for independence – so in this case, that counts of crashes by vehicle type distribute independently of injury severity (Slight or KSI) – and can be derived from the contingency table:\n\\(E_i = \\frac{C_i \\times R_i}{GT}\\)\nSo for an observed value (\\(O_i\\)), \\(C_i\\) is its column total in the contingency table; \\(R_i\\) is its row total; and \\(GT\\) is the grand total. Below is the contingency table updated with expected values and signed-chi-scores. A score \\(\u0026gt;0\\) means that casualty counts for that injury type and vehicle is greater than expected; a score \\(\u0026lt;0\\) means casualty counts for that injury type and vehicle is less than expected. The signed chi-square residuals have mathematical properties and can be interpreted as any standard score. They are assumed to have a mean \\(\\sim1\\) and standard deviation of \\(\\sim0\\), with residuals of \\(±2\\) having statistical “significance”.\n  Table 3: Pedestrian casualties by vehicle involved and injury severity: contingency table with signed chi-scores.     Observed   Expected   Signed chi-scores     Vehicle type  KSI  Slight  Row Total  KSI Exp  Slight Exp  KSI Resid  Slight Resid      Car  42305  137924  180229  43195  137034  -4.28  2.40    Van  3786  11422  15208  3645  11563  2.34  -1.31    Taxi  2580  9188  11768  2820  8948  -4.53  2.54    Bus  2951  8425  11376  2726  8650  4.30  -2.41    Motorcycle  2137  7102  9239  2214  7025  -1.64  0.92    HGV  2030  3195  5225  1252  3973  21.98  -12.34    Other  1096  3302  4398  1054  3344  1.29  -0.73    Bicycle  1033  3184  4217  1011  3206  0.70  -0.39    Column Total  57918  183742  NA  NA  NA  NA  NA     The benefit for EDA is that the signed-scores are very quick and easy to compute, can be derived entirely from the contingency table without applying prior knowledge. They help to identify and locate anomalies, or deviations from expectation, in a way that is sensitive to both absolute and relative numbers. A common workflow in EDA is (e.g. Correll and Heer 2017b):\n Expose pattern  Model an expectation derived from pattern  Show deviation from expectation.  The heatmap in Figure 5 is effectively a contingency table, so we could demonstrate this by generating a modelled expectation – signed chi-score residuals for each cell. Figure 7 does this for days of the week and periods of the day separately. The signed residuals are mapped to a diverging colour scheme – purple for cells with fewer crash counts than expected, green for cells with more crash counts than expected. To simplify things a little I generated heatmaps separately for day of week and period of day. In each case the assumption, the modelled expectation, is that crash counts by borough distribute independently of day of week or period of day.\n Figure 7: Heatmaps of crashes by day and time period for London Boroughs.  Looking first by day of week, we see that inner London boroughs generally have fewer than expected crash counts during the weekends and that the reverse is true for outer London boroughs – the first column (Sunday) and last column (Saturday) are purple for the top half of the graphic and green for the bottom half. Unsurprisingly this pattern is most extreme for City of London, London’s financial centre. Looking a little more closely at the outer London boroughs we can also identify where this tendency is stronger: Enfield and Haringey, Newham, Redbridge and Waltham Forest. With some knowledge of London’s social and economic geography we could speculate around these patterns and investigate further as part of our EDA. Notice also that whilst we can interpret the direction of relative differences using colour hue (green:positive | purple:negative) for boroughs containing comparatively few crashes (bottom of graphic), these patterns are de-emphasised by the chi-score residuals which are mapped to colour value (lightness).\nFor the period of day heatmap, the pattern is of higher than expected crash frequencies at night for particular inner London boroughs – Westminster, Lambeth, Tower Hamlets and especially Hackney – and lower than expected for particular outer London boroughs – Kingston upon Thames, Bromley, Richmond upon Thames. For the morning peak, City of London, Wandsworth, Southwark and Lambeth contain greater than expected crash frequencies. Again, with knowledge of London’s geography, one could speculate about why this might be the case and investigate further.\n  After reading this section, you might have felt that the process of generating modelled expectations extends beyond an initial EDA. I’d argue that the sort of statistical computations described above are necessary as when visually analysing raw values as the dominant “signal” is one that is often already obvious and subject to heavy confounding variables. We will return to this in the technical element of the session.\nAdditional aside: The discussion of Risk Ratios (RRs) was reasonably abbreviated. RRs are very interpretable measures of effect size, especially when considered alongside absolute risk (observed KSI rates in this case). As they are a ratio of ratios, and therefore agnostic to sample size, RRs can nevertheless be unreliable. Two ratios might be compared that have very different sample sizes and no compensation is made for the one that contains more data. Although this was our justification for selecting signed chi-scores, this can be addressed by estimating confidence intervals or using robust Bayesian estimates for RRs (representing RRs as distributions) (Greenland 2006).     Supporting comparison with layout and colour Fundamental to EDA, and to the data graphics above, is the process of comparison. Effective data graphics are arranged in such a way as to support and invite relevant comparison. There are three ways in which this can be achieved (via Gleicher and Roberts 2011):\n Juxtaposition: Laying the data items to be compared side-by-side, for example the regional comparison of Swing in the small multiple histograms that appeared in the previous session. Superposition: Laying the data items to be compared on top of each other on the same coordinate space, for example the gendered comparison of trip counts by hour of day and day of week in the line chart in session 2. Direct encoding: Computing some comparison values and encoding those values explicitly, for example the signed chi-scores in Figure 7.  Throughout this module you will generate data graphics that support comparison using each of these strategies. Table 4 is a useful guide for their implementation in ggplot2.\n  Table 4: Implementing Gleicher and Roberts (2011)’s three comparison strategies in ggplot2.    Strategy  Function  Use      Juxtaposition  facetting  Create separate plots in rows and/or columns by conditioning on a categorical variable. Each plot has same encoding and coordinate space.    Juxtaposition  patchwork  Flexibly arrange plots of different data types, encodings and coordinate space.    Superposition  geoms  Layering marks on top of each other. Marks may be of different data types but must share the same coordinate space.    Direct encoding  NA  No strategy specialised to direct encoding. Often variables cross \\(0\\) and so diverging schemes, clearly annotated or symbolised thresholds are important.     As with most visualization design, these strategies require careful implementation. Particularly when juxtaposing views for comparison, it is important to use layout or arrangement in a way that best supports comparison. In the practical we will consider some more routine ways in which plots can be composed to support comparison as we analyse pedestrian casualties in the Stats19 dataset. To demonstrate how layout and colour can be used for effecting spatial comparison, we will return to our analysis of pedestrian injuries by vehicle type, and explore variation by London Borough.\nIn figure 8 Westminster and Harrow, two very different London Boroughs, are compared. Bar size heights vary according to absolute number of recorded injuries by vehicle type and widths according to relative number of injuries recorded on weekday versus weekends. Whilst the modal vehicle type involved in pedestrian injuries is the same for both boroughs (cars), it is far more dominant for crashes recorded in the outer London borough of Harrow. Where cars are involved in crashes in Westminster these occur more frequently (when compared with Harrow) on weekends. Notice that Taxis, Motorcycles, Vans and Bicycles take up a reasonable share crashes involving pedestrian injuries and that for Bicycles, Vans and Motorcycles these occur more frequently on weekdays.\n Figure 8: Mosaic plots of vehicle type and injury severity for Westminster and Harrow.  Given these differences, it may be useful to compare across all 33 Boroughs in London, juxtaposing mosaic plots for each borough side-by-side (Figure 9). That the mosaic plots are annotated with labels for each vehicle type whose size varies according to frequency helps with interpretation. However, we can use colour hue to enable selecting and associating of individual vehicle types. This allows very central (City, Westminster, to a lesser extent Camden and Islington) inner (Wandsworth, Southwark, Lambeth) and outer (Croydon, Sutton) to be related.\n Figure 9: Mosaic plots of vehicle type and injury severity for London Boroughs.  The alphabetical layout of boroughs is helps with look-up of specific boroughs, but since we have already identified that patterns of pedestrian injuries by vehicle type varies by central-inner-outer London, we could further support this comparison by giving the mosaic plots a spatial arrangement (Figure 10). This map may at first seem alien as you are most likely familiar seeing maps with a precise geographic arrangement. However, when studying spatial patterns in a dataset such a level of precision is not always needed. Relaxing geography frees up space to introduce richer, more complex designs. In the layout in Figure 10, taken from AfterTheFlood, each borough is represented as a square of regular size and arranged in its approximate geographic position.\n Figure 10: Mosaic plots of vehicle type and injury severity for London Boroughs with spatial arrangement.    Techniques The technical element to this session continues in our analysis of STATS19 road crash data. After importing and describing the dataset, you will generate statistical summaries and data graphics for analysing pedestrian casualties. You will focus on visual design choices – colour and layout – that support comparison of pedestrian casualties, conditioning on numerous categorical variables held in the STATS19 dataset.\n Download the  04-template.Rmd file for this session and save it to the reports folder of your vis-for-gds project. Open your vis-for-gds project in RStudio and load the template file by clicking File \u0026gt; Open File ... \u0026gt; reports/04-template.Rmd.  Import The template file lists the required packages – tidyverse, sf and also the stats19 package for downloading and formatting the road crash data. STATS19 is a form used by the police to record road crashes that result in injury. Raw data are released by the Department for Transport as a series of .csv files spread across numerous .zip folders. This makes working with the dataset somewhat tedious and behind the stats19 package is some laborious work combining, recoding and relabelling the raw data files.\nSTATS19 data are organised into three tables:\n Accidents (or Crashes): Each observation is a recorded road crash with a unique identifier (accident_index), date (date) and time (time), location (longitude, latitude). Many other characteristics associated with the crashes are also stored in this table. Casualties: Each observation is a recorded casualty that resulted from a road crash. The Crashes and Casualties data can be linked via the accident_index variable. As well as the casualty_severity (Slight, Serious, Fatal), information on casualty demographics and other characteristics is stored in this table. Vehicles: Each observation is a vehicle involved in a crash. Again Vehicles can be linked with Crashes and Casualties via the accident_index variable. As well as the vehicle type and manoeuvre being made, information on driver characteristics is recorded in this table.  In  04-template.Rmd is code for downloading each of these tables using the stats19 package’s API. You will collect data on crashes, casualties and vehicles for 2010-2019, so these datasets take a little time to download. For this reason, I suggest that once downloaded you write the data out and read in as .fst.\nThe data analysis that follows is concerned with pedestrian-vehicle crashes. Pedestrian casualties resulting from those crashes are filtered with the explicit purpose of exploring how casualties vary by the socio-economic characteristics of the area in which the crashes took place.\nThe first data processing task is to generate a subset of data describing these casualties and the crashes and vehicles to which they are linked. I provide code for generating this subset. The crashes (crashes_all) and casualties (casualties_all) tables are joined and pedestrian casualties filtered filter(casualty_type==\"Pedestrian\"). This new table is then joined on vehicles (vehicles_all).\nThere are some simplifications and assumptions made here. Some pedestrian crashes involve many vehicles, but ideally for the analysis we need a single vehicle (and vehicle type) to be attributed to each crash. For each casualty, the largest vehicle involved in the crash is assigned. This is achieved by filtering all vehicle records in vehicles_all involved in a pedestrian casualty, a semi_join on the pedestrian casualty table. We then recode the vehicle_type variable as an ordered factor, group the vehicles table by the crash identifier (accident_index), and for each crash identify the largest vehicle involved, and then filter these largest vehicles. It is common to have several vehicles of the same type involved in a crash, so a final filter is on a vehicle_reference variable. This is an integer variable starting from 1 to n assigned to each vehicle involved in the crash. I do not know whether there is an inherent order to this variables, but make the assumption that vehicle_reference=1 is the main vehicle involved in the crash. So single vehicles are linked to single casualties based first on the largest vehicle involved and then on vehicle_reference.\n# Data frame of all pedestrian casualties. ped_veh \u0026lt;- crashes_all %\u0026gt;% left_join(casualties_all %\u0026gt;% select(accident_index, age_of_casualty,sex_of_casualty, casualty_type, casualty_imd_decile, casualty_severity, pedestrian_location, pedestrian_movement, casualty_reference)) %\u0026gt;% filter(casualty_type==\u0026quot;Pedestrian\u0026quot;) # Ordered factor of vehicle types. vehicles_all %\u0026gt;% select(vehicle_type) %\u0026gt;% distinct() %\u0026gt;% pull() veh_orders \u0026lt;- c(\u0026quot;Bicycle\u0026quot;, \u0026quot;Motorcycle\u0026quot;,\u0026quot;Car\u0026quot;, \u0026quot;Taxi\u0026quot;, \u0026quot;Other\u0026quot;, \u0026quot;Van\u0026quot;, \u0026quot;Bus\u0026quot;, \u0026quot;HGV\u0026quot;) # Filtering join to filter all vehicles involved in pedestrian crashes. # Then identify largest vehicle involved in each crash and filter on these. # Then identify \u0026quot;first\u0026quot; vehicle_reference involved in each crash. temp_ped_vehicles \u0026lt;- vehicles_all %\u0026gt;% semi_join(ped_veh) %\u0026gt;% mutate(vehicle_type=factor(vehicle_type, levels = veh_orders, ordered = TRUE)) %\u0026gt;% group_by(accident_index) %\u0026gt;% mutate(largest_vehicle = max(vehicle_type)) %\u0026gt;% filter(vehicle_type==largest_vehicle) %\u0026gt;% mutate(unique_vehicle=min(vehicle_reference)) %\u0026gt;% filter(vehicle_reference==min(vehicle_reference)) ped_veh \u0026lt;- ped_veh %\u0026gt;% left_join(temp_ped_vehicles) # Remove temporary/staging data frame. rm(temp_ped_vehicles) A focus for our analysis is around the geo-demographic characteristics of the locations in which crashes occur, and of the pedestrians and vehicles involved in each crash. Collected in the STATS19 dataset is the Indices of Multiple Deprivation decile of the small area neighbourhood in which casualties and drivers live. Often IMD is reported at the quintile level (5 rather than 10 quantiles), and aggregating to this level may be useful as we condition on many categorical variables in our EDA. Code for performing this aggregation using an SQL-style case_when is in the template.\nThe crashes data does not automatically contain information on the IMD for the location of crashes. However, the neighbourhood area (LSOA) at which IMD is measured is recorded for each crash location. A final bit of data processing code in the template is for downloading the IMD dataset and assigning IMD ranks to quintiles (using an SQL-style ntile() function). IMD measures deprivation for LSOA’s in England and so when joining this on the crashes dataset we filter only crashes taking place in England.\n Explore Crash location geodemographics To start we explore month-on-month frequencies of pedestrian road crashes by IMD of the location in which the crash took place (Figure 11). From this we see that there is some seasonality to pedestrian casualties, that there are more crashes recorded in locations containing higher deprivation – unsurprising as cities tend to contain comparatively high/mid deprivation levels – and that overall recorded pedestrian crash frequencies are reasonably static over the 10-year period.\n Figure 11: Pedestrian casualties by month and IMD.  The code:\nped_veh %\u0026gt;% mutate(year_month=floor_date(date, \u0026quot;month\u0026quot;)) %\u0026gt;% group_by(year_month, crash_quintile) %\u0026gt;% summarise(total=n()) %\u0026gt;% mutate( crash_quintile=factor( crash_quintile, levels=c(\u0026quot;5 least deprived\u0026quot;,\u0026quot;4 less deprived\u0026quot;,\u0026quot;3 mid deprived\u0026quot;, \u0026quot;2 more deprived\u0026quot;, \u0026quot;1 most deprived\u0026quot;) ) ) %\u0026gt;% ggplot(aes(x=year_month, y=total)) + geom_col(aes(fill=crash_quintile),colour=\u0026quot;#8d8d8d\u0026quot;, size=.1 ) + scale_fill_brewer(palette = \u0026quot;Blues\u0026quot;, type=\u0026quot;seq\u0026quot;) + scale_x_date(date_breaks = \u0026quot;1 year\u0026quot;, date_labels = \u0026quot;%Y-%m\u0026quot;)+ theme(axis.text.x = element_text(angle=90)) The plot specification:\nData: Create a new variable of the year and month in which crash occurred (year_month), using lubridate’s floor_date() function. Summarise over this and IMD crash_quintile and make crash_quintile an ordered factor for charting. Encoding: Bar length varies according to crash counts, so map the date variable (year_month) to the x-axis and crash counts (total) to the y-axis. Fill the bars according to crash_quintile. Marks: geom_col for bars. Scale: scale_fill_brewer for a perceptually valid sequential scheme for IMD – the darker the colour the higher the level of deprivation. scale_x_date for control over the intervals at which we wish to draw labels on the x-axis/ Setting: A consequence of using very light colours for the low-deprivation quintile is that bar height becomes difficult to discern. We therefore draw thin grey outlines around the bars, specified with colour=\"#8d8d8d\" and size=.1. Additionally rotate the date labels 90 degrees to resolve overlaps: axis.text.x=....  With only five data categories (IMD quintiles) and counts that do not differ wildly per category, the stacked bars are reasonably effective. However, to better support trends within category, it is useful to have a common baseline and juxtapose separate charts for crashes in each IMD category (using facet_wrap). This improves height comparison between IMD categories, but also within category temporal trends are easier to detect – for example, that of slightly increasing seasonality across all quintiles, especially in the second most deprived quintile. By default facet_wrap lays out individual charts across rows and columns. We could instead arrange the bars on top of each other, in rows, by parameterising the call to facet_wrap: facet_wrap(~crash_quintile, ncol=1).\n Figure 12: Pedestrian casualties by year and IMD.  Picking seasonality in these plots is still challenging; so too whether overall crashes are increasing or decreasing. To support seasonal comparison we could consider arranging in two dimensions such that the bars are grouped according to year along the columns and by IMD decile along the rows. This can be achieved with facet_grid(). To support relative comparison within IMD decile we may choose to give each IMD decile its own (local) y-axis scaling. Rather than comparing absolute heights between IMD deciles we compare instead relative shapes in the month-on-month bars. To further explore whether crash counts are increasing by year, we include the monthly average by year using a horizontal line. This arrangement – Figure 13 – better exposes the fact that seasonal variation is greater for crashes that occur in the least deprived quintile. There are many speculative explanations for why this might be the case – many confounding factors, also that this may be a spurious observation due to differences in sample size, that would need to be explored/eliminated as part of an EDA.\n Figure 13: Pedestrian casualties by year and IMD.  The code:\nped_veh %\u0026gt;% mutate( year=year(date), month=month(date, label=TRUE), ) %\u0026gt;% group_by(year, month, crash_quintile) %\u0026gt;% summarise(total=n()) %\u0026gt;% ungroup() %\u0026gt;% group_by(year, crash_quintile) %\u0026gt;% mutate(month_avg=mean(total)) %\u0026gt;% ungroup %\u0026gt;% mutate( crash_quintile=factor( crash_quintile, levels=c(\u0026quot;5 least deprived\u0026quot;,\u0026quot;4 less deprived\u0026quot;,\u0026quot;3 mid deprived\u0026quot;, \u0026quot;2 more deprived\u0026quot;, \u0026quot;1 most deprived\u0026quot;) ) ) %\u0026gt;% ggplot(aes(x=month, y=total)) + geom_col(fill=site_colours$primary, width=1, alpha=.6)+ geom_line(aes(y=month_avg, group=interaction(year, crash_quintile))) + scale_x_discrete(breaks=c(\u0026quot;Jan\u0026quot;, \u0026quot;Apr\u0026quot;, \u0026quot;Jul\u0026quot;, \u0026quot;Oct\u0026quot;)) + facet_grid(crash_quintile~year, scales=\u0026quot;free_y\u0026quot;) The plot specification:\nData: Create new variables for the year and month in which crash occurred using lubridate functions. Summarise over this and IMD crash_quintile. Also generate a monthly_avg variable, grouping by year and IMD but not month. Encoding: Bar length varies according to crash counts by month, so map the month variable (month) to the x-axis and crash counts (total) to the y-axis. The monthly average lines use the same coordinate space as the bars, so x-position remains unchanged, but the y-position is the monthly average variable rather than month count. We need to explicitly say how the lines are grouped – a concatenation of year and IMD, specified via interaction(). This is within aes() as it describes encoding of the lines with data, not some additional setting. Marks: geom_col for bars, geom_line for lines. Scale: scale_x_discrete for control over the intervals at which we wish to draw labels on the x-axis. Our x-axis variable is an ordered factor and we supply a vector of the months that we wish to label. Facets: facet_grid() for faceting on two variables. Parameterise with scales=\"free_y\" in order to apply a local scaling and therefore support relative comparison by month within IMD group (at the expense of absolute number between IMD). Setting: De-emphasise the bars in order to see the monthly average reference lines by applying transparency (alpha).   Crash location and driver purpose Collected in the STATS19 vehicles table is the reported journey purpose of the driver. Prior to 2011 this field was not available and unfortunately is “not known” for a large proportion (67%) of pedestrian-vehicle crashes. Figure 14 is an updated version of the previous plot, but with bars additionally coloured by journey purpose. Persisting with the analysis of monthly frequencies is useful here as it exposes the fact that an “Other” category has been used inconsistently over time. Further investigation of what this category constitutes is therefore necessary. We would also want to explore in detail the nature of non-reporting – whether there is systematic biases here. Any inferences drawn from Figure 14 are therefore very speculative, but it does appear that the “school run” is reported more frequently in relative terms for crashes occurring in the least deprived quintile – relatively more blue (“school run”) and less green (“commute”). Again, there are speculative explanations for this that could be investigated as part of an EDA.\n Figure 14: Pedestrian casualties by year, IMD and driver purpose.   Crash location and driver-casualty geodemographics For the final part of our EDA, we consider the geodemographics of the individuals involved in crashes. In the casualties and vehicles data geodemographic characteristics are recorded – the IMD decile of the neighbourhood in which the driver and pedestrian lives. Geodemographic variables need to be treated cautiously (e.g. ecological fallacy), but it may be instructive to explore how the characteristics of drivers and pedestrians co-vary.\nTo do this we can construct contingency tables of the joint frequency of each permutation of driver-pedestrian IMD group co-occurring. For consistency with the earlier analysis, IMD is aggregated to quintiles, so our contingency table contains 25 cells – 5x5 driver-pedestrian combinations. It may be useful to display these counts in a way that enables linearity to be inferred. Firstly this is because IMD is an ordered category variable, making it possible to explore linear association in ordered ranks. Secondly, we’d expect similarity in the IMD status of drivers-pedestrians – that is, drivers crashing with pedestrians within the neighbourhoods, or nearby neighbourhoods, in which they both live.\nIn Figure 15 a heatmap matrix shows this association. Cells in the matrix are ordered along the x-axis according to the IMD quintile of the casualty (pedestrian) and along the y-axis according to the IMD quintile of the driver. The darker blues in the diagonals demonstrate that, as expected, it is more common for drivers-pedestrians involved in crashes to share the same geodemographic characteristics. That colour concentrates in the bottom left is therefore also to be expected. Previously we established that a greater number of pedestrian-vehicle crashes occur in high deprivation neighbourhoods and given there is an association between driver-casualty geodemographics, the largest cell frequencies will be concentrated in the bottom left of the matrix.\nThis trend can be further investigated through direct encoding - constructing a modelled expectation and representing deviation from expectation – the right heatmap in the top row of Figure 15. Expected values are generated based on the assumption that crash frequencies distribute by driver IMD class independently of the IMD class of the pedestrian injured. I have also plotted raw expected values and the column and row marginals from which the residuals are derived. This demonstrates how expected values are spread across cells in the contingency table based on the overall size of each IMD class. Signed chi-scores are mapped to a diverging colour scale – green for residuals that are positive (cell counts are greater than expected), purple for residuals that are negative (cell counts are less than expected).\nThe observed-versus-expected plot highlights that the largest positive residuals are in and around the diagonals and the largest negative residuals are those furthest from the diagonals: we see many more crash frequencies between drivers and pedestrians living in the same or similar IMD quintiles and fewer between those in different quintiles. That the bottom left cell – high-deprivation-driver + high-deprivation-pedestrian – is dark green can be understood when remembering that signed chi-scores emphasise effect sizes that are large in absolute as well as relative terms. Not only is there an association between the geodemographics of drivers and casualties, but larger crash counts are recorded in locations containing highest deprivation and so residuals here are large. The largest positive residuals (the darkest green) are nevertheless recorded in the top right of the matrix – and this is more surprising. Against an expectation of no association between the geodemographic characteristics of drivers and pedestrians involved in road crashes, there is a particularly high number of crashes between drivers and pedestrians living in neighbourhoods containing the lowest deprivation. An alternative phrasing: the association between the geodemographics of drivers and pedestrians is greater for those living in the lowest deprivation quintiles.\n Figure 15: IMD of driver-casualty.  The code:\n# Data staging plot_data \u0026lt;- ped_veh %\u0026gt;% # Filter out crashes for which the IMD of the driver and pedestrian is not missing. filter( !is.na(casualty_imd_decile), !is.na(driver_imd_decile), casualty_imd_decile!=\u0026quot;Data missing or out of range\u0026quot;, driver_imd_decile!=\u0026quot;Data missing or out of range\u0026quot;, !is.na(crash_quintile)) %\u0026gt;% # Calculate the grand total -- used for deriving singed-chi-scores. mutate(grand_total=n()) %\u0026gt;% # Calculate row margins -- total crashes by IMD of driver. group_by(driver_imd_quintile) %\u0026gt;% mutate(row_total=n()) %\u0026gt;% ungroup %\u0026gt;% # Calculate col margins -- total crashes by IMD of pedestrian. group_by(casualty_imd_quintile) %\u0026gt;% mutate(col_total=n()) %\u0026gt;% ungroup %\u0026gt;% # Summarise over cells of contingency table -- observed, expected and residuals. group_by(casualty_imd_quintile, driver_imd_quintile) %\u0026gt;% summarise( observed=n(), row_total=first(row_total), col_total=first(col_total), grand_total=first(grand_total), expected=(row_total*col_total)/grand_total, resid=(observed-expected)/sqrt(expected), max_resid=max(abs(resid)) ) %\u0026gt;% ungroup # Plot observed plot_data %\u0026gt;% ggplot(aes(x=casualty_imd_quintile, y=driver_imd_quintile)) + geom_point()+ geom_tile(aes(fill=observed), colour=\u0026quot;#707070\u0026quot;, size=.2) + scale_fill_distiller(palette=\u0026quot;Blues\u0026quot;, direction=1) # Plot signed chi-score plot_data %\u0026gt;% ggplot(aes(x=casualty_imd_quintile, y=driver_imd_quintile)) + geom_point()+ geom_tile(aes(fill=resid), colour=\u0026quot;#707070\u0026quot;, size=.2) + scale_fill_distiller( palette=\u0026quot;PRGn\u0026quot;, direction=1, limits=c(-max(plot_data$max_resid), max(plot_data$max_resid))) The plot specification:\nData: As it is to be re-used (and reasonably lengthy) I have created a staged data frame for charting (plot_data). Hopefully you can see how various dplyr calls are used to generate row and column marginals of the contingency table and that the calculation for the signed chi-scores is as described earlier. You might have noticed that I also calculate a variable storing the absolute maximum value of these scores (max_resid). This is to help centre the diverging colour scheme in the residuals plot. Encoding: For both heatmaps, the IMD class of pedestrians is mapped to the x-axis and drivers to the y-axis. Tiles are coloured according to observed counts (fill=observed) or signed chi-scores (fill=resid). Marks: geom_tile for cells of the heatmap. Scale: scale_fill_distiller for continuous colorbrewer colour schemes. Note that for the divergin scheme (palette=“PRGn”) I set limits based on the maximum absolute value of the residuals. Try deleting the limits call to see what happens to the plot without this. Setting: Note that I apply a colour to geom_tile – this draws a border around each cell.   Detailed analysis of crash location and driver-casualty geodemographics   I had intended to finish the session here. However, I got a little carried away with the pedestrian-driver demographics theme and, using layout and direct encoding, generated data-driven hypotheses/expectations to be explored in more detail. I do not provide code for the analysis below – it extends beyond what I anticipated for this session on EDA. It is optional for the interested reader.    Figure 16: IMD driver-casualty.  Ideally we want to update our expectations to model explicitly for some of the pattern in Figure 16. We have established that an association between the IMD class of drivers, pedestrians and crash locations exists. The more interesting question is whether this is association is stronger for certain driver-pedestrian-location combinations than others: that is, net of the dominant pattern, in which cells are there greater or fewer crash counts.\nThis is not a straightforward task as the dependency is baked-in to our contingency table. Essentially we are interested in how crash counts vary depending on geodemographic distance – of drivers, pedestrians, crash locations. I calculated a new variable measuring this distance directly – the euclidian distance between the geodemographics of the driver and pedestrian from the location of each crash, treating the ordinal IMD class variable as a continuous variable ranging from 1-5. To illustrate this, in Figure 17 I’ve mapped this distance variable to each cell of the heatmap matrix.\n Figure 17: IMD driver-pedestrian-location geodemographic distance.  I then generated counts of pedestrian casualties by demographic distance, and from this, cell probabilities/likelihoods for each unique position in the matrix in Figure 17. The probabilities assume independence between crash location: that is, the relative likelihood of a crash occurring given a cell’s geodemographic distance does not vary by IMD class of the crash location.\n Figure 18: IMD driver-pedestrian-location with modelled expectations based on geodemographic distance.  Salient in Figure 17 is the vertical block of green in the left column of the plot for injuries occurring in the most deprived quintile. Remembering that pedestrian IMD class is mapped to the x-axis, this indicates higher than expected injury counts where pedestrians are involved in crashes that take place in the same IMD class in which they live, even where the driver lives in a different IMD class. Pedestrian injuries occurring in the most deprived quintile appear to be experienced disproportionately by those living in that quintile. In the second-most deprived quintile are green blocks in the left corresponding with pedestrians living in that quintile, but especially the the most deprived quintile (darkest green), being hit by drivers living in the most deprived quintile. In the mid deprived quintile, drivers and pedestrians living in the most deprived quintile are again overrepresented. Glancing in each of the the matrices, cells towards the right, and top are generally closer to purple: pedestrians living in the less deprived quintile groups are less likely to be injured in crashes, and drivers living in those less deprived quintiles are less likely to contribute to those pedestrian injuries, even after controlling geodemographic distance. And the corollary is those those pedestrians and drivers living in the more deprived quintiles are more likely to be injured pedestrians, and drivers contributing to those injuries. There is much to unpick here, and I’m not sure about the validity of spreading estimated counts from a model based on the derived “geodemographic distance” variable. The upshot from this exploratory analysis is, like many health issues, pedestrian road injuries have a heavy socio-economic element and so is worthy of formal investigation. Hopefully from this you get a sense of the common workflow for EDA:\nExpose pattern Model an expectation derived from pattern Show deviation from expectation     Conclusions Exploratory data analysis (EDA) is an approach to analysis that aims to expand knowledge and understanding of a dataset. The idea is to explore structure, and data-driven hypotheses, by quickly generating many often throwaway statistical and graphical summaries. In the session we have discussed chart idioms (Munzner 2014) for exposing distributions and relationships in a dataset, depending on data type. We also showed that EDA is not model-free. Data graphics help us to see dominant patterns and from here formulate expectations that are to be modelled. Different from so-called confirmatory data analysis, in an EDA the goal of model-building is not “to identify whether the model fits or not […] but rather to understand in what ways the fitted model departs from the data” (see Gelman 2004). We covered visualization approaches to supporting comparison between data and expectation using juxtaposition, superimposition and/or direct encoding (Gleicher and Roberts 2011). The session did not provide an exhaustive survey of EDA approaches, and certainly not an exhaustive set of chart idioms (Munzner 2014) for exposing distributions and relationships. By locating the session closely in the STATS19 dataset, you hopefully got a sense of a workflow for EDA that I would argue is common to all effective data analysis (and communication) activity.\n References Correll, M., and J. Heer. 2017a. “Regression by Eye: Estimating Trends in Bivariate Visualizations.” In ACM Human Factors in Computing Systems (Chi). http://idl.cs.washington.edu/papers/regression-by-eye.\n ———. 2017b. “Surprise! Bayesian Weighting for de-Biasing Thematic Maps.” IEEE Transactions on Visualization \u0026amp; Computer Graphics.\n Friendly, M.. 1992. “Mosaic Displays for Loglinear Models.” In ASA, Proceedings of the Statistical Graphics Section, 61–68.\n Gelman, Andrew. 2004. “Exploratory Data Analysis for Complex Models.” Journal of Computational and Graphical Statistics 13 (4). Taylor \u0026amp; Francis: 755–79. doi:10.1198/106186004X11435.\n Gleicher, Albers, M., and J. Roberts. 2011. “Visual Comparison for Information Visualization. Information Visualization.” Information Visualization 10 (4): 289–309.\n Greenland, Sander. 2006. “Bayesian perspectives for epidemiological research: I. Foundations and basic methods.” International Journal of Epidemiology 35 (3): 765–75. doi:10.1093/ije/dyi312.\n Harrison, L., F. Yang, S. Franconeri, and R. Chang. 2014. “Ranking Visualizations of Correlation Using Weber’s Law.” IEEE Conference on Information Visualization (InfoVis) 20 (12): 1943–52.\n Kay, Matthew, and Jeffrey Heer. 2016. “Beyond Weber’s Law: A Second Look at Ranking Visualizations of Correlation.” IEEE Trans. Visualization \u0026amp; Comp. Graphics (InfoVis) 22 (1): 469–78.\n McGill, Tukey, R., and W. A. Larsen. 1978. “Variations of Box Plots.” The American Statistician 32: 12–16.\n Munzner, Tamara. 2014. Visualization Analysis and Design. AK Peters Visualization Series. Boca Raton, FL: CRC Press.\n Rensink, R, and G Baldridge. 2010. “The Perception of Correlation in Scatterplots.” Computer Graphics Forum 29 (3): 1203–10.\n Tukey, J. W. 1977. Exploratory Data Analysis. Reading, MA, USA: Addison-Wesley.\n Visalingam, M. 1981. “The Signed Chi-Score Measure for the Classification and Mapping of Plychotomous Data.” The Cartographic Journal 18 (1): 32–43.\n Wickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. Sebastopol, California: O’Reilly Media. http://r4ds.had.co.nz/.\n   ","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en-uk","lastmod":1606644925,"objectID":"ac7533426a3114e90edf09c0635b197a","permalink":"/class/04-class/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/class/04-class/","section":"class","summary":"Contents  Introduction Concepts Exploratory data analysis and statistical graphics Plots for continuous variables Plots for categorical variables Supporting comparison with layout and colour  Techniques Import Explore  Conclusions References   By the end of this session you should gain the following knowledge:\n   An appreciation of the exploratory data analysis (EDA) worklow. The main chart types (or idioms Munzner 2014) for summarising variation within- and between- variables.","tags":null,"title":"Visualization for exploratory data analysis: using colour and layout for comparison","type":"docs"},{"authors":null,"categories":null,"content":"  EDA  Hadley Wickham and Garrett Grolemund, R for Data Science: Import, Tidy, Transform, Visualize, and Model Data (Sebastopol, California: O’Reilly Media, 2017), http://r4ds.had.co.nz/.\n– Chapter 7.\nFree online.   Modelling expectation  M. Correll and J. Heer, “Surprise! Bayesian Weighting for de-Biasing Thematic Maps,” IEEE Transactions on Visualization \u0026amp; Computer Graphics (2017). M. Visalingam, “The Signed Chi-Score Measure for the Classification and Mapping of Plychotomous Data,” The Cartographic Journal 18, no. 1 (1981): 32–43.   Categorical data analysis  M.. Friendly, “Mosaic Displays for Loglinear Models,” in ASA, Proceedings of the Statistical Graphics Section, 1992, 61–68.   A stellar application of the above  Badawood Wood J. and A. Slingsby, “BallotMaps: Detecting Name Bias in Alphabetically Ordered Ballot Papers,” IEEE Transactions on Visualization and Computer Graphics 17, no. 12 (2011): 2384–91.   ","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en-uk","lastmod":1622505600,"objectID":"13bbf5064f8e7d91a62ab29bec4d6a2c","permalink":"/reading/04-reading/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/reading/04-reading/","section":"reading","summary":"EDA  Hadley Wickham and Garrett Grolemund, R for Data Science: Import, Tidy, Transform, Visualize, and Model Data (Sebastopol, California: O’Reilly Media, 2017), http://r4ds.had.co.nz/.\n– Chapter 7.\nFree online.   Modelling expectation  M. Correll and J. Heer, “Surprise! Bayesian Weighting for de-Biasing Thematic Maps,” IEEE Transactions on Visualization \u0026amp; Computer Graphics (2017). M. Visalingam, “The Signed Chi-Score Measure for the Classification and Mapping of Plychotomous Data,” The Cartographic Journal 18, no.","tags":null,"title":"Visualization for exploratory data analysis","type":"docs"},{"authors":null,"categories":null,"content":"  Contents  Session Outcomes Introduction Visualization interpretation challenge Task 1: Read Task 2: Interpret  References   Session Outcomes By the end of this homework session you should be able to:\n          Introduction This homework requires you to apply the concepts and skills developed in the class session on exploring spatial networks. Do complete the task and be sure to save your work as it will be submitted as part of your portfolio for Assignment 1.\n Visualization interpretation challenge You may be pleased to know that this week’s homework does not require you to write a single line of code. Instead, it is designed to get you reading and interpreting data graphics. OD Maps are a good candidate here as they have been catalogued by Marteen Lambrechts as a Xenographic – Weird but (sometime) useful. I’d add that within the list of other Xenographics, OD Maps are Not that weird and often useful. Nevertheless, they do take some interpretation.\n Figure 1: OD map of commutes between London boroughs  Task 1: Read For a clear explanation of OD maps and interesting application, read Slingsby, Kelly, and Dykes (2014)’s very short paper: OD maps for showing changes in Irish female migration between 1851 and 1911.\n Task 2: Interpret Considering the OD maps in Figure 1, try to answer the following tasks. Write your answers in the  05-template.Rmd file.\nLook-up tasks  For jobs filled in the City of London (CTY), from which borough does largest number of workers commute using public transport?  Enter your answer in the template  Which borough in London has the largest absolute number living and working in that borough (and travelling to work by public transport)? Enter your answer in the template For workers living in Barnet (BRN), what are the top 3 boroughs to which they commute? Enter your answer in the template   Insights Identify three isights that can be made around the geography of commuting patterns in London from reading any/each of the graphics in Figure 1. Briefly identify which graphic(s) led to the insight and how. Please do not spend too long over this activity – the idea is to encourage you to make inferences from graphics, as you will do in your coursework.\n Insight 1  Enter your answer in the template  Insight 2  Enter your answer in the template  Insight 3  Enter your answer in the template      References Slingsby, A., M. Kelly, and J. Dykes. 2014. “OD maps for showing changes inIrish female migration between 1851 and 1911.” Environment and Planning A: Economy and Space 46 (12): 2795–7.\n   ","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en-uk","lastmod":1622505600,"objectID":"8ea8600445a72851a3c59c990e2779c0","permalink":"/homework/05-homework/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/homework/05-homework/","section":"homework","summary":"Contents  Session Outcomes Introduction Visualization interpretation challenge Task 1: Read Task 2: Interpret  References   Session Outcomes By the end of this homework session you should be able to:\n          Introduction This homework requires you to apply the concepts and skills developed in the class session on exploring spatial networks. Do complete the task and be sure to save your work as it will be submitted as part of your portfolio for Assignment 1.","tags":null,"title":"Exploring spatial networks","type":"docs"},{"authors":null,"categories":null,"content":"   Contents  Introduction Concepts Characteristics of effective data graphics Grammar of Graphics Marks and visual channels Evaluating designs Symbolisation Checking perceptual rankings Colour  Techniques Import Summarise Plot distributions Plot ranks/magnitudes Plot relationships Plot geography  Conclusions References   By the end of this session you should gain the following knowledge:\n   Recognise the characteristics of effective data graphics. Understand that there is a grammar of graphics, and that this grammar underpins modern visualization toolkits (ggplot, vega-lite and Tableau). Be aware of the vocabulary used by these toolkits – that of encoding data through visual channels. Be able to select appropriate visual channels given a data item’s measurement type. Appreciate how visual channels and evidence of their encoding effectiveness can be used to evaluate data graphics.    By the end of this session you should gain the following practical skills:\n   Write ggplot2 specifications that represent data using marks (geoms) and encoding channels (aesthetics – colour and position).    Introduction This session outlines the fundamentals of visualization design. It offers a position on what effective data graphics should do, before discussing in detail the processes that take place when creating data graphics. You will learn that there is a framework – a vocabulary and grammar – for supporting this process which, combined with established knowledge around visual perception, can be used to describe, evaluate and create effective data graphics. Talking about a vocabulary and grammar of data and graphics may sound alien and abstract, the preserve of Computer Scientists. However, through an analysis of 2019 General Election results data we will demonstrate that these concepts underpin most visual data analysis.\n  Watch Miriah Meyer’s TEDx talk, Information Visualization for Scientific Discovery, which provides a nice introduction to many of the concepts covered in the session.      Concepts Characteristics of effective data graphics Data graphics take numerous forms and are used in many different ways by scientists, journalists, designers and many more. Whilst the intentions of those producing data graphics varies, those that are effective generally have the following characteristics:\nRepresent complex datasets graphically to expose structure, connections and comparisons that could not be achieved easily via other means. Are data rich: present many numbers in a small space. Reveal patterns at several levels of detail: from broad overview to fine structure. Have elegance – emphasise dimensions of a dataset without extraneous details. Generate an aesthetic response that encourages people to engage with the data or question.    Considering these characteristics, take a look at the data graphics below, which present an analysis of the 2016 US Presidential Election. Use the links to read the full stories and accompanying data analyses.    Figure 1: Maps of 2016 US presidential election results. Left - two-colour choropleth in Medium. Right - information-rich data graphic in The Washington Post.  Both maps use 2016 county-level results data, but the The Washington Post graphic encodes many more data items than the Medium post (see Table 1 below).\nIt is not simply the data density that makes the Washington Post graphic successful. The authors usefully incorporate annotations and transformations in order to support comparison and emphasise structure. By varying the height of triangles according to the number of votes cast, the thickness according to whether or not the result for Trump/Clinton was a landslide and rotating the scrollable map 90 degrees, the very obvious differences between metropolitan, densely populated coastal counties that voted emphatically for Clinton and the vast number of suburban, provincial town and rural counties (everywhere else) that voted Trump, are exposed.\n  Table 1: Data items encoded in the Washington Post and Medium articles.    Data item  Data measurement level  Washington Post  Medium      county location  Ratio (cyclic)        county result  Nominal        state result  Nominal       county votes cast (~pop size)  Ratio       county result margin  Ratio       county result landslide  Nominal         Grammar of Graphics  Data graphics visually display measured quantities by means of the combined use of points, lines, a coordinate system, numbers, symbols, words, shading, and color.\nTufte (1983)\n In evidence in the Washington Post graphic is a judicious mapping of data to visuals, underpinned by a secure understanding of analysis context. This act of carefully considering how best to leverage visual systems given the available data and analysis priorities is key to designing effective data graphics.\nIn the late 1990s Leland Wilkinson, a Computer Scientist and Statistician, introduced the Grammar of Graphics as an approach that formalises this process of turning data into visuals. Wilkinson (1999)’s thesis is that if graphics can be described in a consistent way according to their structure and composition, then the process of generating graphics of different types can be systematised. This has obvious benefits for building visualization toolkits: it makes it easy to specify chart types and combinations and helps formalise the process of designing data visualizations. vega-lite, Tableau and ggplot2 are all underpinned by Grammar of Graphics thinking.\nWilkinson (1999)’s grammar separates the construction of a data graphic into a series of components. Below are the components of the Layered Grammar of Graphics on which ggplot2 is based (Wickham 2010), a slight edit on Wilkinson (1999)’s original work.\n Figure 2: Components of Wickham (2010)’s Layered Grammar of Graphics.  The seven components in Figure 2 are together used to create ggplot2 specifications. The aspects to emphasise at this stage are those in emphasis, which are required in any ggplot2 specification: the data containing the variables of interest, the geom or marks to be used to represent data and the aesthetic (mapping=aes(...)) attributes, or visual channels through which variables are to be encoded.\nTo demonstrate this, let’s generate some scatterplots based on the 2019 General Election data we will be analysing later in the session. Two variables worth exploring for association here are: con_1719, the change in Conservative vote share by constituency between 2017-2019, and leave_hanretty, the size of the Leave vote in the 2016 EU referendum, estimated at Parliamentary Constituency level (see Hanretty 2017).\n Figure 3: Plots, grammars and associated ggplot2 specifications for the scatterplot.  In Figure 3 are three plots and associated ggplot2 specifications. Reading-off the graphics and the associated code, you should get a feel for how ggplot2 specifications are constructed:\nWe start with a data frame, in this case each observation is an electoral result for a Parliamentary Constituency. In the ggplot2 spec this is passed using the pipe operator (data_ge %\u0026gt;%). We also identify the variables we wish to encode and their measurement type. Remembering last session’s materials, both con_1719 and leave_hanretty are ratio scale variables. Next is the encoding (mapping=aes()), which determines how the data are to be mapped to visual channels. A scatterplot is a 2d representation in which horizontal and vertical position varies in a meaningful way, in response to the values of a data set. Here the values of leave_hanretty are mapped along the x-axis and the values of con_1719 are mapped along the y-axis. Finally, we represent individual data items with marks using the geom_point geometry.  In the middle plot, the grammar is updated such that the points are coloured according to winning_party, a variable of type categorical nominal. In the bottom plot constituencies that flipped from Labour-to-Conservative between 2017-19 are emphasised by varying the transparency (alpha) of points. I have described flipped as an ordinal variable, but strictly it is a nominal (binary) variable. Due to the way it is encoded in the plot – constituencies that flipped (flipped=TRUE) are given greater visual emphasis – I think it is more appropriate to call it an ordinal variable.\n  It is understandable if at this stage the specifications in Figure 3 still seem alien to you. We will be updating, expanding and refining ggplot2 specifications throughout this module to support all aspects of modern data analysis – from data cleaning and exploratory analysis through to model evaluation and communication.    Marks and visual channels  Effective data visualization design is concerned with representing data through marks and visual channels in a way that best conveys the properties of the data that are to be depicted.\nvia Jo Wood\n You might have noticed that in my descriptions of ggplot2 specifications I introduced marks as another term for geometry and visual encoding channels as another term for aesthetics. I also paid special attention to the data types that are being encoded.\nMarks are graphical elements such as bars, lines, points, ellipses that can be used to represent data items – in ggplot2, these are accessed through the functions prefaced with geom_*. Visual channels are attributes such as colour, size, position that, when mapped to data, control the appearance of marks in response to the values of a dataset. Not all channels are equally effective. In fact we can say confidently that for particular data types and tasks, some channels perform better than others.\nMarks and channels are terms used in the interface of Tableau and in vega-lite specifications. They are also used widely in Information Visualization, an academic discipline devoted to the study of data graphics, and most notably by Tamara Munzner (2014) in her textbook Visualization Analysis and Design. Munzner (2014)’s work is important and widely adopted as it synthesises over foundational research in Information Visualization and Cognitive Science testing how effective different visual channels are at supporting different tasks.\n Figure 4: Visual channels to which data items can be encoded, as they appear in Munzner (2014).  Figure 4 is taken from Chapter 5 of Munzner (2014) and lists the main visual channels with which data might be encoded. The grouping and order of the Figure is meaningful. Channels are grouped according to the tasks to which they are best suited and then ordered according to their effectiveness at supporting those tasks. To the left are magnitude:order channels – those that are best suited to tasks aimed at quantifying data items. Tothe Right are identify:category channels – those that are most suited to supporting tasks that involve isolating, grouping and associating data items.\nWe can use this organisation of visual channels to make decisions about appropriate encodings given a variable’s measurement level. If we wished to convey the magnitude of something, for example a quantitative (ratio) variable like the size of the Conservative vote share in a constituency, we might select a channel that has good quantitative effectiveness – position on a common scale or length. If we wished to also effectively identify and associate constituencies according to the political party that was elected, a categorical nominal variable, we might select a channel that has good associative properties such as colour hue.\n Evaluating designs The effectiveness rankings of visual channels in Figure 4 are not simply based on Munzner’s preference. They are informed by detailed experimental work – Cleveland and McGill (1984), later replicated by Heer and Bostock (2010) – which involved conducting controlled experiments testing people’s ability to make judgements from graphical elements. We can use Figure 4 to help make decisions around which data item to encode with which visual channel. This is particularly useful when designing data-rich graphics, where several data items are to be encoded simultaneously. The Figure also offers a low cost way of evaluating different designs against their encoding effectiveness. To illustrate this, we can use Munzner’s ranking of channels to evaluate the Washington Post graphic discussed in Figure 1.\n  Table 2: Encoding effectiveness for Washington Post graphic that emphasises vote margin and size of counties using triangle marks.    Mark  Data item  Type  Channel  Rank     Magnitude:Order         Location  ratio (cyclic)  position in x,y  quant          Votes cast (~pop size)  ratio  length  quant          Margin  ratio  length  quant          Landslide  ordinal  area  quant   Identify:Category          Winner  nominal  colour hue  cat      Table 2 provides a summary of the encodings used in the version of the graphic emphasising vote margin and size. US counties are represented using a peak-shaped mark (). The key purpose of the graphic is to depict the geography of voting outcomes, and so the most effective quantitative channel – position on an aligned scale – is used to order the county marks () with a 2D geographic arrangement. With the positional channels taken, the two quantitive measures, votes cast and result margin, are encoded with the next highest ranked channel, 1D length: height varies according to number of votes cast and width according to result margin. The marks are additionally encoded with two categorical variables: whether the county-level result was a landslide and also the ultimate winner. Since the intention is to give greater visual saliency to counties that resulted in a landslide, this as an ordinal variable, encoded with a quantitative channel: 2D area. The winning party, a categorical nominal variable, is encoded using colour hue.\nEach of the encoding choices used in the graphic follow conventional wisdom in that data items are encoded using visual channels that are appropriate to their measurement level. Glancing down the “rank” column we can also argue that the graphic has high effectiveness. Whilst technically spatial region is the most effective channel for encoding nominal data, it is already in use in our graphic as the  marks are arranged by geographic position. Additionally, it makes sense to distinguish Republican and Democrat wins using the colours with which they are always represented. Given the fact that the positional channels are in use to represent geographic location, length to represent votes cast and vote margin, the only superior visual channel to 2D area that could be used to encode the landslide variable is orientation. There are very good reasons for not varying the orientation of the  marks. Most obvious is that this would clearly undermine perception of length encodings used to represent the vote margin (width) and absolute vote size (height).\n  Data visualization design almost always involves trade-offs. When deciding on a design configuration, it is necessary to prioritise analysis tasks and data and match representations and encodings that are most effective to the tasks that are most important. This then constrains the encoding options for less important data items and tasks. Good visualization design is sensitive to this interplay between tasks, data and encoding.    Symbolisation  Symbolization is the process of encoding something with meaning in order to represent something else. Effective symbol design requires that the relationship between a symbol and the information that symbol represents (the referent) be clear and easily interpreted.\nWhite (2017)\n Implicit in the discussion above, and when making design decisions, is the importance of symbolisation. Scrolling through the original Washington Post article, the overall pattern that can be discerned is of population-dense coastal and metropolitan counties voting Democrat – densely-packed, tall, wide and blue  marks – contrasted against population-sparse rural and small town areas voting Republican – short, wide and red  marks. The graphic evokes a distinctive landscape of voting behaviour, emphasised by its caption: “The peaks and valleys of Trump and Clinton’s support”.\nSymbolisation is used equally well in the variant of the graphic emphasising two-party Swing between the 2012 and 2016 elections. Each county is represented as a | mark. The Swing variable is then encoded by continuously varying mark angles: counties swinging Republican are angled to the right /; counties swinging Democrat are angled to the left \\. Although angle is a less effective channel at encoding quantities than is length, there are obvious links to the political phenomena in the symbolisation – angled right for counties that moved to the right politically. Additionally, the variable itself might be regarded as cyclic – or at least it has a ceiling with an important mid-point that requires emphasis. It is worth taking a second look at the full graphic here. Since there is spatial autocorrelation in case trajectories, we quickly assemble from the graphic dominant patterns of Swing to the Republicans (Great Lakes, rural East Coast), predictable Republican stasis (the mid west) and to detect more isolated, locally exceptional Swings to the Democrats (rapidly urbanising counties in the deep south).\n Checking perceptual rankings I mentioned that Munzner’s effectiveness ordering of visual channels is informed by empirical evidence – controlled experiments that examine perceptual abilities at making judgements from graphical primitives. It is worth elaborating a little on this experimental work, and on how established knowledge in Cognitive Science can be used to inform design choices.\nCleveland (1993) emphasises three perceptual activities that take place when we make sense of data graphics:\n Detection : the element of the graphic must be easily discernible. Assembly : the process of identifying patterns and structure within the graphical elements of the visualization. Estimation : the process of making comparisons of the magnitudes of data items from the visual elements used.  These activities can be related to the categories of task outlined earlier. Detection is especially important for selective and associative tasks that involve isolating and grouping data items, whilst estimation is necessary for tasks that are orderable and quantitative, involving the ranking and reading-off of quantities.\nDetection and preattentive processing A useful distinction when considering graphical cognition is between processes that are attentive and pre-attentive (Ware 2008). Attentive processing describes the conscious processing that happens when we attempt to make sense of a visual field. Preattentive processing happens unconsciously and is the type of cognitive processing that allows something to be understood ‘at a glance’. Visual items that immediately pop-out to us induce preattentive processing.\nThe ability to provoke pop-out – making some things on a data graphic more easily detectible than others – relates to detection. It can be useful for supporting selective and associative tasks, and so is often used in a data graphic to encode categorical variables. For example, in the Washington Post graphic the use of colour hue to differentiate and group together counties that voted Republican or Democrat. Preattentive processes can also apply to assembly. We naturally construct and assemble patterns that are smooth and continuous when perceiving a graphic and so deviations from this continuity are often attended to unconsciously. An example here would be those urbanising counties in the deep South, which were locally exceptional in swinging to Democrat (to the left).\nWe can test this preattentive processing by using visual encoding channels to assist a task that requires us to select and associate visual items. Below are a set of data graphics containing 200 numbers. The graphics are currently hidden, but can be revealed by clicking the  icon. For each graphic I want you to scan across the number, isolate or select the number 3, then group or associate the 3s together and count the number of instances that they occur. Speed is important here – so work as quickly as you can.\nFirst, a set of numbers without applying any special encoding to the number 3.\n Encoding: none\n  Figure 5: Encoding: none.  \n Isolate/select 3 from the list of numbers and count its number of occurrences.   If you were racing to complete the task, I imagine you found it moderately stressful. Let’s explore using visual encoding to off-load some of this cognitive effort. We’ll start with a visual channel that does not have particularly strong preattentive properties: area.\n Encoding: area\n  Figure 6: Encoding: area.  \n Isolate/select 3 from the list of numbers and count its number of occurrences.   Using visualization to support the task makes it an order of magnitude easier. But let’s explore some visual channels that have even more powerful properties. I mentioned that tilt/angle has preattentive properties where the data items to be emphasised deviate from some regular pattern. In the graphic below, the number 3 is encoded with tilt or angle.\n Encoding: angle\n  Figure 7: Encoding: tilt/angle.  \n Isolate/select 3 from the list of numbers and count its number of occurrences.   This is in fact more challenging than the size encoding. I think this is most likely because the geometric patterns of the marks used (numbers) is being varied and so this limits the extent to which we unconsciously perceive smoothness and continuity (e.g. limits assembly).\nNext we’ll use a visual channel with known effectiveness at assisting select and associate tasks. Colour hue appears as the second-ranked most effective in Munzner (2014)’s ordering.\n Encoding: colour hue\n  Figure 8: Encoding: colour hue.  \n Isolate/select 3 from the list of numbers and count its number of occurrences.   Finally, though a slightly contrived example, we can use the top-ranked channel according to Munzner (2014): spatial region.\n Encoding: spatial region\n  Figure 9: Encoding: spatial region.  \n Isolate/select 3 from the list of numbers and count its number of occurrences.    Estimation The informal tests above hopefully persuade you of Munzner (2014)’s ordering of identity:category channels in the right side of Figure 4. The ranking of magnitude:order channels is also informed by established theory and evidence.\nWhen using data graphics to communicate quantities, certain visual channels are known to induce biases. Psychophysics is a branch of psychology that develops methods aimed at capturing the often non-linear relationship between the properties of a stimuli such as symbol length, area or colour value, and their perceived response. Stevens’ power law is an empirically-derived relationship that models this effect. The power function takes the form:\n\\(R=kS_n\\)\nWhere \\(S\\) is the magnitude of the stimulus, for example, the absolute length of a line or area of a circle, \\(R\\) is the response, the perceived length and area, and \\(_n\\) is the power law exponent that varies with the type of stimulus. If there is a perfect linear mapping between the stimulus and response, \\(_n\\) is 1.\nStevens and Guirao (1963)’ experimental work involved varying the length of lines and areas of squares and deriving power functions for their perception. For length, an exponent of ~1.0 was estimated; for area an exponent of 0.7. So whilst variation in length is accurately perceived, we underestimate the size of areas as they increase. Flannery (1971)’s work, which was concerned with the perception of quantities in graduated point maps, estimated an exponent of 0.87 for the perception of circle size.\nExperimental findings vary and so these models of human perception are also subject to variation. Nevertheless, corrections can be applied. In cartography a Flannery compensation is used when representing quantities with area.\n Figure 10: Differences in power law exponents for the perception of variation in length and area.    This early experimental work that tries to understand how encoded quantities are perceived is clearly important. But we use data graphics to do much more than estimate single quantities. If data graphics are to serve as tools for analysis, we also need some confidence that the inferences made when studying data using graphics are accurate and reliable. In the Information Visualization domain, experimental work has recently been published exploring the perception of statistical quantities – dispersion (Correll and Gleicher 2014), correlation (Rensink and Baldridge 2010; Harrison et al. 2014; Kay and Heer 2016) and spatial autocorrelation (Klippel, Hardisty, and Li 2011; Beecham et al. 2017) – in commonly used chart types. More on this later in the module.     Colour As demonstrated in the section on preattentive processing, colour is very powerful visual channel. When considering how to encode data with colour, it is helpful to consider three properties:\n Hue : what we generally refer to as “colour” in everyday life – red, blue green, etc. Saturation : how much of a colour there is. Luminance/Brightness : how dark or light a colour is.  The underlying rule when using colour in data graphics is to use properties of colour that match the properties of the data. Categorical nominal data – data that cannot be easily ordered – should be encoded using discrete colours with no obvious order: colour hue. Categorical ordinal data – data whose categories can be ordered – should be encoded with colours that contain an intrinsic order: saturation or brightness, usually allocated into gradients. Quantitative data – data that can be ordered and contain values on a continuous scale – should also be encoded with colours that contain an intrinsic order: saturation or brightness, expressed on a continuous scale.\nAs we will discover shortly, these principles are applied by default in ggplot2, along with access to perceptually uniform schemes. Its brewer scales, for example.\n  There are many considerations when using colour to support visual data analysis and communication – and we will return to these at various points in the module. Read Lisa Charotte-Rost’s Guide to Colours in Data Visualization before processing to the Techniques section.     Techniques The technical element to this session involves analysing data from the 2019 UK General Election, reported by Parliamentary Constituency. After importing and describing the dataset, you will generate data graphics that expose patterns in voting behaviour. You will do so by writing ggplot2 specifications.\n Download the  03-template.Rmd file for this session and save it to the reports folder of your vis-for-gds project. Open your vis-for-gds project in RStudio and load the template file by clicking File \u0026gt; Open File ... \u0026gt; reports/03-template.Rmd.  Import The template file lists the required packages – tidyverse, sf and also the parlitools. Installing parlitools brings down the 2019 UK General Election dataset, along with other constituency-level datasets. Loading it with library(parlitools) makes these data available to your R session.\nThe dataset containing 2019 UK General Election data is called bes_2019. This contains results data released by House of Commons Library. We can get a quick overview in the usual way – with a call to glimpse(\u0026lt;dataset-name\u0026gt;). The dataset’s variables are also described on the parlitools web pages. You will notice that bes_2019 contains 650 rows, one for each Parliamentary Constituency, and 118 columns. Contained in the columns are variables reporting vote numbers and shares for the main political parties for 2019 and 2017 General Elections, as well as names and codes (IDs) for each Parliamentary Constituency and the county, region and country in which they are contained. You might want to count the number of counties and regions in the UK, and the number of constituencies contained by counties and regions, using some of the dplyr functions introduced in the last session – for example with calls to group_by() and count().\nThe aim of this analysis session is to get you familiar with ggplot2 specifications. We will be replicating some of the visual data analysis of the 2019 UK General Election in Beecham (2020), inspired by the Washington Post graphic. For this we need to calculate an additional variable – Butler Swing (Butler and Van Beek 1990) – which represents the average change in share of the vote won by two parties contesting successive elections. Code for calculating this variable (named swing_con_lab) is in the  03-template.Rmd. Although initially intuitive, the measure takes a little interpretation. A Swing to the Conservatives, which we observe most often in this dataset, could happen in three ways:\nAn increase in Conservative vote share and a decrease in Labour vote share. An increase in both Conservative and Labour vote share, but with the Conservative increase outstripping that of Labour’s. A decrease in both Conservative and Labour vote share, but with the Conservative decline being less severe than that of Labour’s.  Different from the US where “third parties” play a negligible role, scenarios 2 and 3 do occur in the UK. You will notice that swing_con_lab is a signed value: positive indicates a Swing to Conservative, negative a Swing to Labour.\nThe only other dataset to load is a .geojson file containing the geometries of constituencies, collected originally from ONS Open Geography Portal and simplified using mapshaper. This is a special class of data frame containing a Simple Features geometry column – more on this later in the module.\n Summarise You will no doubt be familiar with the ultimate result of the 2019 General Election – a landslide Conservative victory that confounded expectations. To start, we can quickly compute some summary statistics around the vote. In the code block below, we count the number of seats won by party and overall vote share by party. For the latter, my code is a little more elaborate than I intended it to be. I needed to reshape the data frame using pivot_wider() such that each row represents a vote for a party in a constituency. From here I computed in a single function the vote share for each party.\nWhilst the Conservative party hold 56% of constituencies, they won only 44% of the vote share. The equivalent stats for Labour are 31% and 32% respectively. Incidentally, whilst the Conservatives increased their share of constituencies from 2017 (where they had just 317, 49% of constituencies) their vote share increase was reasonably small – in 2017 they gained 42.5% of the vote.\n# Number of constituencies won by party. bes_2019 %\u0026gt;% group_by(winner_19) %\u0026gt;% summarise(count=n()) %\u0026gt;% arrange(desc(count)) ## # A tibble: 11 x 2 ## winner_19 count ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 Conservative 365 ## 2 Labour 202 ## 3 Scottish National Party 48 ## 4 Liberal Democrat 11 ## 5 Democratic Unionist Party 8 ## 6 Sinn Fein 7 ## 7 Plaid Cymru 4 ## 8 Social Democratic \u0026amp; Labour Party 2 ## 9 Alliance 1 ## 10 Green 1 ## 11 Speaker 1 # Share of vote by party. bes_2019 %\u0026gt;% select(constituency_name, total_vote_19, con_vote_19:alliance_vote_19) %\u0026gt;% # Select cols containing vote counts by party. pivot_longer(cols=con_vote_19:alliance_vote_19, names_to=\u0026quot;party\u0026quot;, values_to=\u0026quot;votes\u0026quot;) %\u0026gt;% # Pivot to make each row a vote for a party in a constituency. mutate(party=str_extract(party, \u0026quot;[^_]+\u0026quot;)) %\u0026gt;% # Use some regex to pull out party name. group_by(party) %\u0026gt;% summarise(vote_share=sum(votes, na.rm=TRUE)/sum(total_vote_19)) %\u0026gt;% arrange(desc(vote_share)) ## # A tibble: 12 x 2 ## party vote_share ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 con 0.436 ## 2 lab 0.321 ## 3 ld 0.115 ## 4 snp 0.0388 ## 5 green 0.0270 ## 6 brexit 0.0201 ## 7 dup 0.00763 ## 8 sf 0.00568 ## 9 pc 0.00479 ## 10 alliance 0.00419 ## 11 sdlp 0.00371 ## 12 uup 0.00291 Below are some summary statistics computed over the newly created swing_con_lab variable. As the Conservative and Labour votes are negligible in Northern Ireland, it makes sense to focus on Great Britain for our analysis of Con-Lab Swing and so the first step in the code is to create a new data frame filtering out Northern Ireland. We will work with this for the rest of the session.\ndata_gb \u0026lt;- bes_2019 %\u0026gt;% filter(region!=\u0026quot;Northern Ireland\u0026quot;) data_gb %\u0026gt;% summarise( min_swing=min(swing_con_lab), max_swing=max(swing_con_lab), median_swing=median(swing_con_lab), num_swing=sum(swing_con_lab\u0026gt;0), num_landslide_con=sum(con_19\u0026gt;50, na.rm=TRUE), num_landslide_lab=sum(lab_19\u0026gt;50, na.rm=TRUE) ) ## # A tibble: 1 x 6 ## min_swing max_swing median_swing num_swing num_landslide_con num_landslide_lab ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 -6.47 18.4 4.44 599 280 120  Plot distributions  Figure 11: Histograms of Swing variable.  Let’s get going with some ggplot2 specifications by plotting some of these variables. Below is the code for plotting a histogram of the Swing variable.\ndata_gb %\u0026gt;% ggplot(mapping=aes(swing_con_lab)) + geom_histogram() A reminder of the general form of the ggplot2 specification (first covered inGrammar of Graphics section):\nStart with some data: data_gb. Define the encoding: mapping=aes(). In this case, we want to summarise over the swing_con_lab variable. Specify the marks to be used: geom_histogram() in this case.  Different from the scatterplot example, there is more happening in the internals of ggplot2 when creating a histogram. Technically geom_histogram is what Munzner (2014) would describe as a chart idiom rather than a mark (geometric primitive). The Swing variable is partitioned into bins and observations in each bin are counted. The x-axis (bins) and y-axis (counts by bin) is therefore derived from the supplied variable (swing_con_lab). Should you wish, you could enter ?geom_histogram for fuller detail and documentation around controlling bin sizes amongst other things.\nYou will notice that by default the histogram’s bars are given a grey colour. To set them to a different colour, add a fill= argument to geom_histogram(). In the code block below, colour is set using hex codes – \"#003c8f\", based on the theme for this course website. I use the term set here and not map or encode and there is a principled explanation for this. Any part of a ggplot2 specification that involves encoding data – mapping data to a visual channel – should be specified through the mapping=aes() argument. Anything else, for example changing the default colour of marks, their thickness and transparency, needs to be set outside of this argument.\ndata_gb %\u0026gt;% ggplot(mapping=aes(swing_con_lab)) + geom_histogram(fill=\u0026quot;#003c8f\u0026quot;) + labs( title=\u0026quot;Butler two-party Labour-Conservative Swing for Constituencies in GB\u0026quot;, subtitle=\u0026quot;-- 2019 versus 2017 election\u0026quot;, caption=\u0026quot;Data published by House of Commons Library, accessed via `parlitools`\u0026quot;, x=\u0026quot;Swing\u0026quot;, y=\u0026quot;count\u0026quot; ) You might have noticed that different elements of a ggplot2 specification are added (+) as layers. In the example above, the additional layer of labels (labs()) is not intrinsic to the graphic. However, often you will add layers that do affect the graphic itself: for example the scaling of encoded values (e.g. scale_*_continuous()) or whether the graphic is to be conditioned on another variable to generate small multiples for comparison (e.g. facet_*()).\n  Read this design story by Lunzner and McNamara, 2020 for an excellent discussion of the analysis and design considerations when working with histograms. There are of course other geoms for summarising over 1D distributions: geom_boxplot(), geom_dotplot(), geom_violin().   Faceting by region  Figure 12: Histograms of Swing variable, grouped by region.  Adding a call to facet_*(), we can quickly compare how Swing varies by region (as in Figure 12). The plot is annotated with the median value for Swing (4.4) by adding a vertical line layer (geom_vline()) and setting its x-intercept at this value. From this, there is some evidence of a regional geography to the 2019 vote: London and Scotland are particularly distinctive in containing relatively few constituencies swinging greater than the expected midpoint; North East, Yorkshire \u0026amp; The Humber, and to a lesser extent West and East Midlands, appear to show the largest relative number of constituencies swinging greater than the midpoint. It was this graphic, especially the fact that London and Scotland look different from the rest of the country, that prompted the scatterplots in Figure 3 comparing gain in Conservative vote shares against the Brexit vote.\n  Plot ranks/magnitudes  Figure 13: Plots of vote shares by party.  Previously we calculated overall vote share by Political Party. We could continue the exploration of votes by region by re-using this code to generate plots displaying quantities but also comparing region, using marks and encoding channels that are suitable for magnitudes.\nTo generate a bar chart similar to the left of Figure 13 the ggplot2 specification would be:\ndata_gb %\u0026gt;% \u0026lt;some dplyr code\u0026gt; %\u0026gt;% # The code block summarising vote by party. ... %\u0026gt;% # \u0026lt;summarised data frame\u0026gt; %\u0026gt;% # The summarised data frame of vote share by party, piped to ggplot2. ggplot(aes(x=reorder(party, -vote_share), y=vote_share)) + # Categorical-ordinal x-axis (party, reordered), Ratio y-axis (vote_share). geom_col(fill=\u0026quot;#003c8f\u0026quot;) + # Set colour by website theme. theme_v_gds() # A theme I\u0026#39;ve created for the module. A quick breakdown of the specification:\nData: This is the summarised data frame in which each row is a political party and the column describes the vote share recorded for that party. Encoding: I have dropped the mapping=. ggplot2 always looks for aes() and so can save some code clutter. In this case we are mapping party to the x-axis, a categorical variable made ordinal by the fact that we reorder the axis left-to-right descending according to vote_share. vote_share is mapped to the y-axis – so encoded using bar length, on an aligned scale, an effective channel for conveying magnitudes. Marks: geom_col() for generating the bars. Setting: Again, I’ve set bar colour according to the website theme and included titles and captions. I also set the theme to a custom theme I have created for this module theme_v_gds(). Optionally we add a coord_flip() layer in order to display the bars horizontally. This makes the category axis labels easier to read and also seems more appropriate for the visual “ranking” of bars.    ggplot2 themes control the appearance of all non-data items – font sizes and types, gridlines, axes labels. Checkout the complete list of ggplot2’s default themes. If you like the look of the BBC’s in-house data graphics – I do (or at least I like many of them) – explore their Data Journalism cookbook. In fact I’d recommend working through the cookbook as it is a great resource for distilling many of the non-data-related decisions that are made when communicating graphically.   Faceting by region  Figure 14: Plots of vote shares by party and region.  In Figure 14 the graphic is faceted by region. This requires an updated derived dataset grouping by vote_share and region and of course adding a faceting layer (geom_facet(~region)) to the ggplot2 specification. The graphic is more data-rich, but additional cognitive effort is required in relating the bars representing political parties between different graphical subsets. We can assist this identify and associate task by encoding the bars with an appropriate visual channel: colour hue. The ggplot2 specification for this is as you would expect – we add a mapping to geom_col() and pass the variable name party to the fill argument (aes(fill=party).\n\u0026lt;derived_data\u0026gt; %\u0026gt;% ggplot(aes(x=reorder(party, vote_share), y=vote_share)) + geom_col(aes(fill=party)) + coord_flip() + facet_wrap(~region) Trying this for yourself, you will observe that the ggplot2 internals are clever here. Since party is a categorical variable, a categorical (hue-based) colour scheme is automatically applied. Try passing a quantitative variable (fill=vote_share) and see what happens.\nClever as this is, when encoding political parties with colour symbolisation is important. More control over the encoding is necessary in order to specify the colours with which parties are most commonly represented. We can override ggplot2’s default colour by adding a scale_fill_manual() layer into which a vector of hex codes describing the colour of each political party is passed (party_colours). We also need to tell ggplot2 which element of party_colours to apply to which value of party. In the code below, a derived table is generated summarising vote_share by political party and region. In the final line the party variable is recoded as a factor. You might recall from last session that factors are categorical variables of fixed and potentially orderable values, called levels. The call to mutate() recodes party as a factor variable and orders the levels according to overall vote share.\n# Generate derived data. temp_party_shares_region \u0026lt;- data_gb %\u0026gt;% select(constituency_name, region, total_vote_19, con_vote_19:alliance_vote_19) %\u0026gt;% pivot_longer(cols=con_vote_19:alliance_vote_19, names_to=\u0026quot;party\u0026quot;, values_to=\u0026quot;votes\u0026quot;) %\u0026gt;% mutate(party=str_extract(party, \u0026quot;[^_]+\u0026quot;)) %\u0026gt;% group_by(party, region) %\u0026gt;% summarise(vote_share=sum(votes, na.rm=TRUE)/sum(total_vote_19)) %\u0026gt;% filter(party %in% c(\u0026quot;con\u0026quot;, \u0026quot;lab\u0026quot;, \u0026quot;ld\u0026quot;, \u0026quot;snp\u0026quot;, \u0026quot;green\u0026quot;, \u0026quot;brexit\u0026quot;, \u0026quot;pc\u0026quot;)) %\u0026gt;% mutate(party=factor(party, levels=c(\u0026quot;con\u0026quot;, \u0026quot;lab\u0026quot;, \u0026quot;ld\u0026quot;, \u0026quot;snp\u0026quot;, \u0026quot;green\u0026quot;, \u0026quot;brexit\u0026quot;, \u0026quot;pc\u0026quot;))) Next, a vector of objects is created containing the hex codes for the colours of political parties (party_colours). This is a named vector, with names assigned from the levels of the party variable that was just created.\n# Define colours. con \u0026lt;- \u0026quot;#0575c9\u0026quot; lab \u0026lt;- \u0026quot;#ed1e0e\u0026quot; ld \u0026lt;- \u0026quot;#fe8300\u0026quot; snp \u0026lt;- \u0026quot;#ebc31c\u0026quot; green \u0026lt;- \u0026quot;#78c31e\u0026quot; pc \u0026lt;- \u0026quot;#4e9f2f\u0026quot; brexit \u0026lt;- \u0026quot;#25b6ce\u0026quot; other \u0026lt;- \u0026quot;#bdbdbd\u0026quot; party_colours \u0026lt;- c(con, lab, ld, snp, green, brexit, pc) names(party_colours) \u0026lt;- levels(temp_party_shares %\u0026gt;% pull(party)) The ggplot2 specification is then updated with the scale_fill_manual() layer:\n# ggplot2 spec. temp_party_shares_region %\u0026gt;% ggplot(aes(x=reorder(party, vote_share), y=vote_share)) + geom_col(aes(fill=party)) + scale_fill_manual(values=party_colours) + coord_flip() + facet_wrap(~region)   The idea behind visualization toolkits such as vega-lite, Tableau and ggplot2 is to insert visual data analysis approaches into the Data Scientist’s workflow. Rather than being overly concerned with low-level aspects of drawing, mapping to screen coordinates and scaling factors, the analyst instead focuses on aspects crucial to analysis – exposing patterns in the data by carefully specifying an encoding of data to visuals. Hadley Wickham talks about the type of workflow you will see used throughout this module – bits of dplyr to prepare data for charting before being piped (%\u0026gt;%) to a ggplot2 specification – as equivalent to a grammar of interactive graphics.\nThe process of searching for, defining and inserting manual colour schemes for creating Figure 14 might seem inimical to this. Indeed I was reluctant to include this code so early in the module – there is some reasonably advanced dplyr and a little regular expression in the data preparation code that I don’t want you to be overly concerned with. However, having control of these slightly more low-level properties is sometimes necessary even for supporting exploratory analysis, in this case for enabling a symbolisation that is clear and easily interpretable. Try relating the bars without our manual setting of colours by political party – it certainly requires some mental gymnastics.    Faceting by region   Plot relationships  Figure 15: Plots of 2019 versus 2017 vote shares.  In the Grammar of Graphics section we demonstrated how scatterplots are specified in ggplot2. Scatterplots are useful examples for introducing ggplot2 specifications as they involve working with genuine mark primitives (geom_point()) and can be built up using a wide range of encoding channels.\nTo continue the investigation of change in vote shares for the major parties between 2017 and 2019, Figure 15 contains scatterplots of vote share in 2019 (y-axis) against vote share in 2017 (x-axis) for Conservative and Labour. The graphics are annotated with a diagonal line. If constituencies voted in 2019 in exactly the same way as 2017, the points would all converge on the diagonal, points above the diagonal indicate a larger vote share than 2017, those below the diagonal represent a smaller vote share than 2017. Points are coloured according to the winning party in 2019 and constituencies that flipped from Labour to Conservative are emphasised using transparency and shape.\nThe code for generating most of the scatterplot comparing Conservative vote shares is below.\ndata_gb %\u0026gt;% mutate(winner_19=case_when( winner_19 == \u0026quot;Conservative\u0026quot; ~ \u0026quot;Conservative\u0026quot;, winner_19 == \u0026quot;Labour\u0026quot; ~ \u0026quot;Labour\u0026quot;, TRUE ~ \u0026quot;Other\u0026quot; )) %\u0026gt;% ggplot(aes(x=con_17, y=con_19)) + geom_point(aes(colour=winner_19), alpha=.8) + geom_abline(intercept = 0, slope = 1) + scale_colour_manual(values=c(con,lab,other)) + ... Hopefully there is little surprising here:\nData: The data_gb data frame. Values of winner_19 that are not Conservative or Labour are recoded to Other using a conditional statement. This is because points are eventually coloured according to winning party, but the occlusion of points adds visual complexity and so I’ve chosen to prioritise the two main parties and recode remaining parties to other. Encoding: Conservative vote share in 2017 and 2019 are mapped to the x- and y- axes respectively and winner_19 to colour. scale_colour_manual() is used for customising the colours. Marks: geom_point() for generating the points of the scatterplot; geom_abline() for drawing the reference diagonal.    You will have encountered conditionals in the reading from last session. case_when allows you to avoid writing multiple if_else() statements. It wasn’t really necessary here – I could have used a single if_else with something like:\ndata_gb %\u0026gt;% mutate( winner_19=if_else(!winner_19 %in% c(\u0026quot;Conservative\u0026quot;, \u0026quot;Labour\u0026quot;), \u0026quot;Other\u0026quot;, winner_19) ) A general point from the code blocks in this session is of the importance of proficiency in dplyr. Throughout the module you will find yourself needing to calculate new variables, recode variables, and reorganise data frames before passing through to ggplot2.    Plot geography  Figure 16: Choropleth of elected parties in 2019 General Election.  In the graphics that facet by region, our analysis suggests at a geography to voting and certainly to observed changes in voting comparing the 2017 and 2019 elections (e.g. Figure 12). We end the session by encoding the results data with a spatial arrangement – we’ll generate some maps.\nTo do this we need to define a join on the boundary data (cons_outline):\n# Join constituency boundaries. data_gb \u0026lt;- cons_outline %\u0026gt;% inner_join(data_gb, by=c(\u0026quot;pcon19cd\u0026quot;=\u0026quot;ons_const_id\u0026quot;)) # Check class. ## [1] \u0026quot;sf\u0026quot; \u0026quot;data.frame\u0026quot; The code for generating the Choropleth maps of winning party by constituency in Figure 16:\n# Recode winner_19 as a factor variable for assigning colours. data_gb \u0026lt;- data_gb %\u0026gt;% mutate( winner_19=if_else(winner_19==\u0026quot;Speaker\u0026quot;, \u0026quot;Other\u0026quot;, winner_19), winner_19=as_factor(winner_19)) # Create a named vector of colours party_colours \u0026lt;- c(con, lab, ld, green, other, snp, pc) names(party_colours) \u0026lt;- levels(data_gb %\u0026gt;% pull(winner_19)) # Plot map. data_gb %\u0026gt;% ggplot(aes(fill=winner_19)) + geom_sf(colour=\u0026quot;#eeeeee\u0026quot;, size=0.01)+ # Optionally add a layer for regional boundaries. # geom_sf(data=. %\u0026gt;% group_by(region) %\u0026gt;% summarise(), colour=\u0026quot;#eeeeee\u0026quot;, fill=\u0026quot;transparent\u0026quot;, size=0.08)+ coord_sf(crs=27700, datum=NA) + scale_fill_manual(values=party_colours) A breakdown of the ggplot2 spec:\nData: The dplyr code updates data_gb by recoding winner_19 as a factor and defining a named vector of colours to supply to scale_fill_manual(). Encoding: No surprises here – fill according to winner_19. Marks: geom_sf() is a special class of geometry. It draws objects depending on the contents of the geometry column. In this case MULTIPOLYGON, so read this as a polygon geometric primitive. Coordinates: coord_sf – we set the coordinate system (CRS) explicitly. In this case OS British National Grid. More on this later in the module. Setting: Again, the theme_v_gds() theme. I’ve also subtly introduced light grey (colour=\"#eeeeee\") and thin (size=0.01) constituency boundaries to the geom_sf mark. On the map to the right outlines for regions are added as another geom_sf layer.   Figure 17: Map of Butler Con-Lab Swing in 2019 General Election.  This has been a packed session. I’m providing a very abbreviated introduction to map design with ggplot2 and want to reserve the details of how ggplot2 can be used in more involved visualization design for later in the module. Since the graphic has been discussed at length, it would be strange not to demonstrate how the encoding in the Washington Post piece can be applied here to analyse our Butler two-party swing variable (e.g. Beecham 2020).\nFirst, some helper functions – converting degrees to radians and centring geom_spoke() geometries. Don’t bother yourself with these details, just run the code.\n# Convert degrees to radians. get_radians \u0026lt;- function(degrees) { (degrees * pi) / (180) } # Rescaling function. map_scale \u0026lt;- function(value, min1, max1, min2, max2) { return (min2+(max2-min2)*((value-min1)/(max1-min1))) } # Position subclass for centred geom_spoke as per -- # https://stackoverflow.com/questions/55474143/how-to-center-geom-spoke-around-their-origin position_center_spoke \u0026lt;- function() PositionCenterSpoke PositionCenterSpoke \u0026lt;- ggplot2::ggproto(\u0026#39;PositionCenterSpoke\u0026#39;, ggplot2::Position, compute_panel = function(self, data, params, scales) { data$x \u0026lt;- 2*data$x - data$xend data$y \u0026lt;- 2*data$y - data$yend data$radius \u0026lt;- 2*data$radius data } ) Next re-define party_colours, the object we use for manually setting colours, to contain just three values: hex codes for Conservative, Labour and Other.\nparty_colours \u0026lt;- c(con, lab, other) names(party_colours) \u0026lt;- c(\u0026quot;Conservative\u0026quot;, \u0026quot;Labour\u0026quot;, \u0026quot;Other\u0026quot;) And the ggplot2 specification:\nmax_shift \u0026lt;- max(abs(data_gb %\u0026gt;% pull(swing_con_lab))) min_shift \u0026lt;- -max_shift gb \u0026lt;- data_gb %\u0026gt;% mutate( is_flipped=seat_change_1719 %in% c(\u0026quot;Conservative gain from Labour\u0026quot;,\u0026quot;Labour gain from Conservative\u0026quot;), elected=if_else(!winner_19 %in% c(\u0026quot;Conservative\u0026quot;, \u0026quot;Labour\u0026quot;), \u0026quot;Other\u0026quot;, as.character(winner_19)) ) %\u0026gt;% ggplot()+ geom_sf(aes(fill=elected), colour=\u0026quot;#636363\u0026quot;, alpha=.2, size=.01)+ geom_spoke( aes(x=bng_e, y=bng_n, angle=get_radians(map_scale(swing_con_lab,min_shift,max_shift,135,45)), colour=elected, size=is_flipped), radius=7000, position=\u0026quot;center_spoke\u0026quot; )+ coord_sf(crs=27700, datum=NA)+ scale_size_ordinal(range=c(.3,.9))+ scale_colour_manual(values=party_colours)+ scale_fill_manual(values=party_colours) A breakdown:\nData: data_gb is updated with a boolean identifying whether or not the Constituency flipped Con-Lab/Lab-Con between successive elections (is_flipped), and a variable simplifying the party elected to either Conservative, Labour or Other. Encoding: geom_sf is again filled by elected party. This encoding is made more subtle by adding transparency (alpha=.2). geom_spoke() is a line primitive that can be encoded with a location and direction. It is mapped to the geographic centroid of each Constituency (bng_e - easting, bng_n - northing), coloured according to elected party, sized according to whether the Constituency flipped its vote and tilted according to the Swing variable. Here I’ve created a function (map_scale) which pegs the maximum Swing values in either direction to 45 degrees (max Swing to the right, Conservative) and 135 degrees (max Swing to the left, Labour). Marks: geom_sf() for the Constituency boundaries, geom_spoke() for the angled line primitives. Scale: geom_spoke() primitives are sized to emphasise whether constituencies have flipped. The size encoding is censored to two values with scale_size_ordinal(). Passed to scale_colour_manual() and scale_fill_manual() is the vector of party_colours Coordinates: coord_sf – the CRS is OS British National Grid. Setting: The radius, the of geom_spoke() primitives is a sensible default arrived at through trial and error, its position set using our center_spoke class.    Conclusions Visualization design is ultimately a process of decision-making. Data must be filtered and prioritised before being encoded with marks, visual channels and symbolisation. The most successful data graphics are those that expose structure, connections and comparisons that could not be achieved easily via other, non-visual means. This session has introduced concepts – a vocabulary, framework and empirically-informed guidelines – that helps support this decision-making process and that underpins modern visualization toolkits (ggplot2 included). Through an analysis of UK 2019 General Election data, we have demonstrated how these concepts can be applied in a real data analysis.\n References Beecham, R. 2020. “Using position, angle and thickness to expose the shifting geographies of the 2019 UK General Election.” Environment and Planning A: Economy and Space 52 (5): 833–36.\n Beecham, R., J. Dykes, W. Meulemans, A. Slingsby, C. Turkay, and J. Wood. 2017. “Map Line-Ups: Effects of Spatial Structure on Graphical Inference.” IEEE Transactions on Visualization \u0026amp; Computer Graphics 23 (1): 391–400.\n Butler, D., and S. Van Beek. 1990. “Why not swing? Measuring electoral change.” Political Science \u0026amp; politics 23 (2): 178–84.\n Cleveland, William S. 1993. The Elements of Graphing Data. Hobart Press.\n Cleveland, W., and R. McGill. 1984. “Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods.” Journal of the American Statistical Association 79 (387): 531–54.\n Correll, M., and M. Gleicher. 2014. “Error Bars Considered Harmful: Exploring Alternate Encodings for Mean and Error.” IEEE Transactions on Visualization \u0026amp; Computer Graphics 20 (12): 2141–51.\n Flannery, J. J. 1971. “The Relative Effectiveness of Some Common Graduated Point Symbols in the Presentation of Quantitative Data.” Cartographica 8 (2): 96–109.\n Hanretty, Chris. 2017. “Areal Interpolation and the Uk’s Referendum on Eu Membership.” Journal of Elections, Public Opinion and Parties 37 (4): 466–83.\n Harrison, L., F. Yang, S. Franconeri, and R. Chang. 2014. “Ranking Visualizations of Correlation Using Weber’s Law.” IEEE Conference on Information Visualization (InfoVis) 20 (12): 1943–52.\n Heer, Jeffrey, and Michael Bostock. 2010. “Crowdsourcing Graphical Perception: Using Mechanical Turk to Assess Visualization Design.” In ACM Human Factors in Computing Systems, 203–12. doi:10.1145/1753326.1753357.\n Kay, Matthew, and Jeffrey Heer. 2016. “Beyond Weber’s Law: A Second Look at Ranking Visualizations of Correlation.” IEEE Trans. Visualization \u0026amp; Comp. Graphics (InfoVis) 22 (1): 469–78.\n Klippel, A., F. Hardisty, and Rui. Li. 2011. “Interpreting Spatial Patterns: An Inquiry into Formal and Cognitive Aspects of Tobler’s First Law of Geography.” Annals of the Association of American Geographers 101 (5): 1011–31.\n Munzner, Tamara. 2014. Visualization Analysis and Design. AK Peters Visualization Series. Boca Raton, FL: CRC Press.\n Rensink, R, and G Baldridge. 2010. “The Perception of Correlation in Scatterplots.” Computer Graphics Forum 29 (3): 1203–10.\n Stevens, S, and M. Guirao. 1963. “Subjective Scaling of Length and Area and the Matching of Length to Loudness and Brightness.” Journal of Experimental Psychology 66 (2): 177–86.\n Tufte, Edward R. 1983. The Visual Display of Quantitative Information. Cheshire, CT: Graphics Press.\n Ware, Colin. 2008. Visual Thinking for Design. Waltham, MA: Morgan Kaufman.\n White, T. 2017. “Symbolization and the Visual Variables.” In He Geographic Information Science \u0026amp; Technology Body of Knowledge, edited by John P. Wilson.\n Wickham, Hadley. 2010. “A Layered Grammar of Graphics.” Journal of Computational and Graphical Statistics 19 (1): 3–28.\n Wilkinson, Leland. 1999. The Grammar of Graphics. New York: Springer.\n   ","date":1621900800,"expirydate":-62135596800,"kind":"page","lang":"en-uk","lastmod":1606644925,"objectID":"a0c01f3889200201f6df018341b3db8f","permalink":"/class/03-class/","publishdate":"2021-05-25T00:00:00Z","relpermalink":"/class/03-class/","section":"class","summary":"Contents  Introduction Concepts Characteristics of effective data graphics Grammar of Graphics Marks and visual channels Evaluating designs Symbolisation Checking perceptual rankings Colour  Techniques Import Summarise Plot distributions Plot ranks/magnitudes Plot relationships Plot geography  Conclusions References   By the end of this session you should gain the following knowledge:\n   Recognise the characteristics of effective data graphics. Understand that there is a grammar of graphics, and that this grammar underpins modern visualization toolkits (ggplot, vega-lite and Tableau).","tags":null,"title":"Visualization fundamentals: Codify, map, evaluate","type":"docs"},{"authors":null,"categories":null,"content":"   Contents  Session outcomes Introduction Concepts Data structure Types of variable Types of observation Tidy data  Techniques Import Describe Transform Tidy  Conclusions References   Session outcomes By the end of this session you should gain the following knowledge:\n   The vocabulary and concepts used to describe data. Appreciate the characteristics and importance of tidy data (Wickham 2014) for data processing and analysis.    By the end of this session you should gain the following practical skills:\n   Load flat file datasets RStudio via querying an API. Calculate descriptive summaries over datasets. Apply high-level functions in dplyr and tidyr for working with data. Create statistical graphics that expose high-level structure in data for cleaning purposes.     Introduction This session covers some of the basics around how to describe and organise data. Whilst this might sound prosaic, there are several reasons why being able to consistently describe a dataset is important. First: it is the initial step in any analysis and helps delimit the research themes and technical procedures that can be deployed. This is especially relevant to modern Data Science-type workflows (like those supported by tidyverse), where it is common apply the same analysis templates for working over data. Describing your dataset with a consistent vocabulary enables you to identify which analysis templates to reuse. Second relates to the point in Session 1 that Geographic Data Science projects usually involve repurposing datasets for social science research for the first time. It is often not obvious whether the data contain sufficient detail and structure to characterise the target behaviours to be researched and the target populations they are assumed to represent. This leads to additional levels of uncertainty and places greater importance on the initial step of data processing, description and exploration.\nThrough the session we will learn both language and concepts for describing and thinking about data, but also how to deploy some of the most important data processing and organisation techniques in R to wrangle over a real dataset. We will be working throughout with data from New York’s Citibike scheme, accessed through the bikedata package, an API to Citibike’s publicly available origin-destination trip data.\n  The idea of applying a consistent vocabulary to describing your data applies especially to working with modern visualization toolkits (ggplot2, Tableau, vega-lite), and will be covered in some detail during the next session as we introduce Visualization Fundamentals and the Grammar of Graphics.    Concepts Data structure In the module we will work with data frames in R. These are spreadsheet like representations where rows are observations (case/record) and columns are variables. Each variable (column) in a data frame is a vector that must be of equal length. Where observations have missing values for certain variables, therefore where they may violate this equal-length requirement, the missing values must be substituted with something, usually with NA or similar. This constraint occasionally causes difficulties, for example when working with variables that contain values of different length for an observation. In these cases we create a special class of column, a list-column, something we’ll return to later in the module.\nOrganising data according this simple structure – rows as observations, columns as variables – makes working with data more straightforward. A specific set of tools, made available via the tidyverse, can be deployed for doing most data tidying tasks (Wickham 2014).\n Types of variable   Table 1: A breakdown of Stevens (1946) variable types, operators and measures of central tendency that can be applied to each.    Measurement  Description  Example  Operators  Midpoint  Dispersion     Categories    Nominal  Non-orderable categories  Political parties; street names  = ≠  mode  entropy    Ordinal  Orderable categories  Terrorism threat levels  … | \u0026lt;\u0026gt;  … | median  … | percentile   Measures    Interval  Numeric measurements  Temperatures; years  … | + -  … | mean  … | variance    Ratio  … | Counts  Distances; prices  … | × ÷  … | mean  … | variance     A classification you may have encountered for describing variables is that developed by Stevens (1946), which considers the level of measurement of a variable. Stevens (1946) classed variables into two groups: variables that describe categories of things and variables that describe measurements of things. Categories include attributes like gender, titles, Subscribers or Casual users of a bikeshare scheme and ranked orders (1st, 2nd, 3rd largest etc.). Measurements include quantities like distance, age, travel time, number of journeys made on a bikeshare scheme.\nCategories can be further subdivided into those that are unordered (nominal) from those that are ordered (ordinal). Measurements can also be subdivided. Interval measurements are quantities that can be ordered and where the difference between two values is meaningful. Ratio measurements have both these properties, but also have a meaningful 0 – where 0 means the absence of something – and where the ratio of two values can be computed. The most common cited example of an interval measurement is temperature (in degrees C). Temperatures can be ordered and compared additively, but 0 degrees C does not mean the absence of temperature and 20 degrees C is not twice as “hot” as 10 degrees C. Add Cyclic ratio (measures bound to range – circle, clock time)\nWhy is this important? The measurement level of a variable determines the types of data analysis procedures that can be performed and therefore allows us to efficiently make decisions when working with a dataset for the first time (Table 1).\n Types of observation Observations either together form an entire population or a subset, or sample that we expect represents a target population.\nYou no doubt will be familiar with these concepts, but we have to think a little more about this in Geographic Data Science applications as we may often be working with datasets that are so-called population-level. The Citibike dataset is a complete, population-level dataset in that every journey made through the scheme is recorded. Whether or not this is truly a population-level dataset, however, depends on the analysis purpose. When analysing the bikeshare dataset are we interested only in describing use within the Citibike scheme or are we taking the patterns observed through our analysis to make claims and inferences about cycling more generally?\nIf the latter, then there are problems as the level of detail we have on our sample is pretty trivial compared to traditional datasets, where we deliberately design data collection activities with a specified target population in mind. It may therefore be difficult to gauge how representative Citibike users and Citibike cycling is of New York’s general cycling population. The flipside is that passively collected data do not suffer from the same problems such as non-response bias and social-desirability bias as traditionally collected datasets.\n Tidy data I mentioned that we would be working with data frames organised such that columns always and only refer to variables and rows always and only refer to observations. This arrangement, called tidy (Wickham 2014), has two key advantages. First, if data are arranged in a consistent way, then it is easier to apply and re-use tools for wrangling them due to data having the same underlying structure. Second, placing variables into columns, with each column containing a vector of values, means that we can take advantage of R’s vectorised functions for transforming data – we will demonstrate this in the technical element of this session.\nThe three rules for tidy data:\nEach variable forms a column. Each observation forms a row. Each type of observational unit forms a table.  Drug treatment dataset To elaborate further, we can use the example given in Wickham (2014), a drug treatment dataset in which two different treatments were administered to participants.\nThe data could be represented as:\n  Table 2: Table 1 of Wickham (2014).    person  treatment_a  treatment_b      John Smith  –  2    Jane Doe  16  11    Mary Johnson  3  1     An alternative organisation could be:\n  Table 3: Alternative organisation of Table 1 of Wickham (2014).    treatment  John Smith  Jane Doe  Mary Johnson      treatment_a  –  16  3    treatment_b  2  11  1     Both present the same information unambiguously – Table 3 is simply Table 2 transposed. However, neither is tidy as the observations are spread across both the rows and columns. This means that we need to apply different procedures to extract, perform computations on, and visually represent, these data.\nMuch better would be to organise the table into a tidy form. To do this we need to identify the variables:\nperson: a categorical nominal variable which takes three values: John Smith, Jane Doe, Mary Johnson. treatment: a categorical nominal variable which takes values: a and b. result: a measurement ratio (I think) variable which six recorded values (including the missing value): -, 16, 3, 2, 11,  Each observation is then a test result returned for each combination of person and treatment.\nSo, a tidy organisation for this dataset would be:\n  Table 4: Tidy version of Table 1 of Wickham (2014).    person  treatment  result      John Smith  a  –    John Smith  b  2    Jane Doe  a  16    Jane Doe  b  11    Mary Johnson  a  3    Mary Johnson  b  1      Gapminder population dataset In chapter 12 of Wickham and Grolemund (2017), the benefits of this layout, particularly for working with R, are demonstrated with the canonical gapminder dataset. I recommend reading this short chapter in full. We will be applying similar approaches in the technique part of this class (which follows shortly) and also the homework. To consolidate our conceptual understanding of tidy data let’s quickly look at the gapminder data, as it is a dataset we’re probably more likely to encounter.\nFirst, a tidy version of the data:\n  Table 5: Tidy excerpt of gapminder dataset.    country  year  cases  population      Afghanistan  1999  745  19987071    Afghanistan  2000  2666  20595360    Brazil  1999  37737  172006362    Brazil  2000  80488  174504898    China  1999  212258  1272915272    China  2000  213766  1280428583     So the variables:\ncountry: a categorical nominal variable. year: a date (cyclic ratio) variable. cases: a ratio (count) variable. population: a ratio (count) variable.  Each observation is therefore a recorded count of cases and population for a country in a year.\nAn alternative organisation of this dataset that appears in Wickham and Grolemund (2017) is below. This is untidy as the observations are spread across two rows. This makes operations that we might want to perform on the cases and population variables – for example computing exposure rates – somewhat tedious.\n  Table 6: Untidy excerpt of gapminder dataset: observations spread across rows    country  year  type  count      Afghanistan  1999  cases  745    Afghanistan  1999  population  19987071    Afghanistan  2000  cases  2666    Afghanistan  2000  population  20595360    Brazil  1999  cases  37737    Brazil  1999  population  174504898    …  …  …  …     This actually doesn’t appear in Wickham and Grolemund (2017), but imagine that the gapminder dataset instead reported values of cases separately by gender. A type of representation I’ve often seen in social sciences, probably as it is helpful for data entry, is where observations are spread across the columns. This too creates problems for performing aggregate functions, but also for specifying visualization designs (in ggplot2) as we will discover in the next session.\n  Table 7: Untidy possible excerpt of gapminder dataset: observations spread across columns    country  year  f_cases  m_cases  f_population  m_population      Afghanistan  1999  447  298  9993400  9993671    Afghanistan  2000  1599  1067  10296280  10299080    Brazil  1999  16982  20755  86001181  86005181    Brazil  2000  39440  41048  87251329  87253569    China  1999  104007  108252  636451250  636464022    China  2000  104746  109759  640212600  640215983        Techniques The technical element to this session involves importing, describing, transforming and tidying data from a large bikeshare scheme – New York’s Citibike scheme.\n Download the  02-template.Rmd file for this session and save it to the reports folder of your vis-for-gds project that you created in session 1. Open your vis-for-gds project in RStudio and load the template file by clicking File \u0026gt; Open File ... \u0026gt; reports/02-template.Rmd.  Import In the template file there is a discussion of how to setup your R session with key packages – tidyverse , fst, lubridate, sf – and also the bikedata package for accessing bikeshare data.\nAvailable via the bikedata package are trip and occupancy data for a number of bikeshare schemes (as below). We will work with data from New York’s Citibike scheme for June 2020. A list of all cities covered by the bikedata package is below:\nbike_cities() ## city city_name bike_system ## 1 bo Boston Hubway ## 2 ch Chicago Divvy ## 3 dc Washington DC CapitalBikeShare ## 4 gu Guadalajara mibici ## 5 la Los Angeles Metro ## 6 lo London Santander ## 7 mo Montreal Bixi ## 8 mn Minneapolis NiceRide ## 9 ny New York Citibike ## 10 ph Philadelphia Indego ## 11 sf Bay Area FordGoBike In the template there are code chunks demonstrating how to download and process these data using bikedata’s API. This is mainly for illustrative purposes and the code chunks take some time to execute. We ultimately use the fst package for serializing and reading in the these data. So I suggest you ignore the import code and calls to the bikedata API and instead follow the instructions for downloading and reading in the .fst file with the trips data and also the .csv  file containing stations data, with:\n# Create subdirectory in data folder for storing bike data. if(!dir.exists(here(\u0026quot;data\u0026quot;, \u0026quot;bikedata\u0026quot;))) dir.create(here(\u0026quot;data\u0026quot;, \u0026quot;bikedata\u0026quot;)) # Read in .csv file of stations data from url. tmp_file \u0026lt;- tempfile() url \u0026lt;- \u0026quot;https://www.roger-beecham.com/datasets/ny_stations.csv\u0026quot; curl::curl_download(url, tmp_file, mode=\u0026quot;wb\u0026quot;) ny_stations \u0026lt;- read_csv(tmp_file) # Read in .fst file of trips data from url. tmp_file \u0026lt;- tempfile() cs_url \u0026lt;- \u0026quot;https://www.roger-beecham.com/datasets/ny_trips.fst\u0026quot; curl::curl_download(url, tmp_file, mode=\u0026quot;wb\u0026quot;) ny_trips \u0026lt;- read_fst(tmp_file) # Write out to subdirectory for future use. write_fst(trips, here(\u0026quot;data\u0026quot;, \u0026quot;ny_trips.fst\u0026quot;)) write_csv(stations, here(\u0026quot;data\u0026quot;, \u0026quot;ny_stations.csv\u0026quot;)) # Clean workspace. rm(url, tmp_file)   fst implements in the background various operations such as multi-threading to reduce load on disk space. It therefore makes it possible to work with large datasets in-memory in R rather than connecting to a database and serving up summaries/subsets to be loaded into R. We will be working with just 2 million records, but with fst it is possible to work in-memory with much larger datasets – in Lovelace (2020) we ended up working with 80 million + trip records.   If you completed the reading and research from the Session 1 Homework, some of the above should be familiar to you. The key arguments to look at are read_csv() and read_fst(), into which we pass the path to the file. In this case we created a tmpfile() within the R session. We then write these data out and save locally to the project’s data folder. This is useful as we only want to download the data once. In the write_*\u0026lt;\u0026gt; functions we reference this location using the here package’s here() function. here is really useful for reliably creating paths relative to your project’s root. To read in these data for future sessions:\n# Read in these local copies of the trips and stations data. ny_trips \u0026lt;- read_fst(here(\u0026quot;data\u0026quot;, \u0026quot;ny_trips.fst\u0026quot;)) ny_stations \u0026lt;- read_csv(here(\u0026quot;data\u0026quot;, \u0026quot;ny_stations.csv\u0026quot;)) Notice that we use assignment here (\u0026lt;-) so that these data are loaded as objects and appear in the Environment pane of your RStudio window. An efficient description of data import with read_csv() is also in Chapter 11 of Wickham and Grolemund (2017).\nny_stations and ny_trips are data frames, spreadsheet type representations containing observations in rows and variables in columns. Inspecting the layout of the stations data with View(ny_stations) you will notice that the top line is the header and contains column (variable) names.  Figure 1: ny_trips and ny_stations as they appear when calling View().   Describe There are several functions for generating a quick overview of a data frame’s contents. glimpse\u0026lt;dataset-name\u0026gt; is particularly useful. It provides a summary of the data frame dimensions – we have c. 1.9 million trip observations in ny_trips and 11 variables. The function also prints out the object type for each of these variables, with the variables either of type int or chr in this case.\nglimpse(ny_trips) ## Rows: 1,882,273 ## Columns: 11 ## $ id \u0026lt;int\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21… ## $ city \u0026lt;chr\u0026gt; \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;… ## $ trip_duration \u0026lt;dbl\u0026gt; 1062, 3810, 1017, 226, 1437, 355, 99, 1810, 87, 2714, 2096, 1611, 529, 69… ## $ start_time \u0026lt;chr\u0026gt; \u0026quot;2020-06-01 00:00:03\u0026quot;, \u0026quot;2020-06-01 00:00:03\u0026quot;, \u0026quot;2020-06-01 00:00:09\u0026quot;, \u0026quot;202… ## $ stop_time \u0026lt;chr\u0026gt; \u0026quot;2020-06-01 00:17:46\u0026quot;, \u0026quot;2020-06-01 01:03:33\u0026quot;, \u0026quot;2020-06-01 00:17:06\u0026quot;, \u0026quot;202… ## $ start_station_id \u0026lt;chr\u0026gt; \u0026quot;ny3419\u0026quot;, \u0026quot;ny366\u0026quot;, \u0026quot;ny389\u0026quot;, \u0026quot;ny3255\u0026quot;, \u0026quot;ny367\u0026quot;, \u0026quot;ny248\u0026quot;, \u0026quot;ny3232\u0026quot;, \u0026quot;ny3263… ## $ end_station_id \u0026lt;chr\u0026gt; \u0026quot;ny3419\u0026quot;, \u0026quot;ny336\u0026quot;, \u0026quot;ny3562\u0026quot;, \u0026quot;ny505\u0026quot;, \u0026quot;ny497\u0026quot;, \u0026quot;ny247\u0026quot;, \u0026quot;ny390\u0026quot;, \u0026quot;ny496\u0026quot;,… ## $ bike_id \u0026lt;chr\u0026gt; \u0026quot;39852\u0026quot;, \u0026quot;37558\u0026quot;, \u0026quot;37512\u0026quot;, \u0026quot;39674\u0026quot;, \u0026quot;21093\u0026quot;, \u0026quot;39594\u0026quot;, \u0026quot;43315\u0026quot;, \u0026quot;16571\u0026quot;, \u0026quot;… ## $ user_type \u0026lt;chr\u0026gt; \u0026quot;Customer\u0026quot;, \u0026quot;Subscriber\u0026quot;, \u0026quot;Customer\u0026quot;, \u0026quot;Customer\u0026quot;, \u0026quot;Customer\u0026quot;, \u0026quot;Subscriber… ## $ birth_year \u0026lt;chr\u0026gt; \u0026quot;1997\u0026quot;, \u0026quot;1969\u0026quot;, \u0026quot;1988\u0026quot;, \u0026quot;1969\u0026quot;, \u0026quot;1997\u0026quot;, \u0026quot;1990\u0026quot;, \u0026quot;1938\u0026quot;, \u0026quot;1995\u0026quot;, \u0026quot;1971\u0026quot;, \u0026quot;… ## $ gender \u0026lt;dbl\u0026gt; 2, 0, 2, 0, 2, 1, 2, 2, 2, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… glimpse(ny_stations) ## Rows: 1,010 ## Columns: 6 ## $ id \u0026lt;int\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 2… ## $ city \u0026lt;chr\u0026gt; \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;n… ## $ stn_id \u0026lt;chr\u0026gt; \u0026quot;ny116\u0026quot;, \u0026quot;ny119\u0026quot;, \u0026quot;ny120\u0026quot;, \u0026quot;ny127\u0026quot;, \u0026quot;ny128\u0026quot;, \u0026quot;ny143\u0026quot;, \u0026quot;ny144\u0026quot;, \u0026quot;ny146\u0026quot;, \u0026quot;ny150\u0026quot;,… ## $ name \u0026lt;chr\u0026gt; \u0026quot;W 17 St \u0026amp; 8 Ave\u0026quot;, \u0026quot;Park Ave \u0026amp; St Edwards St\u0026quot;, \u0026quot;Lexington Ave \u0026amp; Classon Ave\u0026quot;, \u0026quot;B… ## $ longitude \u0026lt;chr\u0026gt; \u0026quot;-74.00149746\u0026quot;, \u0026quot;-73.97803415\u0026quot;, \u0026quot;-73.95928168\u0026quot;, \u0026quot;-74.00674436\u0026quot;, \u0026quot;-74.00297088\u0026quot;, … ## $ latitude \u0026lt;chr\u0026gt; \u0026quot;40.74177603\u0026quot;, \u0026quot;40.69608941\u0026quot;, \u0026quot;40.68676793\u0026quot;, \u0026quot;40.73172428\u0026quot;, \u0026quot;40.72710258\u0026quot;, \u0026quot;40.6…   Table 8: A breakdown of data types in R.    Type  Description      lgl  Logical – vectors that can contain only TRUE or FALSE values    int  Integers – whole numbers    dbl  Double – real numbers with decimals    chr  Character – text strings    dttm  Date-times – a date + a time    fctr  Factors – represent categorical variables of fixed and potentially orderable values     The object type of a variable in a data frane relates to that variable’s measurement level. It is often useful to convert to types with greater specificity. For example, we may which to convert the start_time and stop_time variables to a date-time format so that various time-related functions could be used. For efficient storage, we may wish to convert the station identifier variables as int types by removing the redundant “ny” text which prefaces end_station_id, end_station_id, stn_id. The geographic coordinates are currently stored as type chr. These could be regarded as quantitative variables, floating points with decimals. So converting to type dbl or as a POINT geometry type (more on this later in the module) may be sensible.\nIn the  02-template.Rmd file there are code chunks for doing these conversions. There are some slightly more involved data transform procedures in this code. Don’t fixate too much on these, but the upshot can be seen when running glimpse() on the converted data frames:\nglimpse(ny_trips) ## Rows: 1,882,273 ## Columns: 10 ## $ id \u0026lt;int\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 2… ## $ trip_duration \u0026lt;dbl\u0026gt; 1062, 3810, 1017, 226, 1437, 355, 99, 1810, 87, 2714, 2096, 1611, 529, 695, 206,… ## $ start_time \u0026lt;dttm\u0026gt; 2020-06-01 00:00:03, 2020-06-01 00:00:03, 2020-06-01 00:00:09, 2020-06-01 00:00… ## $ stop_time \u0026lt;dttm\u0026gt; 2020-06-01 00:00:03, 2020-06-01 00:00:03, 2020-06-01 00:00:09, 2020-06-01 00:00… ## $ start_station_id \u0026lt;int\u0026gt; 3419, 366, 389, 3255, 367, 248, 3232, 3263, 390, 319, 237, 3630, 3610, 3708, 465… ## $ end_station_id \u0026lt;int\u0026gt; 3419, 336, 3562, 505, 497, 247, 390, 496, 3232, 455, 3263, 3630, 3523, 3740, 379… ## $ bike_id \u0026lt;int\u0026gt; 39852, 37558, 37512, 39674, 21093, 39594, 43315, 16571, 28205, 41760, 30745, 380… ## $ user_type \u0026lt;chr\u0026gt; \u0026quot;Customer\u0026quot;, \u0026quot;Subscriber\u0026quot;, \u0026quot;Customer\u0026quot;, \u0026quot;Customer\u0026quot;, \u0026quot;Customer\u0026quot;, \u0026quot;Subscriber\u0026quot;, \u0026quot;Sub… ## $ birth_year \u0026lt;int\u0026gt; 1997, 1969, 1988, 1969, 1997, 1990, 1938, 1995, 1971, 1989, 1990, 1969, 1984, 19… ## $ gender \u0026lt;chr\u0026gt; \u0026quot;female\u0026quot;, \u0026quot;unknown\u0026quot;, \u0026quot;female\u0026quot;, \u0026quot;unknown\u0026quot;, \u0026quot;female\u0026quot;, \u0026quot;male\u0026quot;, \u0026quot;female\u0026quot;, \u0026quot;female\u0026quot;, … glimpse(ny_stations) ## Rows: 1,010 ## Columns: 5 ## $ id \u0026lt;dbl\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, … ## $ stn_id \u0026lt;int\u0026gt; 116, 119, 120, 127, 128, 143, 144, 146, 150, 151, 157, 161, 164, 167, 168, 173, 174, 19… ## $ name \u0026lt;chr\u0026gt; \u0026quot;W 17 St \u0026amp; 8 Ave\u0026quot;, \u0026quot;Park Ave \u0026amp; St Edwards St\u0026quot;, \u0026quot;Lexington Ave \u0026amp; Classon Ave\u0026quot;, \u0026quot;Barrow S… ## $ longitude \u0026lt;dbl\u0026gt; -74.00150, -73.97803, -73.95928, -74.00674, -74.00297, -73.99338, -73.98069, -74.00911,… ## $ latitude \u0026lt;dbl\u0026gt; 40.74178, 40.69609, 40.68677, 40.73172, 40.72710, 40.69240, 40.69840, 40.71625, 40.7208…  Transform Transform with dplyr   Table 9: dplyr funcitions (verbs) for manipulating data frames.    function()  Description      filter()  Picks rows (observations) if their values match a specified criteria    arrange()  Reorders rows (observations) based on their values    select()  Picks a subset of columns (variables) by name (or name characteristics)    rename()  Changes the name of columns in the data frame    mutate()  Adds new columns (or variables)    group_by()  Chunks the dataset into groups for grouped operations    summarise()  Calculate single-row (non-grouped) or multiple-row (if grouped) summary values    ..and more      dplyr is one of the most important packages for supporting modern data analysis workflows. The package provides a grammar of data manipulation, with access to functions that can be variously combined to support most data processing and transformation activity. Once you become familiar with dplyr functions (or verbs) you will find yourself generating analysis templates to re-use whenever you work on a new dataset.\nAll dplyr functions work in the same way:\nStart with a data frame. Pass some arguments to the function which control what you do to the data frame. Return the updated data frame.  So every dplyr function expects a data frame and will always return a data frame.\n Use pipes %\u0026gt;% with dplyr dplyr is most effective when its functions are chained together – you will see this shortly as we explore the New York bikeshare data. This chaining of functions can be achieved using the pipe operator (%\u0026gt;%). Pipes are used for passing information in a program. They take the output of a set of code (a dplyr specification) and make it the input of the next set (another dplyr specification).\nPipes can be easily applied to dplyr functions, and the functions of all packages that form the tidyverse. I mentioned in Session 1 that ggplot2 provides a framework for specifying a layered grammar of graphics (more on this in Session 3). Together with the pipe operator (%\u0026gt;%), dplyr supports a layered grammar of data manipulation.\n count() rows This might sound a little abstract so let’s use and combine some dplyr functions to generate some statistical summaries on the New York bikeshare data.\nFirst we’ll count the number of trips made in Jun 2020 by gender. dplyr has a convenience function for counting, so we could run the code below, also in the  02-template.Rmd for this session. I’ve commented the code block to convey what each line achieves.\nny_trips %\u0026gt;% # Take the ny_trips data frame. count(gender, sort=TRUE) # Run the count function over the data frame and set the sort parameter to TRUE. ## gender n ## 1 male 1044621 ## 2 female 586361 ## 3 unknown 251291 There are a few things happening in the count() function. It takes the gender variable from ny_trips, organises or groups the rows in the data frame according to its values (female | male | unknown), counts the rows and then orders the summarised output descending on the counts.\n summarise() over rows   Table 10: A breakdown of aggregate functions commonly used with summarise().    Function  Description      n()  Counts the number of observations    n_distinct(var)  Counts the number of unique observations    sum(var)  Sums the values of observations    max(var)|min(var)  Finds the min|max values of observations    mean(var)|median(var)|sd(var)| ...  Calculates central tendency of observations    ...  Many more     Often you will want to do more than simply counting and you may also want to be more explicit in the way the data frame is grouped for computation. We’ll demonstrate this here with a more involved analysis of the usage data and using some key aggregate functions (Table 10).\nA common workflow is to combine group_by() and summarise(), and in this case arrange() to replicate the count() example.\nny_trips %\u0026gt;% # Take the ny_trips data frame. group_by(gender) %\u0026gt;% # Group by gender. summarise(count=n()) %\u0026gt;% # Count the number of observations per group. arrange(desc(count)) # Arrange the grouped and summarised (collapsed) rows according to count. ## # A tibble: 3 x 2 ## gender count ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 male 1044621 ## 2 female 586361 ## 3 unknown 251291 In ny_trips there is a variable measuring trip duration in seconds (trip_duration) and distinguishing casual users from those formally registered to use the scheme (user_type - Customer vs. Subscriber). It may be instructive to calculate some summary statistics to see how trip duration varies between these groups.\nThe code below uses group_by(), summarise() and arrange() in exactly the same way, but with the addition of other aggregate functions profiles the trip_duration variable according to central tendency and by user_type.\nny_trips %\u0026gt;% # Take the ny_trips data frame. group_by(user_type) %\u0026gt;% # Group by user type. summarise( # Summarise over the grouped rows, generate a new variable for each type of summary. count=n(), avg_duration=mean(trip_duration/60), median_duration=median(trip_duration/60), sd_duration=sd(trip_duration/60), min_duration=min(trip_duration/60), max_duration=min(trip_duration/60) ) %\u0026gt;% arrange(desc(count)) # Arrange on the count variable. ## # A tibble: 2 x 6 ## user_type count avg_duration median_duration sd_duration min_duration max_duration ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Subscriber 1306688 20.2 14.4 110. 1.02 1.02 ## 2 Customer 575585 43.6 23.2 393. 1.02 1.02 Clearly there are some outlier trips that may need to be examined. Bikeshare schemes are built to incentivise short journeys of \u0026lt;30 minutes. However, the max trip duration made by a Subscriber was almost 2 hours and a Customer 6.5 hours. These may be legitimate trips rather than errors in the data – the durations are not in the order of months or years. It also makes sense that more casual users may have longer trip durations, as they are more likely to be tourists or occasional cyclists using the scheme for non-utility trips. However, they do skew the mean travel time.\nReturning to the breakdown of usage by gender, an interesting question is whether or not the male-female split in bikehare is similar to that of the cycling population of New York City as a whole. This might tell us something about whether the bikeshare scheme could be representative of wider cycling. This could be achieved with the code below. A couple of new additions: we use filter(), to remove observations where the gender of the cyclist is unknown. We also use mutate() for the first time, which allows us to modify or create new variables.\nny_trips %\u0026gt;% # Take the ny_trips data frame. filter(gender != \u0026quot;unknown\u0026quot;) %\u0026gt;% # Filter out rows with the value \u0026quot;unknown\u0026quot; on gender. group_by(gender) %\u0026gt;% # Group by gender. summarise(count=n()) %\u0026gt;% # Count the number of observations per group. mutate(prop=count/sum(count)) %\u0026gt;% # Add a new column called `prop`, divide the value in the row for the variable count by the sum of the count variable across all rows. arrange(desc(count)) # Arrange on the count variable. ## # A tibble: 2 x 3 ## gender count prop ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 male 1044621 0.640 ## 2 female 586361 0.360 As I’ve commented each line you hopefully get a sense of what is happening in the code above. I mentioned that dplyr functions read like verbs. This is a very deliberate design decision. With the code laid out as above – each dplyr verb occupying a single line, separated by a pipe (%\u0026gt;%) – you can generally understand the code with a cursory glance. There are obvious benefits to this. Once you become familiar with dplyr it becomes very easy to read, write and share code.\n  Remembering that pipes (%\u0026gt;%) take the output of a set of code and make it the input of the next set, what do you think would happen if you were to comment out the call to arrange() in the code block above? Try it for yourself. You will notice that I use separate lines for each call to the pipe operator. This is good practice for supporting readibility of your code.    Manipulate dates with lubridate Let’s continue this investigation of usage by gender, and whether bikeshare might be representative of regular cycling, by profiling how usage varies over time. To do this we will need to work with date-time variables. The lubridate package provides various convenience functions for this.\nIn the code block below we extract the day of week and hour of day from the start_time variable using lubridate’s day accessor functions. Documentation on these can be accessed in the usual way (?\u0026lt;function-name\u0026gt;), but reading down the code it should be clear to you how this works. Next we count the number of trips made by hour of day, day of week and gender. The summarised data frame will be re-used several times in our analysis, so we store it as an object with a suitable name (ny_temporal) using the assignment operator.\n# Create a hod dow summary by gender and assign it the name \u0026quot;ny_temporal\u0026quot;. ny_temporal \u0026lt;- ny_trips %\u0026gt;% # Take the ny_trips data frame. mutate( day=wday(start_time, label=TRUE), # Create a new column identify dow. hour=hour(start_time)) %\u0026gt;% # Create a new column identify hod. group_by(gender, day, hour) %\u0026gt;% # Group by day, hour, gender. summarise(count=n()) %\u0026gt;% # Count the grouped rows. ungroup()   Whether or not to store derived data tables like ny_temporal in a session is not an easy decision. You want to try to avoid cluttering your Environment pane with many data objects. Often when generating charts it is necessary to create these sorts of derived tables as input data (to ggplot2) – and so when doing visual data analysis you may end up with an unhelpfully large number of these derived tables. The general rule I apply: if the derived table is to be used \u0026gt;3 times in a data analysis or is computationally intensive, assign it to an object.   In Figure 2 below these derived data are plotted. The template contains ggplot2 code for creating the graphic. Don’t obsess too much on it – more on this next session. The plot demonstrates a familiar weekday-weekend pattern of usage. Trip frequencies peak in the morning and evening rush hours during weekdays and mid/late-morning and afternoon during weekends. This is consistent with typical travel behaviour. Notice though that the weekday afternoon peak is much larger than the morning peak. There are several speculative explanations for this and re-running the plot on Subscriber users only may be instructive. A secondary observation is that whilst men and women share this overall pattern of usage, the relative number of trips taken by each day of week varies. Men make many more trips at peak times during the start of the week than they do later in the week. The same pattern does not appear for women. This is certainly something to follow up on, for example by collecting data over a longer period of time.\n Figure 2: Line charts generated with ggplot2. Plot data computed using dplyr and lubridate.    Our analysis is based on data from June 2020, a time when New York residents were emerging from lockdown. It would be instructive to compare with data from a non-Covid year. If there is a very clear contrast in usage between this data and a control (non-Covid) year, this suggests bikeshare data may be used for monitoring behavioural change. The fact that bikeshare is collected continuously makes this possible. Check out Jo Wood’s current work analysing Covid-related change in movement behaviours across a range of cities.    Relate tables with join() Trip distance is not recorded directly in the ny_trips table, but may be important for profiling usage behaviour. Calculating trip distance is eminently achievable as the ny_trips table contains the origin and destination station of every trip and the ny_stations table contains coordinates corresponding to those stations. To relate the two tables, we need to specify a join between them.\nA sensible approach is to:\nSelect all uniquely cycled trip pairs (origin-destination pairs) that appear in the ny_trips table. Bring in the corresponding coordinate pairs representing the origin and destination stations by joining on the ny_stations table. Calculate the distance between the coordinate pairs representing the origin and destination.  The code below is one way of achieving this.\nod_pairs \u0026lt;- ny_trips %\u0026gt;% # Take the ny_trips data frame. select(start_station_id, end_station_id) %\u0026gt;% unique() %\u0026gt;% # Select trip origin and destination (OD) station columns and extract unique OD pairs. left_join(ny_stations %\u0026gt;% select(stn_id, longitude, latitude), by=c(\u0026quot;start_station_id\u0026quot;=\u0026quot;stn_id\u0026quot;)) %\u0026gt;% # Select lat, lon columns from ny_stations and join on the origin column. rename(o_lon=longitude, o_lat=latitude) %\u0026gt;% # Rename new lat, lon columns -- associate with origin station. left_join(ny_stations %\u0026gt;% select(stn_id, longitude, latitude), by=c(\u0026quot;end_station_id\u0026quot;=\u0026quot;stn_id\u0026quot;)) %\u0026gt;% # Select lat, lon columns from ny_stations and join on the destination column. rename(d_lon=longitude, d_lat=latitude) %\u0026gt;% # Rename new lat, lon columns -- associate with destination station. rowwise() %\u0026gt;% # For computing distance calculation one row-at-a-time. mutate(dist=geosphere::distHaversine(c(o_lat, o_lon), c(d_lat, d_lon))/1000) %\u0026gt;% # Calculate distance and express in kms. ungroup() The code block above introduces some new functions: select() to pick or drop variables, rename() to rename variables and a convenience function for calculating straight line distance from polar coordinates (distHaversine()). The key function to emphasise is the left_join(). If you’ve worked with relational databases and SQL, dplyr’s join functions will be familiar to you. In a left_join, all the values from the main table are retained, the one on the left – ny_trips, and variables from the table on the right (ny_stations) are added. We specify explicitly the variable on which the tables should be joined with the by= parameter, station_id in this case. If there is a station_id in ny_trips that doesn’t exist in ny_stations then NA is returned.\nOther join functions provided by dplyr are in the table below. Rather than discussing each, I recommend consulting Chapter 13 of Wickham and Grolemund (2017).\n   *_join(x, y) ...       Table 11: A breakdown of dplyr join functions.    left_join()  all rows from x    right_join()  all rows from y    full_join()  all rows from both x and y    semi_join()  all rows from x where there are matching values in y, keeping just columns from x    inner_join()  all rows from x where there are matching values in y, return all combination of multiple matches in the case of multiple matches    anti_join  return all rows from x where there are not matching values in y, never duplicate rows of x      Figure 3: Histograms generated with ggplot2. Plot data computed using dplyr and lubridate  From the newly created distance variable, we can calculate the average (mean) trip distance for the 1.9m trips – 1.6km. This might seem very short, but remember that the distance calculation is problematic in that these are straight-line distances between pairs of docking stations. Really we should be calculating network distances derived from the cycle network in New York. A separate reason – discovered when generating a histogram on the dist variable – is that there are a large number of trips (124,403) that start and end at the same docking station. Initially these might seem to be unsuccessful hires – people failing to undock a bike for example. We could investigate this further by paying attention to the docking stations at which same origin-destination trips occur, as in the code block below.\nny_trips %\u0026gt;% filter(start_station_id==end_station_id) %\u0026gt;% group_by(start_station_id) %\u0026gt;% summarise(count=n()) %\u0026gt;% left_join(ny_stations %\u0026gt;% select(stn_id, name), by=c(\u0026quot;start_station_id\u0026quot;=\u0026quot;stn_id\u0026quot;)) %\u0026gt;% arrange(desc(count)) ## # A tibble: 958 x 3 ## start_station_id count name ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; ## 1 ny3423 2017 West Drive \u0026amp; Prospect Park West ## 2 ny3881 1263 12 Ave \u0026amp; W 125 St ## 3 ny514 1024 12 Ave \u0026amp; W 40 St ## 4 ny3349 978 Grand Army Plaza \u0026amp; Plaza St West ## 5 ny3992 964 W 169 St \u0026amp; Fort Washington Ave ## 6 ny3374 860 Central Park North \u0026amp; Adam Clayton Powell Blvd ## 7 ny3782 837 Brooklyn Bridge Park - Pier 2 ## 8 ny3599 829 Franklin Ave \u0026amp; Empire Blvd ## 9 ny3521 793 Lenox Ave \u0026amp; W 111 St ## 10 ny2006 782 Central Park S \u0026amp; 6 Ave ## # … with 948 more rows All of the top 10 docking stations are either in parks, near parks or located along river promenades. This coupled with the fact that these trips occur in much greater relative number for casual than regular users (Customer vs Subscriber) is further evidence that these are valid trips.\n Write functions of your own Through most of the module we will be making use of functions written by others – mainly those developed for packages that form the tidyverse and therefore that follow a consistent syntax. However, there may be times where you need abstract over some of your code to make functions of your own. Chapter 19 of Wickham and Grolemund (2017) presents some helpful guidelines around the circumstances under which the data scientist typically tends to write functions. Most often this is when you find yourself copy and pasting the same chunks of code with minimal adaptation.\nFunctions have three key characteristics:\nThey are (usually) named – the name should be expressive and communicate what the function does (we talk about dplyr verbs). They have brackets \u0026lt;function()\u0026gt; usually containing arguments – inputs which determine what the function does and returns. Immediately followed by \u0026lt;function()\u0026gt; are` used to contain the body – in this is code that performs a distinct task, described by the function’s name.  Effective functions are short, perform single discrete operations and are intuitive.\nYou will recall that in the ny_trips table there a variable called birth_year. From this we can derive cyclists’ approximate age. Below I have written a function get_age() for doing this. The function expects two arguments: yob – a year of birth as type chr; yref – a reference year. In the body, lubridate’s as.period function is used to calculate the time in years that elapsed, the value that the function returns. Once defined, and loaded into the session by being executed, it can be used (as below).\n# Function for calculating time elapsed between two dates in years (age). get_age \u0026lt;- function(yob, yref) { period \u0026lt;- lubridate::as.period(lubridate::interval(yob, yref),unit = \u0026quot;year\u0026quot;) return(period$year) } ny_trips \u0026lt;- ny_trips %\u0026gt;% # Take the ny_trips data frame. mutate( age=get_age(as.POSIXct(birth_year, format=\u0026quot;%Y\u0026quot;), as.POSIXct(\u0026quot;2020\u0026quot;, format=\u0026quot;%Y\u0026quot;)) # Calculate age from birth_date. ) We can use the two new derived variables – distance travelled and age – in our analysis. In Figure 4, we explore how approximate travel speeds vary by age, gender and trip distance. The code used to generate the summary data and plot is in the template file. Again the average “speed” calculation should be treated very cautiously as it is based on straight line distances and it is very difficult to select out “utility” from “leisure” trips. I have tried to do the latter by selecting trips that occur only on weekdays and that are made by Subscriber cyclists. Additionally, due to the heavy subsetting data become a little volatile for certain age groups and so I’ve aggregated the age variable into 5-year bands. Collecting more data is probably a good idea. Still there are some interesting patterns. Men tend to cycle at faster speeds than do women, although this gap narrows with the older age groups. The effect of age on speed cycled is more apparent for the longer trips. This trend is reasonably strong, although the volatility in the older age groups for trips \u0026gt;4.5km suggests we probably need more data and a more involved analysis to establish this. For example, it may be that the comparatively rare occurrence of trips in the 65-70 age group is made by only a small subset of cyclists. A larger dataset would result in a regression to the mean effect and negate any noise caused by outlier individuals.\n Figure 4: Line charts generated with ggplot2. Plot data computed using dplyr and lubridate    Tidy The ny_trips and ny_stations data already comply with the rules for tidy data (Wickham 2014). Each row in ny_trips is a distinct trip and each row in ny_stations a distinct station. However throughout the module we will undoubtedly encounter datasets that need to be reshaped. There are two key functions to learn here, made available via the tidyr package: pivot_longer() and pivot_wider(). pivot_longer is used to tidy data in which observations are spread across columns, as in Table 6 (the gapminder dataset). pivot_wider is used to tidy data in which observations are spread across rows, as in Table 7 (the gapminder dataset). You will find yourself using these functions, particularly pivot_longer not only for fixing messy data, but for flexibly reshaping data for use in ggplot2 specifications (more on this in sessions 3 and 4) or joining tables.\nA quick breakdown of pivot_longer:\npivot_longer( data, cols, # Columns to pivot longer (across rows). names_to=\u0026quot;name\u0026quot;, # Name of the column where for pivoted variables. values_to=\u0026quot;name\u0026quot; ) A quick breakdown of pivot_wider:\npivot_wider( data, names_from, # Column in the long format which contains what will be column names in the wide format. values_from # Column in the long format which contains what will be values in the new wide format. ) In the homework you will be tidying some messy derived tables based on the bikeshare data using both of these functions, but we can demonstrate both of these functions in tidying the messy gapminder data in Table 7. Remember that these data were messy as the observations by gender were spread across the columns:\nuntidy_wide ## country year f_cases m_cases f_population m_population ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Afghanistan 1999 447 298 9993400 9993671 ## 2 Afghanistan 2000 1599 1067 10296280 10299080 ## 3 Brazil 1999 16982 20755 86001181 86005181 ## 4 Brazil 2000 39440 41048 87251329 87253569 ## 5 China 1999 104007 108252 636451250 636464022 ## 6 China 2000 104746 109759 640212600 640215983 First we need to gather the problematic columns with pivot_wider.\nuntidy_wide %\u0026gt;% pivot_longer(cols=c(f_cases: m_population), names_to=c(\u0026quot;gender_count_type\u0026quot;), values_to=c(\u0026quot;counts\u0026quot;)) ## country year gender_count_type counts ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Afghanistan 1999 f_cases 447 ## 2 Afghanistan 1999 m_cases 298 ## 3 Afghanistan 1999 f_population 9993400 ## 4 Afghanistan 1999 m_population 9993671 ## 5 Afghanistan 2000 f_cases 1599 ## 6 Afghanistan 2000 m_cases 1067 ## 7 Afghanistan 2000 f_population 10296280 ## 8 Afghanistan 2000 m_population 10299080 ## 9 Brazil 1999 f_cases 16982 ## 10 Brazil 1999 m_cases 20755 ## # … with 14 more rows So this has usefully collapsed the dataset by gender, we now have a problem similar to that in Table 6 where observations are spread across the rows – in this instance cases and population are better treated as separate variables. This can be fixed by separating the gender_count_type variables and then spreading the values of the new count_type (cases, population) across the columns. Hopefully you can see how this gets us to the tidy gapminder data structure in Table 5\nuntidy_wide %\u0026gt;% pivot_longer(cols=c(f_cases: m_population), names_to=c(\u0026quot;gender_count_type\u0026quot;), values_to=c(\u0026quot;counts\u0026quot;)) %\u0026gt;% separate(col=gender_count_type, into=c(\u0026quot;gender\u0026quot;, \u0026quot;count_type\u0026quot;), sep=\u0026quot;_\u0026quot;) ## country year gender count_type counts ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Afghanistan 1999 f cases 447 ## 2 Afghanistan 1999 m cases 298 ## 3 Afghanistan 1999 f population 9993400 ## 4 Afghanistan 1999 m population 9993671 ## 5 Afghanistan 2000 f cases 1599 ## 6 Afghanistan 2000 m cases 1067 ## 7 Afghanistan 2000 f population 10296280 ## 8 Afghanistan 2000 m population 10299080 ## 9 Brazil 1999 f cases 16982 ## 10 Brazil 1999 m cases 20755 ## # … with 14 more rows untidy_wide %\u0026gt;% pivot_longer(cols=c(f_cases: m_population), names_to=c(\u0026quot;gender_count_type\u0026quot;), values_to=c(\u0026quot;counts\u0026quot;)) %\u0026gt;% separate(col=gender_count_type, into=c(\u0026quot;gender\u0026quot;, \u0026quot;count_type\u0026quot;), sep=\u0026quot;_\u0026quot;) %\u0026gt;% pivot_wider(names_from=count_type, values_from=counts) ## country year gender cases population ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Afghanistan 1999 f 447 9993400 ## 2 Afghanistan 1999 m 298 9993671 ## 3 Afghanistan 2000 f 1599 10296280 ## 4 Afghanistan 2000 m 1067 10299080 ## 5 Brazil 1999 f 16982 86001181 ## 6 Brazil 1999 m 20755 86005181 ## 7 Brazil 2000 f 39440 87251329 ## 8 Brazil 2000 m 41048 87253569 ## 9 China 1999 f 104007 636451250 ## 10 China 1999 m 108252 636464022 ## 11 China 2000 f 104746 640212600 ## 12 China 2000 m 109759 640215983   Conclusions Developing the vocabulary and technical skills to systematically describe and organise data is crucial to modern data analysis. This session has covered the fundamentals here: that data consist of observations and variables of different types (Stevens 1946) and that in order to work effectively with datasets, especially in a functional way in R, these data must be organised according to the rules of tidy data (Wickham 2014). Most of the session content was dedicated to the techniques that enable these concepts to be operationalised. We covered how to download, transform and reshape a reasonably large set of data from New York’s Citibike scheme. In doing so, we generated insights that might inform further data collection and analysis activity. In the next session we will apply and extend this conceptual and technical knowledge as we introduce the fundamental of visual data analysis and ggplot2’s grammar of graphics.\n References Lovelace, Beecham, R. 2020. “Is the London Cycle Hire Scheme Becoming More Inclusive? An Evaluation of the Shifting Spatial Distribution of Uptake Based on 70 Million Trips.” Transportation Research Part A: Policy and Practice 140 (October): 1–15.\n Stevens, S. 1946. “On the Theory of Scales of Measurement.” Science 103 (2684): 677–80.\n Wickham, H. 2014. “Tidy Data.” Journal of Statistical Software 59 (10): 1–23.\n Wickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. Sebastopol, California: O’Reilly Media. http://r4ds.had.co.nz/.\n   ","date":1621296000,"expirydate":-62135596800,"kind":"page","lang":"en-uk","lastmod":1606644925,"objectID":"9ae245330f59c57e6f62568d635c26e4","permalink":"/class/02-class/","publishdate":"2021-05-18T00:00:00Z","relpermalink":"/class/02-class/","section":"class","summary":"Contents  Session outcomes Introduction Concepts Data structure Types of variable Types of observation Tidy data  Techniques Import Describe Transform Tidy  Conclusions References   Session outcomes By the end of this session you should gain the following knowledge:\n   The vocabulary and concepts used to describe data. Appreciate the characteristics and importance of tidy data (Wickham 2014) for data processing and analysis.    By the end of this session you should gain the following practical skills:","tags":null,"title":"Data fundamentals: Describe, wrangle, tidy","type":"docs"},{"authors":null,"categories":null,"content":"  Each session has three sections.\n  Reading: Reading and other reference material.  Class: Main content – a mix of ideas/theory and worked examples.  Homework: Homework activity to complete after having worked through the main class content.    Week 1 Introduction      Week 2 Data fundamentals      Week 3 Visualization fundamentals      Week 4 Visualization for exploratory data analysis      Week 5 Visualization for exploring spatial networks         Week 6 Visualization for model building 1         Week 7 Visualization for model building 2         Week 8 Visualization for uncertainty analysis          ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en-uk","lastmod":1606644925,"objectID":"3e223d7ba58b0122b42458e4cf52e04c","permalink":"/schedule/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/schedule/","section":"","summary":"  Each session has three sections.\n  Reading: Reading and other reference material.  Class: Main content – a mix of ideas/theory and worked examples.  Homework: Homework activity to complete after having worked through the main class content.    Week 1 Introduction      Week 2 Data fundamentals      Week 3 Visualization fundamentals      Week 4 Visualization for exploratory data analysis      Week 5 Visualization for exploring spatial networks         Week 6 Visualization for model building 1         Week 7 Visualization for model building 2         Week 8 Visualization for uncertainty analysis          ","tags":null,"title":"Schedule","type":"page"},{"authors":null,"categories":null,"content":"   Overview Module objectives Assessment Software Module breakdown Navigating the course materials Self-guided learning Slack Asking questions   Instructor  Roger Beecham  10.139 Manton Building  r.j.beecham@leeds.ac.uk  @rjbeecham   Course details  Wednesdays  25th May - 13th Jul  10:00 - 12:00  ODL   Contacting me Questions to do with the substantive aspects of the module should be directed to the #general channel of the course Slack in the first instance. For matters such as absences, extensions, email me: r.j.beecham@leeds.ac.uk.\n  -- Overview In modern data analysis, graphics and computational statistics are increasingly used together to explore and identify complex patterns in data and to make and communicate claims under uncertainty. This course will go beyond traditional ideas of charts, graphs, maps (and also statistics!) to equip you with the critical analysis, design and technical skills to analyse and communicate with geographic datasets.\nThe course emphasises real-world applications. You will work with both new, large-scale behavioural datasets, as well as more traditional, administrative datasets located within various social science domains: Political Science, Crime Science, Urban and Transport Planning. As well as learning how to use graphics and statistics to explore patterns in these data, implementing recent ideas from data journalism you will learn how to communicate research findings – how to tell stories with data.\n Module objectives By the end of the course you will\nDescribe, process and combine geographic datasets from a range of sources Design statistical graphics that expose structure in geographic data and that are underpinned by established principles in information visualization and cartography Use modern data science and visualization frameworks to produce coherent, reproducible data analyses Apply modern statistical techniques for analysing, representing and communicating data and model uncertainty   Assessment A detailed and formal description of the Assessment for this module can be found on the Minerva pages. The summative assessment consists of:\n 30% - Portfolio of work from completing session homeworks 2-6 (1500 word equivalent) 70% - Visual data analysis report (2500 word equivalent)   Software All work in the module – data collection, analysis and reporting – will be completed using R and the RStudio Integrated Development Environment (IDE). Along with Python R is the programming environment for modern data analysis.\n Module breakdown   Week 1 Introduction      Week 2 Data fundamentals      Week 3 Visualization fundamentals      Week 4 Visualization for exploratory data analysis      Week 5 Visualization for exploring spatial networks         Week 6 Visualization for model building 1         Week 7 Visualization for model building 2         Week 8 Visualization for uncertainty analysis           Navigating the course materials The home for this module is this website. From here, you will find the course Schedule (also above), where for each session there is  Reading,  Class content and  Homework. Also accompanying each session is an R Markdown file with a template to complete coding activities introduced through the class and homework pages. These session templates contain pre-prepared code chunks for you to execute. As the module progresses and you become more familiar and competent in R, you will be required to contribute more code of your own to these.\nAs you work through the course materials you will notice special icons are used to distinguish learning outcomes  and instructions  that need to be completed, along with important informational asides . Do pay special attention to these, and in particular revisit the learning outcomes  as you progress through the sessions.\nUse Minerva to access the Module Handbook and to upload assignments in the usual way.\n Self-guided learning  The bad news is whenever you’re learning a new tool, for a long time you’re going to suck. It’s going to be very frustrating. But, the good news is that that is typical, it’s something that happens to everyone, and it’s only temporary … [T]here is no way to go from knowing nothing about a subject to knowing something about a subject and being an expert in it without going through a period of great frustration.\"\nHadley Wickham\n From the module overview and outline you will have got the sense that this is a reasonably technical module. You will be introduced to the key components of modern data analysis (Data Science) through doing – the course inevitably requires you to do a fair amount of “coding”, in this case in R.\nIt is understandable if this feels like a daunting prospect. The barrier to entry is greater than with point-and-click interfaces such as ArcGIS and SPSS. So do expect that this module may require a degree of patience and persistence – but isn’t that true of all things that are worth learning?\nIn order reduce the pain, I’ve tried to include within the module a balance of content between programming fundamentals, theoretical/conceptual learning and more procedural ‘grunt-work’ with datasets. I have also carefully considered and incorporated ideas from some of the really high quality Resources out there aimed at lowering the barrier to doing Data Science in R.\nIn return, I expect you to:\n Read all course materials Complete the class session tasks, homework and coursework assignment Participate in the discussion forum   Slack A key mechanism through which you can participate is by contributing to the discussion forum. Engaging fully with this will help to foster a sort of collegiate atmosphere on the module that will maximise your learning. I have set up a course Slack, which should provide a useful mechanism for sharing information, resources and importantly posting and discussing code.\nIf you’ve not used Slack before, then follow these pages on getting started with Slack. You should post all substantive questions associated with the module to Slack. These will get answered. If you wish to discuss more personal matters around your completing the course, then send those directly to me via e-mail.\n  Create an account on the vis-for-gds Slack. Be sue to register with you .leeds.ac.uk e-mail. If you run into any problems, try getting started with Slack.    Asking questions As many of you will be learning to program in R for the first time, you should expect to be baffled at times and to routinely encounter scary-looking ERROR messages. Counterintuitively, this is actually to be welcomed. You need to be making mistakes and hitting obstacles on a regular basis if you are to progress.\nHow you respond to these obstacles is important. In a face-to-face lab, the temptation when hitting a problem is to raise your hand, gesture towards your screen and have a demonstrator ‘de-bug’ for you. Whilst this may initially seem like an efficient solution, you risk learning very little if this is your only course of action.\nWhen you encounter problems working through the material in this course, try to force yourself to spend 15-20 minutes troubleshooting the problem individually. Google your problem or try StackOverflow. If you are not able to resolve the problem on your own, then post your question to the course Slack, which I will monitor regularly. When doing this, make an effort to be specific and unambiguous about your problem. You might wish to consult StackOverflow’s guidance on how to ask a good question.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en-uk","lastmod":1606644925,"objectID":"e4d5a4a79239f08c6ad0d7cbf1be756c","permalink":"/syllabus/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/syllabus/","section":"","summary":"Overview Module objectives Assessment Software Module breakdown Navigating the course materials Self-guided learning Slack Asking questions   Instructor  Roger Beecham  10.139 Manton Building  r.j.beecham@leeds.ac.uk  @rjbeecham   Course details  Wednesdays  25th May - 13th Jul  10:00 - 12:00  ODL   Contacting me Questions to do with the substantive aspects of the module should be directed to the #general channel of the course Slack in the first instance.","tags":null,"title":"Syllabus","type":"page"}]